{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/farshed.abdukhakimov/miniconda3/envs/main/bin/python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "farshed.abdukhakimov\n",
      "cn-05\n",
      "/home/farshed.abdukhakimov/projects/sania\n"
     ]
    }
   ],
   "source": [
    "!which python \n",
    "!whoami \n",
    "!hostname\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from experiments import utils, datasets, methods\n",
    "from experiments.loss_functions import LogisticRegressionLoss, BaseOracle\n",
    "\n",
    "\n",
    "import scipy\n",
    "# import optuna \n",
    "\n",
    "import svmlight_loader\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataset_name: str, dataset_scale: int, batch_size: int, n_epochs: int,\n",
    "               optimizer: methods.BaseOptimizer, **optimizer_kwargs) -> dict: \n",
    "    \n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    # dataset \n",
    "    train_data, train_target, _= datasets.get_libsvm(name=dataset_name, scale=dataset_scale, seed=0)\n",
    "    train_load = TensorDataset(torch.tensor(train_data), torch.tensor(train_target))\n",
    "    train_dataloader = DataLoader(train_load, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    test_data, test_target, _ = datasets.get_libsvm(name=dataset_name+\".t\", scale=dataset_scale, seed=0)\n",
    "    \n",
    "    # parameters\n",
    "    params = np.zeros(train_data.shape[1])\n",
    "    optim = optimizer(params=params, **optimizer_kwargs)\n",
    "\n",
    "    # oracle \n",
    "    loss_function = LogisticRegressionLoss()\n",
    "    \n",
    "    # e.g. libsvm dataset has {0.0, 1.0} classes that cannot be used for LogisticRegressionLoss \n",
    "    # hence they will be remapped to {-1.0, 1.0}        \n",
    "    if isinstance(loss_function, LogisticRegressionLoss):\n",
    "        assert np.array_equal(np.unique(train_target), (-1.0, 1.0))\n",
    "    \n",
    "    # logging \n",
    "    history = defaultdict(list)\n",
    "\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        print(f\"Epoch [{epoch}/{n_epochs}]\")\n",
    "\n",
    "        # Testing hist\n",
    "        loss = loss_function.func(params, test_data, test_target)\n",
    "        grad = loss_function.grad(params, test_data, test_target)\n",
    "        g_norm = np.linalg.norm(grad)**2\n",
    "        acc = (np.sign(test_data @ params) == test_target).sum() / test_target.shape[0]\n",
    "        \n",
    "        history[\"test/epoch/loss\"].append(loss)\n",
    "        history[\"test/epoch/acc\"].append(acc)\n",
    "        history[\"test/epoch/grad_norm\"].append(g_norm)\n",
    "        \n",
    "        print(f\"Test Loss: {loss} | Test Acc: {acc} | Test GradNorm: {g_norm}\")\n",
    "        \n",
    "        # Training hist \n",
    "        loss = loss_function.func(params, train_data, train_target)\n",
    "        grad = loss_function.grad(params, train_data, train_target)\n",
    "        g_norm = np.linalg.norm(grad)**2\n",
    "        acc = (np.sign(train_data @ params) == train_target).sum() / train_target.shape[0]\n",
    "        \n",
    "        history[\"train/epoch/loss\"].append(loss)\n",
    "        history[\"train/epoch/acc\"].append(acc)\n",
    "        history[\"train/epoch/grad_norm\"].append(g_norm)\n",
    "        \n",
    "        print(f\"Train Loss: {loss} | Train Acc: {acc} | Train GradNorm: {g_norm}\")\n",
    "        \n",
    "        # Training \n",
    "        for i, (batch_data, batch_target) in enumerate(train_dataloader): \n",
    "            batch_data = batch_data.numpy()\n",
    "            batch_target = batch_target.numpy()\n",
    "\n",
    "            loss_closure = lambda params: loss_function.func(params, batch_data, batch_target)\n",
    "            grad_closure = lambda params: loss_function.grad(params, batch_data, batch_target)\n",
    "            \n",
    "            loss, grad = optim.step(loss_closure=loss_closure, grad_closure=grad_closure)\n",
    "            g_norm = np.linalg.norm(grad)**2\n",
    "            acc = (np.sign(batch_data @ params) == batch_target).sum() / batch_target.shape[0]\n",
    "\n",
    "            history[\"train/batch/loss\"].append(loss)\n",
    "            history[\"train/batch/acc\"].append(acc)\n",
    "            history[\"train/batch/grad_norm\"].append(g_norm)\n",
    "        \n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/500]\n",
      "Test Loss: 0.6931471805599452 | Test Acc: 0.0 | Test GradNorm: 0.4549499969390798\n",
      "Train Loss: 0.6931471805599452 | Train Acc: 0.0 | Train GradNorm: 0.4359846080686329\n",
      "Epoch [1/500]\n",
      "Test Loss: 0.3641255618979228 | Test Acc: 0.8290153766636517 | Test GradNorm: 0.008814422450947334\n",
      "Train Loss: 0.3639153930418685 | Train Acc: 0.8292834890965732 | Train GradNorm: 0.010326347766071045\n",
      "Epoch [2/500]\n",
      "Test Loss: 0.42139860132607804 | Test Acc: 0.8063703320842486 | Test GradNorm: 0.09378428549373355\n",
      "Train Loss: 0.42066124195142995 | Train Acc: 0.8068535825545171 | Train GradNorm: 0.10105798900399285\n",
      "Epoch [3/500]\n",
      "Test Loss: 0.34923201749142585 | Test Acc: 0.8364452771675928 | Test GradNorm: 0.009224207954157732\n",
      "Train Loss: 0.3433956207198768 | Train Acc: 0.8355140186915888 | Train GradNorm: 0.01127583518685349\n",
      "Epoch [4/500]\n",
      "Test Loss: 0.38203540590488544 | Test Acc: 0.8216823879054141 | Test GradNorm: 0.054989579922183424\n",
      "Train Loss: 0.3774314963905613 | Train Acc: 0.8230529595015577 | Train GradNorm: 0.06141363039817179\n",
      "Epoch [5/500]\n",
      "Test Loss: 0.4113374213760383 | Test Acc: 0.8155769479260886 | Test GradNorm: 0.08519918200370763\n",
      "Train Loss: 0.40609653513321226 | Train Acc: 0.8186915887850468 | Train GradNorm: 0.09423880362255475\n",
      "Epoch [6/500]\n",
      "Test Loss: 0.3409073368220279 | Test Acc: 0.8427122367230908 | Test GradNorm: 0.0004914879493791078\n",
      "Train Loss: 0.32666798146689496 | Train Acc: 0.8411214953271028 | Train GradNorm: 0.0002154333945182533\n",
      "Epoch [7/500]\n",
      "Test Loss: 0.3456605910008204 | Test Acc: 0.8398048843519834 | Test GradNorm: 0.009720428362840466\n",
      "Train Loss: 0.3300062066835937 | Train Acc: 0.8429906542056075 | Train GradNorm: 0.007349635545907319\n",
      "Epoch [8/500]\n",
      "Test Loss: 0.34038643725154444 | Test Acc: 0.8414846879441789 | Test GradNorm: 0.001725985923486339\n",
      "Train Loss: 0.3254127922555319 | Train Acc: 0.8404984423676013 | Train GradNorm: 0.00252347444756849\n",
      "Epoch [9/500]\n",
      "Test Loss: 0.40176598680465825 | Test Acc: 0.8059180772709652 | Test GradNorm: 0.09921573715483094\n",
      "Train Loss: 0.3808606482185668 | Train Acc: 0.8180685358255452 | Train GradNorm: 0.09249278408218206\n",
      "Epoch [10/500]\n",
      "Test Loss: 0.34000033872841523 | Test Acc: 0.8426799328078564 | Test GradNorm: 0.0005843263999424592\n",
      "Train Loss: 0.3216632414870139 | Train Acc: 0.8485981308411215 | Train GradNorm: 8.205676479005959e-05\n",
      "Epoch [11/500]\n",
      "Test Loss: 0.34339311895506036 | Test Acc: 0.8407740018090193 | Test GradNorm: 0.0067335675355813165\n",
      "Train Loss: 0.32387214531921704 | Train Acc: 0.8461059190031153 | Train GradNorm: 0.004282896103257723\n",
      "Epoch [12/500]\n",
      "Test Loss: 0.3399362198844811 | Test Acc: 0.8434875306887195 | Test GradNorm: 0.0005645284282877193\n",
      "Train Loss: 0.3202433701370727 | Train Acc: 0.84797507788162 | Train GradNorm: 6.796493061027543e-05\n",
      "Epoch [13/500]\n",
      "Test Loss: 0.5659875210519978 | Test Acc: 0.7350755911616488 | Test GradNorm: 0.3414765448721926\n",
      "Train Loss: 0.5390667604598756 | Train Acc: 0.746417445482866 | Train GradNorm: 0.33794289318034587\n",
      "Epoch [14/500]\n",
      "Test Loss: 0.34305478439732245 | Test Acc: 0.842292285825042 | Test GradNorm: 0.0043251241411734\n",
      "Train Loss: 0.32382512591904894 | Train Acc: 0.8448598130841122 | Train GradNorm: 0.0063346645243330625\n",
      "Epoch [15/500]\n",
      "Test Loss: 0.3755319119167953 | Test Acc: 0.8273678769866908 | Test GradNorm: 0.0438760546598435\n",
      "Train Loss: 0.35860185854098475 | Train Acc: 0.8292834890965732 | Train GradNorm: 0.050695043917778425\n",
      "Epoch [16/500]\n",
      "Test Loss: 0.34625097561869506 | Test Acc: 0.839901796097687 | Test GradNorm: 0.0075586951212931065\n",
      "Train Loss: 0.32193187041129256 | Train Acc: 0.8492211838006231 | Train GradNorm: 0.005379883972384272\n",
      "Epoch [17/500]\n",
      "Test Loss: 0.34227285743483654 | Test Acc: 0.84277684455356 | Test GradNorm: 0.0011033892449122333\n",
      "Train Loss: 0.31911072579126515 | Train Acc: 0.8498442367601247 | Train GradNorm: 0.0015132493551983865\n",
      "Epoch [18/500]\n",
      "Test Loss: 0.3532081996074248 | Test Acc: 0.8358638066933712 | Test GradNorm: 0.019337423787908835\n",
      "Train Loss: 0.32748813581155706 | Train Acc: 0.8461059190031153 | Train GradNorm: 0.0155778441462971\n",
      "Epoch [19/500]\n",
      "Test Loss: 0.34982221346571024 | Test Acc: 0.8389326786406512 | Test GradNorm: 0.013370027511447957\n",
      "Train Loss: 0.3234521172122482 | Train Acc: 0.8498442367601247 | Train GradNorm: 0.009694017192376767\n",
      "Epoch [20/500]\n",
      "Test Loss: 0.36030437831283135 | Test Acc: 0.8318258172890555 | Test GradNorm: 0.028525141534878687\n",
      "Train Loss: 0.33201397738545235 | Train Acc: 0.8429906542056075 | Train GradNorm: 0.023806305609185858\n",
      "Epoch [21/500]\n",
      "Test Loss: 0.34672703528869575 | Test Acc: 0.841678511435586 | Test GradNorm: 0.007508587383001954\n",
      "Train Loss: 0.31962157477543646 | Train Acc: 0.8517133956386292 | Train GradNorm: 0.004655696828395828\n",
      "Epoch [22/500]\n",
      "Test Loss: 0.39770597670964425 | Test Acc: 0.8240405737175346 | Test GradNorm: 0.06372056974023907\n",
      "Train Loss: 0.38071903505710314 | Train Acc: 0.8342679127725857 | Train GradNorm: 0.07375456647832096\n",
      "Epoch [23/500]\n",
      "Test Loss: 0.34257111864659634 | Test Acc: 0.8431644915363742 | Test GradNorm: 0.0004110164553810054\n",
      "Train Loss: 0.3157060495252902 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.00040680268386529653\n",
      "Epoch [24/500]\n",
      "Test Loss: 0.3423650631220709 | Test Acc: 0.8431644915363742 | Test GradNorm: 0.0008143256023471071\n",
      "Train Loss: 0.31506983463882277 | Train Acc: 0.8566978193146417 | Train GradNorm: 4.8373392764229315e-05\n",
      "Epoch [25/500]\n",
      "Test Loss: 0.36348786615598805 | Test Acc: 0.8302106215273292 | Test GradNorm: 0.03142551027181233\n",
      "Train Loss: 0.33235670283098384 | Train Acc: 0.8423676012461059 | Train GradNorm: 0.02627600213128554\n",
      "Epoch [26/500]\n",
      "Test Loss: 0.34603855043198034 | Test Acc: 0.8416462075203515 | Test GradNorm: 0.004351605208916053\n",
      "Train Loss: 0.3159128197517426 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.0022008576983122458\n",
      "Epoch [27/500]\n",
      "Test Loss: 0.34247570351069856 | Test Acc: 0.8428091484687944 | Test GradNorm: 0.0004771971172834977\n",
      "Train Loss: 0.3148433397887383 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.0008065979653435118\n",
      "Epoch [28/500]\n",
      "Test Loss: 0.34222376895843293 | Test Acc: 0.8436167463496576 | Test GradNorm: 0.00034912465278219833\n",
      "Train Loss: 0.31451857061442434 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.00030680434539785576\n",
      "Epoch [29/500]\n",
      "Test Loss: 0.3468959980449016 | Test Acc: 0.8416139036051169 | Test GradNorm: 0.005266888128640207\n",
      "Train Loss: 0.316604414715429 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.0028297093986314956\n",
      "Epoch [30/500]\n",
      "Test Loss: 0.3451085754745403 | Test Acc: 0.8420661584184003 | Test GradNorm: 0.002972592128730963\n",
      "Train Loss: 0.3173193365814511 | Train Acc: 0.84797507788162 | Train GradNorm: 0.004651372090941633\n",
      "Epoch [31/500]\n",
      "Test Loss: 0.36825145491692396 | Test Acc: 0.8326011112546841 | Test GradNorm: 0.031197002633599365\n",
      "Train Loss: 0.34285277060356595 | Train Acc: 0.8367601246105919 | Train GradNorm: 0.03721416548118294\n",
      "Epoch [32/500]\n",
      "Test Loss: 0.3455349694253875 | Test Acc: 0.8417754231812896 | Test GradNorm: 0.0022886934566397166\n",
      "Train Loss: 0.3141768018685294 | Train Acc: 0.8554517133956386 | Train GradNorm: 0.0007378406923709214\n",
      "Epoch [33/500]\n",
      "Test Loss: 0.34737533826623856 | Test Acc: 0.8409678253004265 | Test GradNorm: 0.00522162342002184\n",
      "Train Loss: 0.31487927804494803 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.0030314260278677045\n",
      "Epoch [34/500]\n",
      "Test Loss: 0.34528081249847076 | Test Acc: 0.8416462075203515 | Test GradNorm: 0.001588971844705488\n",
      "Train Loss: 0.31455067167814144 | Train Acc: 0.8535825545171339 | Train GradNorm: 0.0023670183307711867\n",
      "Epoch [35/500]\n",
      "Test Loss: 0.3662033155617444 | Test Acc: 0.830695180255847 | Test GradNorm: 0.03222956770835712\n",
      "Train Loss: 0.3314607101539111 | Train Acc: 0.8436137071651091 | Train GradNorm: 0.026832474167439148\n",
      "Epoch [36/500]\n",
      "Test Loss: 0.344971587492584 | Test Acc: 0.842356893655511 | Test GradNorm: 0.00114708594230856\n",
      "Train Loss: 0.3125232513489759 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.00014250467432878883\n",
      "Epoch [37/500]\n",
      "Test Loss: 0.353341962173717 | Test Acc: 0.837382090709394 | Test GradNorm: 0.013209617008239501\n",
      "Train Loss: 0.3187683675912486 | Train Acc: 0.8498442367601247 | Train GradNorm: 0.009678826365012012\n",
      "Epoch [38/500]\n",
      "Test Loss: 0.358853036276877 | Test Acc: 0.8360253262695438 | Test GradNorm: 0.018486333144761644\n",
      "Train Loss: 0.3302078837960882 | Train Acc: 0.8411214953271028 | Train GradNorm: 0.023483754048022904\n",
      "Epoch [39/500]\n",
      "Test Loss: 0.34657933513501993 | Test Acc: 0.8405155704871431 | Test GradNorm: 0.0023393350103858056\n",
      "Train Loss: 0.31247727290728233 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.0007610113282719314\n",
      "Epoch [40/500]\n",
      "Test Loss: 0.35021480034604635 | Test Acc: 0.838609639488306 | Test GradNorm: 0.006831470694433945\n",
      "Train Loss: 0.31900120882310956 | Train Acc: 0.8498442367601247 | Train GradNorm: 0.009765475765825505\n",
      "Epoch [41/500]\n",
      "Test Loss: 0.3465752034315479 | Test Acc: 0.8418077270965241 | Test GradNorm: 0.001402670652815696\n",
      "Train Loss: 0.3118587828834163 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.00025510624385641163\n",
      "Epoch [42/500]\n",
      "Test Loss: 0.36930435394969624 | Test Acc: 0.8271740534952836 | Test GradNorm: 0.03646465085673053\n",
      "Train Loss: 0.33220897664639953 | Train Acc: 0.8404984423676013 | Train GradNorm: 0.030817709489912297\n",
      "Epoch [43/500]\n",
      "Test Loss: 0.35767472590367366 | Test Acc: 0.8351854244734462 | Test GradNorm: 0.015616883551893232\n",
      "Train Loss: 0.326431372353543 | Train Acc: 0.8417445482866044 | Train GradNorm: 0.019741119904771454\n",
      "Epoch [44/500]\n",
      "Test Loss: 0.34587671760127325 | Test Acc: 0.8413877761984753 | Test GradNorm: 0.0014464560918883646\n",
      "Train Loss: 0.3114690614518504 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.00027682351836667536\n",
      "Epoch [45/500]\n",
      "Test Loss: 0.35830377080976017 | Test Acc: 0.8369621398113452 | Test GradNorm: 0.017339350923102573\n",
      "Train Loss: 0.3285040744087029 | Train Acc: 0.8398753894080997 | Train GradNorm: 0.02247608239771023\n",
      "Epoch [46/500]\n",
      "Test Loss: 0.34895497741677894 | Test Acc: 0.8400633156738597 | Test GradNorm: 0.0037434468563470372\n",
      "Train Loss: 0.315186447759788 | Train Acc: 0.8492211838006231 | Train GradNorm: 0.005623966158484051\n",
      "Epoch [47/500]\n",
      "Test Loss: 0.37297126740589226 | Test Acc: 0.8259788086316061 | Test GradNorm: 0.03842518823189592\n",
      "Train Loss: 0.3328138382180614 | Train Acc: 0.8411214953271028 | Train GradNorm: 0.032574590442053665\n",
      "Epoch [48/500]\n",
      "Test Loss: 0.3546832011436097 | Test Acc: 0.837446698539863 | Test GradNorm: 0.011841158815446605\n",
      "Train Loss: 0.31647346623624795 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.008278258151272068\n",
      "Epoch [49/500]\n",
      "Test Loss: 0.393491717694852 | Test Acc: 0.811668174182711 | Test GradNorm: 0.06986198197065584\n",
      "Train Loss: 0.35259996726633297 | Train Acc: 0.8305295950155763 | Train GradNorm: 0.0638298391056613\n",
      "Epoch [50/500]\n",
      "Test Loss: 0.3847585015596343 | Test Acc: 0.8275940043933324 | Test GradNorm: 0.045962169861195494\n",
      "Train Loss: 0.3554377634476488 | Train Acc: 0.8342679127725857 | Train GradNorm: 0.05368905181041775\n",
      "Epoch [51/500]\n",
      "Test Loss: 0.34849790049084484 | Test Acc: 0.8410001292156609 | Test GradNorm: 0.0037897091547774544\n",
      "Train Loss: 0.3118602923577049 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.001857588004238404\n",
      "Epoch [52/500]\n",
      "Test Loss: 0.34911347895235234 | Test Acc: 0.8412262566223027 | Test GradNorm: 0.002902857001253587\n",
      "Train Loss: 0.3137579184943273 | Train Acc: 0.8498442367601247 | Train GradNorm: 0.0045765733572834125\n",
      "Epoch [53/500]\n",
      "Test Loss: 0.3475870687843221 | Test Acc: 0.8418077270965241 | Test GradNorm: 0.00048201459384179807\n",
      "Train Loss: 0.31030798948311583 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.00033036484470256775\n",
      "Epoch [54/500]\n",
      "Test Loss: 0.38857932051707095 | Test Acc: 0.8268187104277038 | Test GradNorm: 0.04920363879056584\n",
      "Train Loss: 0.3574713452068669 | Train Acc: 0.8348909657320872 | Train GradNorm: 0.05692361164589019\n",
      "Epoch [55/500]\n",
      "Test Loss: 0.35290251387266075 | Test Acc: 0.8387065512340096 | Test GradNorm: 0.008614010885865189\n",
      "Train Loss: 0.31375504805007426 | Train Acc: 0.8510903426791278 | Train GradNorm: 0.0056540464391043235\n",
      "Epoch [56/500]\n",
      "Test Loss: 0.3577337162541084 | Test Acc: 0.8380281690140845 | Test GradNorm: 0.014024586530036645\n",
      "Train Loss: 0.3236345655546725 | Train Acc: 0.8436137071651091 | Train GradNorm: 0.018245595444915854\n",
      "Epoch [57/500]\n",
      "Test Loss: 0.38133428799102786 | Test Acc: 0.819098074686652 | Test GradNorm: 0.05061296369936518\n",
      "Train Loss: 0.3391477826730416 | Train Acc: 0.832398753894081 | Train GradNorm: 0.04376480880554087\n",
      "Epoch [58/500]\n",
      "Test Loss: 0.35986196451848884 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.016968382060289403\n",
      "Train Loss: 0.3268735223727243 | Train Acc: 0.8398753894080997 | Train GradNorm: 0.022130739332713603\n",
      "Epoch [59/500]\n",
      "Test Loss: 0.3572875795366642 | Test Acc: 0.836832924150407 | Test GradNorm: 0.014189413902768821\n",
      "Train Loss: 0.31678104044491723 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.009974572087449923\n",
      "Epoch [60/500]\n",
      "Test Loss: 0.39243187419755315 | Test Acc: 0.81173278201318 | Test GradNorm: 0.06656975682584633\n",
      "Train Loss: 0.34927578414063826 | Train Acc: 0.8274143302180685 | Train GradNorm: 0.05933952114917785\n",
      "Epoch [61/500]\n",
      "Test Loss: 0.3492989010514859 | Test Acc: 0.8411293448765991 | Test GradNorm: 0.0024509862126751393\n",
      "Train Loss: 0.31170175697795366 | Train Acc: 0.8504672897196262 | Train GradNorm: 0.003709687301721511\n",
      "Epoch [62/500]\n",
      "Test Loss: 0.39083575442072166 | Test Acc: 0.81480165396046 | Test GradNorm: 0.0628570456203107\n",
      "Train Loss: 0.34737477295503083 | Train Acc: 0.8311526479750779 | Train GradNorm: 0.05609615884885265\n",
      "Epoch [63/500]\n",
      "Test Loss: 0.3566933394791484 | Test Acc: 0.8381250807597881 | Test GradNorm: 0.011635297658569703\n",
      "Train Loss: 0.32132354877430885 | Train Acc: 0.8473520249221184 | Train GradNorm: 0.015515300792419038\n",
      "Epoch [64/500]\n",
      "Test Loss: 0.34815407452531144 | Test Acc: 0.8409032174699573 | Test GradNorm: 0.0007353169438486807\n",
      "Train Loss: 0.3095225162794063 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.0011658108577259694\n",
      "Epoch [65/500]\n",
      "Test Loss: 0.3508915656841394 | Test Acc: 0.8397402765215144 | Test GradNorm: 0.0036968462889415757\n",
      "Train Loss: 0.31269437853373694 | Train Acc: 0.8517133956386292 | Train GradNorm: 0.005572681043734233\n",
      "Epoch [66/500]\n",
      "Test Loss: 0.37780432091606375 | Test Acc: 0.8222315544644011 | Test GradNorm: 0.04032889497022077\n",
      "Train Loss: 0.33270967297121473 | Train Acc: 0.8342679127725857 | Train GradNorm: 0.0348527546891041\n",
      "Epoch [67/500]\n",
      "Test Loss: 0.3518334505182008 | Test Acc: 0.8397402765215144 | Test GradNorm: 0.004064093231298365\n",
      "Train Loss: 0.3098926588127763 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.0019996412630058223\n",
      "Epoch [68/500]\n",
      "Test Loss: 0.3508142767148253 | Test Acc: 0.839901796097687 | Test GradNorm: 0.0020861064875609915\n",
      "Train Loss: 0.30873177985190425 | Train Acc: 0.8604361370716511 | Train GradNorm: 0.0005787968265236448\n",
      "Epoch [69/500]\n",
      "Test Loss: 0.3663179538480944 | Test Acc: 0.829532239307404 | Test GradNorm: 0.024076975399334618\n",
      "Train Loss: 0.3221640217744162 | Train Acc: 0.84797507788162 | Train GradNorm: 0.019816418843287602\n",
      "Epoch [70/500]\n",
      "Test Loss: 0.3565232993312376 | Test Acc: 0.8385450316578369 | Test GradNorm: 0.010923818895943486\n",
      "Train Loss: 0.31936344986482856 | Train Acc: 0.8517133956386292 | Train GradNorm: 0.014475361552085121\n",
      "Epoch [71/500]\n",
      "Test Loss: 0.353210628140462 | Test Acc: 0.8383189042511953 | Test GradNorm: 0.005363765910653078\n",
      "Train Loss: 0.31391705477609755 | Train Acc: 0.8492211838006231 | Train GradNorm: 0.007207245462969508\n",
      "Epoch [72/500]\n",
      "Test Loss: 0.35016272845082114 | Test Acc: 0.8393849334539346 | Test GradNorm: 0.0026852400602602154\n",
      "Train Loss: 0.3112496630441555 | Train Acc: 0.8517133956386292 | Train GradNorm: 0.003986883057862201\n",
      "Epoch [73/500]\n",
      "Test Loss: 0.3511761790738399 | Test Acc: 0.8389649825558858 | Test GradNorm: 0.001927262193418572\n",
      "Train Loss: 0.31062667595817 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.002737174687624671\n",
      "Epoch [74/500]\n",
      "Test Loss: 0.35958840341087106 | Test Acc: 0.8357991988629022 | Test GradNorm: 0.012945641686158228\n",
      "Train Loss: 0.32072528995521327 | Train Acc: 0.8467289719626169 | Train GradNorm: 0.016464327980909882\n",
      "Epoch [75/500]\n",
      "Test Loss: 0.3502716346893029 | Test Acc: 0.8403540509109704 | Test GradNorm: 0.0018269164451872406\n",
      "Train Loss: 0.31015030716597825 | Train Acc: 0.8523364485981308 | Train GradNorm: 0.0028549355789755193\n",
      "Epoch [76/500]\n",
      "Test Loss: 0.3502983575966886 | Test Acc: 0.8401602274195632 | Test GradNorm: 0.0011473967242049236\n",
      "Train Loss: 0.30965135742176925 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.0019508912236106118\n",
      "Epoch [77/500]\n",
      "Test Loss: 0.35162189347159883 | Test Acc: 0.8399987078433906 | Test GradNorm: 0.0029383248989531025\n",
      "Train Loss: 0.31125231306869205 | Train Acc: 0.8510903426791278 | Train GradNorm: 0.004677142219758432\n",
      "Epoch [78/500]\n",
      "Test Loss: 0.3517409261128812 | Test Acc: 0.839772580436749 | Test GradNorm: 0.002663904811138031\n",
      "Train Loss: 0.3108057906076346 | Train Acc: 0.8535825545171339 | Train GradNorm: 0.004214508733614017\n",
      "Epoch [79/500]\n",
      "Test Loss: 0.355625998733473 | Test Acc: 0.8375759142008011 | Test GradNorm: 0.008070173940523805\n",
      "Train Loss: 0.3110711268602212 | Train Acc: 0.8554517133956386 | Train GradNorm: 0.005212670650831907\n",
      "Epoch [80/500]\n",
      "Test Loss: 0.3879304172788015 | Test Acc: 0.8296291510531076 | Test GradNorm: 0.044081902055319866\n",
      "Train Loss: 0.35246632061382477 | Train Acc: 0.8355140186915888 | Train GradNorm: 0.05213778268039138\n",
      "Epoch [81/500]\n",
      "Test Loss: 0.35114374973084156 | Test Acc: 0.8406124822328466 | Test GradNorm: 0.002022319633177903\n",
      "Train Loss: 0.3082304523683521 | Train Acc: 0.8635514018691589 | Train GradNorm: 0.0005747065265905104\n",
      "Epoch [82/500]\n",
      "Test Loss: 0.3524170253444337 | Test Acc: 0.8396110608605764 | Test GradNorm: 0.0021232185940124213\n",
      "Train Loss: 0.3102607467598961 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.003196345512741319\n",
      "Epoch [83/500]\n",
      "Test Loss: 0.352806702334214 | Test Acc: 0.839288021708231 | Test GradNorm: 0.004441309633427723\n",
      "Train Loss: 0.30895142891081684 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.002315244383853588\n",
      "Epoch [84/500]\n",
      "Test Loss: 0.38701851698818757 | Test Acc: 0.8274324848171598 | Test GradNorm: 0.04424911124482057\n",
      "Train Loss: 0.3501049067484279 | Train Acc: 0.8342679127725857 | Train GradNorm: 0.05198830709177023\n",
      "Epoch [85/500]\n",
      "Test Loss: 0.41951767202567086 | Test Acc: 0.7991665589869492 | Test GradNorm: 0.10070729340187277\n",
      "Train Loss: 0.3702994502779049 | Train Acc: 0.8161993769470405 | Train GradNorm: 0.09245395748392166\n",
      "Epoch [86/500]\n",
      "Test Loss: 0.3765964697279314 | Test Acc: 0.8220377309729939 | Test GradNorm: 0.03746589389971104\n",
      "Train Loss: 0.32959930672526133 | Train Acc: 0.8417445482866044 | Train GradNorm: 0.03252539987593921\n",
      "Epoch [87/500]\n",
      "Test Loss: 0.40224933542784297 | Test Acc: 0.8230391523452643 | Test GradNorm: 0.060631471550905994\n",
      "Train Loss: 0.366359606139661 | Train Acc: 0.8261682242990654 | Train GradNorm: 0.06963748345143635\n",
      "Epoch [88/500]\n",
      "Test Loss: 0.3747200536739049 | Test Acc: 0.8236852306499548 | Test GradNorm: 0.0339449708695604\n",
      "Train Loss: 0.3272102434960794 | Train Acc: 0.8461059190031153 | Train GradNorm: 0.029221103514486712\n",
      "Epoch [89/500]\n",
      "Test Loss: 0.3520873288429129 | Test Acc: 0.8395464530301072 | Test GradNorm: 0.0017701042447396409\n",
      "Train Loss: 0.30887769459160824 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.0025748915769746583\n",
      "Epoch [90/500]\n",
      "Test Loss: 0.36340299663349435 | Test Acc: 0.8344101305078175 | Test GradNorm: 0.019167716304827335\n",
      "Train Loss: 0.3171626796100173 | Train Acc: 0.8510903426791278 | Train GradNorm: 0.014694528141705989\n",
      "Epoch [91/500]\n",
      "Test Loss: 0.3522951772062673 | Test Acc: 0.8415492957746479 | Test GradNorm: 0.004024340493710611\n",
      "Train Loss: 0.31211942707885476 | Train Acc: 0.8535825545171339 | Train GradNorm: 0.006227944098698855\n",
      "Epoch [92/500]\n",
      "Test Loss: 0.3501978318057227 | Test Acc: 0.8412262566223027 | Test GradNorm: 0.0005424365074616691\n",
      "Train Loss: 0.30714235560475184 | Train Acc: 0.8604361370716511 | Train GradNorm: 0.0007638086645355438\n",
      "Epoch [93/500]\n",
      "Test Loss: 0.35297173609310223 | Test Acc: 0.8395787569453418 | Test GradNorm: 0.0041516527397161326\n",
      "Train Loss: 0.3079556777694345 | Train Acc: 0.8623052959501558 | Train GradNorm: 0.002109544964273586\n",
      "Epoch [94/500]\n",
      "Test Loss: 0.3534893571841765 | Test Acc: 0.8397402765215144 | Test GradNorm: 0.004169852060198946\n",
      "Train Loss: 0.3078254381202873 | Train Acc: 0.8604361370716511 | Train GradNorm: 0.002075816061360637\n",
      "Epoch [95/500]\n",
      "Test Loss: 0.37818457305225867 | Test Acc: 0.8237175345651893 | Test GradNorm: 0.039855038291744735\n",
      "Train Loss: 0.3293859437388856 | Train Acc: 0.8386292834890966 | Train GradNorm: 0.033795460050806164\n",
      "Epoch [96/500]\n",
      "Test Loss: 0.35318544569653193 | Test Acc: 0.8397402765215144 | Test GradNorm: 0.0033629938705710565\n",
      "Train Loss: 0.310015304111548 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.0048605365153551\n",
      "Epoch [97/500]\n",
      "Test Loss: 0.35013618453181633 | Test Acc: 0.8405155704871431 | Test GradNorm: 0.0007735660760443128\n",
      "Train Loss: 0.3061674015685911 | Train Acc: 0.859190031152648 | Train GradNorm: 2.2652135496190937e-05\n",
      "Epoch [98/500]\n",
      "Test Loss: 0.349980112640103 | Test Acc: 0.8405801783176121 | Test GradNorm: 0.0010456076709571514\n",
      "Train Loss: 0.3062110811856865 | Train Acc: 0.8623052959501558 | Train GradNorm: 9.655611665256962e-05\n",
      "Epoch [99/500]\n",
      "Test Loss: 0.35196502515619266 | Test Acc: 0.8402571391652668 | Test GradNorm: 0.002821628334367102\n",
      "Train Loss: 0.30942494447591706 | Train Acc: 0.8517133956386292 | Train GradNorm: 0.004291843610003293\n",
      "Epoch [100/500]\n",
      "Test Loss: 0.3553667087450513 | Test Acc: 0.8386419434035405 | Test GradNorm: 0.007694888827213985\n",
      "Train Loss: 0.3142097983423329 | Train Acc: 0.8517133956386292 | Train GradNorm: 0.01073752229481493\n",
      "Epoch [101/500]\n",
      "Test Loss: 0.38127698458747883 | Test Acc: 0.820874790024551 | Test GradNorm: 0.04559929745906072\n",
      "Train Loss: 0.3323089572999486 | Train Acc: 0.838006230529595 | Train GradNorm: 0.03871384626790909\n",
      "Epoch [102/500]\n",
      "Test Loss: 0.3545904880111895 | Test Acc: 0.8382219925054917 | Test GradNorm: 0.0068413562365065945\n",
      "Train Loss: 0.3088684763463472 | Train Acc: 0.8598130841121495 | Train GradNorm: 0.004096074381646716\n",
      "Epoch [103/500]\n",
      "Test Loss: 0.3530256367739452 | Test Acc: 0.8391265021320584 | Test GradNorm: 0.004442940702361878\n",
      "Train Loss: 0.30782518699452566 | Train Acc: 0.8598130841121495 | Train GradNorm: 0.0022682633351246923\n",
      "Epoch [104/500]\n",
      "Test Loss: 0.4488304695808458 | Test Acc: 0.8143170952319421 | Test GradNorm: 0.1031527399586218\n",
      "Train Loss: 0.41465052104651295 | Train Acc: 0.8186915887850468 | Train GradNorm: 0.11505513079357958\n",
      "Epoch [105/500]\n",
      "Test Loss: 0.3548900525447904 | Test Acc: 0.8389326786406512 | Test GradNorm: 0.0062005610007890045\n",
      "Train Loss: 0.3084264287755098 | Train Acc: 0.8598130841121495 | Train GradNorm: 0.003753827894845496\n",
      "Epoch [106/500]\n",
      "Test Loss: 0.35202908528399746 | Test Acc: 0.8409678253004265 | Test GradNorm: 0.0012714421979875326\n",
      "Train Loss: 0.3080154814464505 | Train Acc: 0.854828660436137 | Train GradNorm: 0.0022790898813299675\n",
      "Epoch [107/500]\n",
      "Test Loss: 0.3870980170531405 | Test Acc: 0.8285308179351337 | Test GradNorm: 0.04207440187134349\n",
      "Train Loss: 0.34644622964417154 | Train Acc: 0.8348909657320872 | Train GradNorm: 0.049155100332050894\n",
      "Epoch [108/500]\n",
      "Test Loss: 0.36245686969038005 | Test Acc: 0.8352500323039153 | Test GradNorm: 0.016433165287136446\n",
      "Train Loss: 0.3147780972851547 | Train Acc: 0.8498442367601247 | Train GradNorm: 0.012349660760322467\n",
      "Epoch [109/500]\n",
      "Test Loss: 0.35888516783544994 | Test Acc: 0.8353469440496188 | Test GradNorm: 0.01196335084156473\n",
      "Train Loss: 0.3114778005563697 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.008329352278536449\n",
      "Epoch [110/500]\n",
      "Test Loss: 0.36573292675628816 | Test Acc: 0.8318904251195245 | Test GradNorm: 0.020683822405819008\n",
      "Train Loss: 0.3174436384094017 | Train Acc: 0.8510903426791278 | Train GradNorm: 0.01648418886921611\n",
      "Epoch [111/500]\n",
      "Test Loss: 0.3542685878946853 | Test Acc: 0.8394495412844036 | Test GradNorm: 0.004784527578231184\n",
      "Train Loss: 0.3108004014763477 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.006997332697558028\n",
      "Epoch [112/500]\n",
      "Test Loss: 0.3529422373399509 | Test Acc: 0.839837188267218 | Test GradNorm: 0.0026264733872450536\n",
      "Train Loss: 0.30870754381653737 | Train Acc: 0.854828660436137 | Train GradNorm: 0.004084619604116552\n",
      "Epoch [113/500]\n",
      "Test Loss: 0.3597186106437689 | Test Acc: 0.8357345910324331 | Test GradNorm: 0.013315039313035517\n",
      "Train Loss: 0.312097572679601 | Train Acc: 0.8492211838006231 | Train GradNorm: 0.009601441853934543\n",
      "Epoch [114/500]\n",
      "Test Loss: 0.3526379460133751 | Test Acc: 0.8402894430805014 | Test GradNorm: 0.00324495221757294\n",
      "Train Loss: 0.30643303942691 | Train Acc: 0.8635514018691589 | Train GradNorm: 0.0014826526871013948\n",
      "Epoch [115/500]\n",
      "Test Loss: 0.381598439596848 | Test Acc: 0.8227807210233881 | Test GradNorm: 0.04326670862464117\n",
      "Train Loss: 0.3319830318635861 | Train Acc: 0.8386292834890966 | Train GradNorm: 0.03730258995616337\n",
      "Epoch [116/500]\n",
      "Test Loss: 0.3654655922840034 | Test Acc: 0.8334410130507818 | Test GradNorm: 0.02066913324629216\n",
      "Train Loss: 0.31684121782244923 | Train Acc: 0.8436137071651091 | Train GradNorm: 0.015760395510202967\n",
      "Epoch [117/500]\n",
      "Test Loss: 0.36175706690671017 | Test Acc: 0.8370913554722832 | Test GradNorm: 0.012837767299289508\n",
      "Train Loss: 0.31882118772939727 | Train Acc: 0.8498442367601247 | Train GradNorm: 0.016979866677090023\n",
      "Epoch [118/500]\n",
      "Test Loss: 0.36877217400347273 | Test Acc: 0.8340224835250032 | Test GradNorm: 0.021194837030136263\n",
      "Train Loss: 0.32604960697728375 | Train Acc: 0.8429906542056075 | Train GradNorm: 0.0260987250282366\n",
      "Epoch [119/500]\n",
      "Test Loss: 0.35777226562244524 | Test Acc: 0.8395141491148728 | Test GradNorm: 0.008113026431292776\n",
      "Train Loss: 0.31374910136507445 | Train Acc: 0.8510903426791278 | Train GradNorm: 0.011107750559677129\n",
      "Epoch [120/500]\n",
      "Test Loss: 0.3537735814903842 | Test Acc: 0.8380927768445535 | Test GradNorm: 0.0021292214408493277\n",
      "Train Loss: 0.3056950328988181 | Train Acc: 0.8623052959501558 | Train GradNorm: 0.0006218894553414326\n",
      "Epoch [121/500]\n",
      "Test Loss: 0.365784369645252 | Test Acc: 0.8361868458457165 | Test GradNorm: 0.01825904210618997\n",
      "Train Loss: 0.3234321103261053 | Train Acc: 0.8423676012461059 | Train GradNorm: 0.02317193036769853\n",
      "Epoch [122/500]\n",
      "Test Loss: 0.40922992310710427 | Test Acc: 0.8045613128311151 | Test GradNorm: 0.08466839469657002\n",
      "Train Loss: 0.3564788768438834 | Train Acc: 0.8280373831775701 | Train GradNorm: 0.07582046526438552\n",
      "Epoch [123/500]\n",
      "Test Loss: 0.3604980733786275 | Test Acc: 0.8371882672179868 | Test GradNorm: 0.011688254365558311\n",
      "Train Loss: 0.31671811900401403 | Train Acc: 0.8504672897196262 | Train GradNorm: 0.015147336626067275\n",
      "Epoch [124/500]\n",
      "Test Loss: 0.3901859751178522 | Test Acc: 0.8144463108928802 | Test GradNorm: 0.054259199811163394\n",
      "Train Loss: 0.3389237098719885 | Train Acc: 0.8330218068535825 | Train GradNorm: 0.04883183676814837\n",
      "Epoch [125/500]\n",
      "Test Loss: 0.4015953114439583 | Test Acc: 0.8077594004393333 | Test GradNorm: 0.07042404046704538\n",
      "Train Loss: 0.3489304460919065 | Train Acc: 0.8280373831775701 | Train GradNorm: 0.06412914164544443\n",
      "Epoch [126/500]\n",
      "Test Loss: 0.3532137602982214 | Test Acc: 0.8399664039281561 | Test GradNorm: 0.0011433977715693535\n",
      "Train Loss: 0.3053036412438675 | Train Acc: 0.859190031152648 | Train GradNorm: 0.0001471226528867869\n",
      "Epoch [127/500]\n",
      "Test Loss: 0.37376203991476337 | Test Acc: 0.8296937588835767 | Test GradNorm: 0.02894554301507065\n",
      "Train Loss: 0.3221888270451822 | Train Acc: 0.84797507788162 | Train GradNorm: 0.02298938162015671\n",
      "Epoch [128/500]\n",
      "Test Loss: 0.355663263511846 | Test Acc: 0.8393203256234656 | Test GradNorm: 0.0033126805272811395\n",
      "Train Loss: 0.30888755090260583 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.005039754237978922\n",
      "Epoch [129/500]\n",
      "Test Loss: 0.35605905797929865 | Test Acc: 0.8391265021320584 | Test GradNorm: 0.003661002431530877\n",
      "Train Loss: 0.3089340865968591 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.005441734580081591\n",
      "Epoch [130/500]\n",
      "Test Loss: 0.35425395297404055 | Test Acc: 0.839223413877762 | Test GradNorm: 0.0005877149573674423\n",
      "Train Loss: 0.30524452797596413 | Train Acc: 0.8623052959501558 | Train GradNorm: 0.0004441535556397621\n",
      "Epoch [131/500]\n",
      "Test Loss: 0.35551301267228713 | Test Acc: 0.8388034629797131 | Test GradNorm: 0.003568111979905265\n",
      "Train Loss: 0.3086020346192969 | Train Acc: 0.8554517133956386 | Train GradNorm: 0.005313686233217775\n",
      "Epoch [132/500]\n",
      "Test Loss: 0.35403914860805635 | Test Acc: 0.8395464530301072 | Test GradNorm: 0.0013828588465705995\n",
      "Train Loss: 0.30547134785186875 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.0002991904341694507\n",
      "Epoch [133/500]\n",
      "Test Loss: 0.37607374234197677 | Test Acc: 0.8329564543222638 | Test GradNorm: 0.02818502835303734\n",
      "Train Loss: 0.33160905815040614 | Train Acc: 0.8398753894080997 | Train GradNorm: 0.03366484428755665\n",
      "Epoch [134/500]\n",
      "Test Loss: 0.3725931041208097 | Test Acc: 0.8278847396304432 | Test GradNorm: 0.02837724955307093\n",
      "Train Loss: 0.32105702543127906 | Train Acc: 0.8467289719626169 | Train GradNorm: 0.023099155492231946\n",
      "Epoch [135/500]\n",
      "Test Loss: 0.36561424701952305 | Test Acc: 0.8319550329499935 | Test GradNorm: 0.01934924202725417\n",
      "Train Loss: 0.3152333671577036 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.015257429437907685\n",
      "Epoch [136/500]\n",
      "Test Loss: 0.3628208190446965 | Test Acc: 0.836832924150407 | Test GradNorm: 0.012108260676582265\n",
      "Train Loss: 0.3166588815024911 | Train Acc: 0.8504672897196262 | Train GradNorm: 0.015374425706953354\n",
      "Epoch [137/500]\n",
      "Test Loss: 0.35541836533816146 | Test Acc: 0.8382866003359607 | Test GradNorm: 0.004117093135928187\n",
      "Train Loss: 0.30590376922299006 | Train Acc: 0.8616822429906542 | Train GradNorm: 0.0019886185270996176\n",
      "Epoch [138/500]\n",
      "Test Loss: 0.3536215989000827 | Test Acc: 0.8395787569453418 | Test GradNorm: 0.0005865224329132966\n",
      "Train Loss: 0.3045347607419653 | Train Acc: 0.8573208722741433 | Train GradNorm: 5.343437963788992e-05\n",
      "Epoch [139/500]\n",
      "Test Loss: 0.46122337757844245 | Test Acc: 0.8123142524874015 | Test GradNorm: 0.11094702534106447\n",
      "Train Loss: 0.4228738233503248 | Train Acc: 0.8174454828660436 | Train GradNorm: 0.12294363681113492\n",
      "Epoch [140/500]\n",
      "Test Loss: 0.3832153950452871 | Test Acc: 0.8200671921436878 | Test GradNorm: 0.04343298532764658\n",
      "Train Loss: 0.3304400748729405 | Train Acc: 0.8342679127725857 | Train GradNorm: 0.03737925882900009\n",
      "Epoch [141/500]\n",
      "Test Loss: 0.383484156382082 | Test Acc: 0.8194857216694663 | Test GradNorm: 0.04381826845643102\n",
      "Train Loss: 0.33048942556790223 | Train Acc: 0.8361370716510903 | Train GradNorm: 0.037459851296508494\n",
      "Epoch [142/500]\n",
      "Test Loss: 0.37665263232376833 | Test Acc: 0.8325688073394495 | Test GradNorm: 0.029454048072229278\n",
      "Train Loss: 0.3331998624117933 | Train Acc: 0.8404984423676013 | Train GradNorm: 0.0356458161568777\n",
      "Epoch [143/500]\n",
      "Test Loss: 0.4211418576877907 | Test Acc: 0.8185812120428996 | Test GradNorm: 0.07664169645793843\n",
      "Train Loss: 0.3817024460978166 | Train Acc: 0.8242990654205608 | Train GradNorm: 0.0873429567716202\n",
      "Epoch [144/500]\n",
      "Test Loss: 0.35803590873664753 | Test Acc: 0.8375759142008011 | Test GradNorm: 0.008506370828768952\n",
      "Train Loss: 0.30836744942942995 | Train Acc: 0.8535825545171339 | Train GradNorm: 0.0054421557540808895\n",
      "Epoch [145/500]\n",
      "Test Loss: 0.36204651465103266 | Test Acc: 0.8356376792867296 | Test GradNorm: 0.012259801725049348\n",
      "Train Loss: 0.3106317573930599 | Train Acc: 0.8523364485981308 | Train GradNorm: 0.008827246699027187\n",
      "Epoch [146/500]\n",
      "Test Loss: 0.3605382083718355 | Test Acc: 0.835605375371495 | Test GradNorm: 0.008745782868779223\n",
      "Train Loss: 0.3084275404344849 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.005774318862010544\n",
      "Epoch [147/500]\n",
      "Test Loss: 0.36142905849576845 | Test Acc: 0.8366391006589998 | Test GradNorm: 0.010887148046848356\n",
      "Train Loss: 0.30974725972729883 | Train Acc: 0.8535825545171339 | Train GradNorm: 0.007450478485281954\n",
      "Epoch [148/500]\n",
      "Test Loss: 0.36360865603893233 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.012259628001116008\n",
      "Train Loss: 0.317498834105681 | Train Acc: 0.8473520249221184 | Train GradNorm: 0.016144552999530885\n",
      "Epoch [149/500]\n",
      "Test Loss: 0.42121296194037533 | Test Acc: 0.7994249903088254 | Test GradNorm: 0.09469941539553689\n",
      "Train Loss: 0.3642775592778672 | Train Acc: 0.8236760124610591 | Train GradNorm: 0.086147428352309\n",
      "Epoch [150/500]\n",
      "Test Loss: 0.3789234393689413 | Test Acc: 0.8244282207003489 | Test GradNorm: 0.03628635617451045\n",
      "Train Loss: 0.3255681213647096 | Train Acc: 0.838006230529595 | Train GradNorm: 0.03077522864284644\n",
      "Epoch [151/500]\n",
      "Test Loss: 0.417742127354968 | Test Acc: 0.7996188138002326 | Test GradNorm: 0.09180040633676274\n",
      "Train Loss: 0.3618429662954539 | Train Acc: 0.8236760124610591 | Train GradNorm: 0.08460914462501053\n",
      "Epoch [152/500]\n",
      "Test Loss: 0.35961201234862245 | Test Acc: 0.8379312572683809 | Test GradNorm: 0.007352305189241046\n",
      "Train Loss: 0.31168326869247603 | Train Acc: 0.8523364485981308 | Train GradNorm: 0.009886302876758365\n",
      "Epoch [153/500]\n",
      "Test Loss: 0.3805689816923912 | Test Acc: 0.8310828272386613 | Test GradNorm: 0.033648254227138755\n",
      "Train Loss: 0.336724246761956 | Train Acc: 0.8373831775700935 | Train GradNorm: 0.04007980640526002\n",
      "Epoch [154/500]\n",
      "Test Loss: 0.3544544249903973 | Test Acc: 0.8385450316578369 | Test GradNorm: 0.000971631789865906\n",
      "Train Loss: 0.30546114629979887 | Train Acc: 0.8604361370716511 | Train GradNorm: 0.001629643586694403\n",
      "Epoch [155/500]\n",
      "Test Loss: 0.39715897334595734 | Test Acc: 0.8105375371495025 | Test GradNorm: 0.06247377991638208\n",
      "Train Loss: 0.34177340570504644 | Train Acc: 0.8330218068535825 | Train GradNorm: 0.055028988465140934\n",
      "Epoch [156/500]\n",
      "Test Loss: 0.37148231238906243 | Test Acc: 0.8299844941206874 | Test GradNorm: 0.025503860567622105\n",
      "Train Loss: 0.3178213960038045 | Train Acc: 0.8473520249221184 | Train GradNorm: 0.02015164936662092\n",
      "Epoch [157/500]\n",
      "Test Loss: 0.3740886360455353 | Test Acc: 0.8279493474609123 | Test GradNorm: 0.02934392882787441\n",
      "Train Loss: 0.3204266068096366 | Train Acc: 0.8473520249221184 | Train GradNorm: 0.02378238204747921\n",
      "Epoch [158/500]\n",
      "Test Loss: 0.3692500372277874 | Test Acc: 0.832471895593746 | Test GradNorm: 0.02123277754307753\n",
      "Train Loss: 0.31709511325119977 | Train Acc: 0.8504672897196262 | Train GradNorm: 0.016553539142444455\n",
      "Epoch [159/500]\n",
      "Test Loss: 0.35419153363936623 | Test Acc: 0.8391265021320584 | Test GradNorm: 0.0006523843304207116\n",
      "Train Loss: 0.30440703954202236 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.0009017751525859754\n",
      "Epoch [160/500]\n",
      "Test Loss: 0.37075643994140955 | Test Acc: 0.8284016022741957 | Test GradNorm: 0.021108843657260984\n",
      "Train Loss: 0.3160447995693868 | Train Acc: 0.8498442367601247 | Train GradNorm: 0.0173877363451047\n",
      "Epoch [161/500]\n",
      "Test Loss: 0.3561465940508204 | Test Acc: 0.8382866003359607 | Test GradNorm: 0.0013224455135259262\n",
      "Train Loss: 0.30447054417791575 | Train Acc: 0.8623052959501558 | Train GradNorm: 0.0002553118169935229\n",
      "Epoch [162/500]\n",
      "Test Loss: 0.35931625304214077 | Test Acc: 0.8363806693371236 | Test GradNorm: 0.00788620176423232\n",
      "Train Loss: 0.30722595490113036 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.005054138997406576\n",
      "Epoch [163/500]\n",
      "Test Loss: 0.35482101516771775 | Test Acc: 0.8381573846750227 | Test GradNorm: 0.0007346608003469423\n",
      "Train Loss: 0.30379542244767904 | Train Acc: 0.8610591900311526 | Train GradNorm: 3.2304833667688425e-05\n",
      "Epoch [164/500]\n",
      "Test Loss: 0.3557456372052428 | Test Acc: 0.8388034629797131 | Test GradNorm: 0.0006386247828225885\n",
      "Train Loss: 0.3043694193181106 | Train Acc: 0.864797507788162 | Train GradNorm: 0.00020764420329681403\n",
      "Epoch [165/500]\n",
      "Test Loss: 0.35465138784415906 | Test Acc: 0.8394818451996382 | Test GradNorm: 0.000448268545160418\n",
      "Train Loss: 0.3037609902194269 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.00019387786029979778\n",
      "Epoch [166/500]\n",
      "Test Loss: 0.3845622754299395 | Test Acc: 0.831308954645303 | Test GradNorm: 0.03435666480216878\n",
      "Train Loss: 0.33840863394076554 | Train Acc: 0.8423676012461059 | Train GradNorm: 0.040835138522101105\n",
      "Epoch [167/500]\n",
      "Test Loss: 0.3554267692512384 | Test Acc: 0.8392557177929965 | Test GradNorm: 0.0005960223454777998\n",
      "Train Loss: 0.303697041429274 | Train Acc: 0.8560747663551402 | Train GradNorm: 3.965264321083911e-05\n",
      "Epoch [168/500]\n",
      "Test Loss: 0.35662522885480263 | Test Acc: 0.8386419434035405 | Test GradNorm: 0.002955823950435723\n",
      "Train Loss: 0.3072745528495186 | Train Acc: 0.8535825545171339 | Train GradNorm: 0.004529776940772906\n",
      "Epoch [169/500]\n",
      "Test Loss: 0.373250652419489 | Test Acc: 0.8278847396304432 | Test GradNorm: 0.026880604435467432\n",
      "Train Loss: 0.3187757929111951 | Train Acc: 0.8454828660436137 | Train GradNorm: 0.02213202641100595\n",
      "Epoch [170/500]\n",
      "Test Loss: 0.3580467490321612 | Test Acc: 0.8389649825558858 | Test GradNorm: 0.004801817031841841\n",
      "Train Loss: 0.3087977566760662 | Train Acc: 0.8535825545171339 | Train GradNorm: 0.00686175865259009\n",
      "Epoch [171/500]\n",
      "Test Loss: 0.3575370853533252 | Test Acc: 0.8380604729293191 | Test GradNorm: 0.003052437256606382\n",
      "Train Loss: 0.3071458251932879 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.004644312928398766\n",
      "Epoch [172/500]\n",
      "Test Loss: 0.3560283837332631 | Test Acc: 0.8393203256234656 | Test GradNorm: 0.0006615814597035668\n",
      "Train Loss: 0.3038172828151965 | Train Acc: 0.8623052959501558 | Train GradNorm: 6.809340000182145e-05\n",
      "Epoch [173/500]\n",
      "Test Loss: 0.3770494392218801 | Test Acc: 0.8284662101046647 | Test GradNorm: 0.030285677922892325\n",
      "Train Loss: 0.32149487087381856 | Train Acc: 0.8504672897196262 | Train GradNorm: 0.02426697964074003\n",
      "Epoch [174/500]\n",
      "Test Loss: 0.36199041459165027 | Test Acc: 0.8357345910324331 | Test GradNorm: 0.008983073167398912\n",
      "Train Loss: 0.3077520706178928 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.006070355610702609\n",
      "Epoch [175/500]\n",
      "Test Loss: 0.35635213754575323 | Test Acc: 0.8385773355730715 | Test GradNorm: 0.000667687013860653\n",
      "Train Loss: 0.30354180233095873 | Train Acc: 0.8616822429906542 | Train GradNorm: 4.65142312831797e-05\n",
      "Epoch [176/500]\n",
      "Test Loss: 0.37730784823450036 | Test Acc: 0.833150277813671 | Test GradNorm: 0.027089015182002397\n",
      "Train Loss: 0.3302718761139198 | Train Acc: 0.8398753894080997 | Train GradNorm: 0.03275088229483335\n",
      "Epoch [177/500]\n",
      "Test Loss: 0.35436918516154026 | Test Acc: 0.8393526295387 | Test GradNorm: 0.0004220269771990234\n",
      "Train Loss: 0.3035735878703255 | Train Acc: 0.8604361370716511 | Train GradNorm: 0.00024021445173850466\n",
      "Epoch [178/500]\n",
      "Test Loss: 0.3606372476003771 | Test Acc: 0.8382866003359607 | Test GradNorm: 0.008046543180390227\n",
      "Train Loss: 0.31174946293786954 | Train Acc: 0.8510903426791278 | Train GradNorm: 0.010991502820060579\n",
      "Epoch [179/500]\n",
      "Test Loss: 0.37261720350005284 | Test Acc: 0.83321488564414 | Test GradNorm: 0.02144354477112519\n",
      "Train Loss: 0.32444798291288823 | Train Acc: 0.8454828660436137 | Train GradNorm: 0.026045456388059957\n",
      "Epoch [180/500]\n",
      "Test Loss: 0.3654874641835922 | Test Acc: 0.836703708489469 | Test GradNorm: 0.013649107947986098\n",
      "Train Loss: 0.3166444391414152 | Train Acc: 0.8485981308411215 | Train GradNorm: 0.017083662385797727\n",
      "Epoch [181/500]\n",
      "Test Loss: 0.3581327144941161 | Test Acc: 0.8368975319808761 | Test GradNorm: 0.005563270059807796\n",
      "Train Loss: 0.3053643406871148 | Train Acc: 0.859190031152648 | Train GradNorm: 0.0030983209800520633\n",
      "Epoch [182/500]\n",
      "Test Loss: 0.3618930624613388 | Test Acc: 0.8370913554722832 | Test GradNorm: 0.009540028585161977\n",
      "Train Loss: 0.31293288665725394 | Train Acc: 0.8504672897196262 | Train GradNorm: 0.012542006150580492\n",
      "Epoch [183/500]\n",
      "Test Loss: 0.38103033773935563 | Test Acc: 0.8315350820519447 | Test GradNorm: 0.03217638839815308\n",
      "Train Loss: 0.33440820123681336 | Train Acc: 0.8373831775700935 | Train GradNorm: 0.03829080199194754\n",
      "Epoch [184/500]\n",
      "Test Loss: 0.38178434253085364 | Test Acc: 0.8191303786018865 | Test GradNorm: 0.036874759928390434\n",
      "Train Loss: 0.3260422861966782 | Train Acc: 0.8429906542056075 | Train GradNorm: 0.03223719932864749\n",
      "Epoch [185/500]\n",
      "Test Loss: 0.3611090866014069 | Test Acc: 0.8365744928285308 | Test GradNorm: 0.008178176807743739\n",
      "Train Loss: 0.30743301940400114 | Train Acc: 0.854828660436137 | Train GradNorm: 0.005427774280413176\n",
      "Epoch [186/500]\n",
      "Test Loss: 0.40526156017221276 | Test Acc: 0.8079532239307404 | Test GradNorm: 0.07172313214565045\n",
      "Train Loss: 0.34752522492350896 | Train Acc: 0.8348909657320872 | Train GradNorm: 0.06400105199744638\n",
      "Epoch [187/500]\n",
      "Test Loss: 0.3986524325903178 | Test Acc: 0.8264310634448895 | Test GradNorm: 0.050087331202138756\n",
      "Train Loss: 0.35296598245977123 | Train Acc: 0.838006230529595 | Train GradNorm: 0.05807974903197668\n",
      "Epoch [188/500]\n",
      "Test Loss: 0.4158052130723315 | Test Acc: 0.8199702803979843 | Test GradNorm: 0.068138074809078\n",
      "Train Loss: 0.3700361179862524 | Train Acc: 0.8230529595015577 | Train GradNorm: 0.07692442828035986\n",
      "Epoch [189/500]\n",
      "Test Loss: 0.3758843567339389 | Test Acc: 0.8272386613257526 | Test GradNorm: 0.029423640833402327\n",
      "Train Loss: 0.3199205473883845 | Train Acc: 0.8442367601246106 | Train GradNorm: 0.0241407405753998\n",
      "Epoch [190/500]\n",
      "Test Loss: 0.36979612458556294 | Test Acc: 0.8320519446956971 | Test GradNorm: 0.020863079101930172\n",
      "Train Loss: 0.31490930278254897 | Train Acc: 0.8504672897196262 | Train GradNorm: 0.01592636909309941\n",
      "Epoch [191/500]\n",
      "Test Loss: 0.3559211880737482 | Test Acc: 0.8393849334539346 | Test GradNorm: 0.0017813083346316435\n",
      "Train Loss: 0.3034257665714715 | Train Acc: 0.8629283489096573 | Train GradNorm: 0.0003621058736129485\n",
      "Epoch [192/500]\n",
      "Test Loss: 0.3598730672877906 | Test Acc: 0.8367360124047034 | Test GradNorm: 0.0058656626100442215\n",
      "Train Loss: 0.30545181189262005 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.0032096176523331924\n",
      "Epoch [193/500]\n",
      "Test Loss: 0.3572543144360736 | Test Acc: 0.8378989533531463 | Test GradNorm: 0.0010719913110560364\n",
      "Train Loss: 0.3048922566268625 | Train Acc: 0.8604361370716511 | Train GradNorm: 0.0014679638341760194\n",
      "Epoch [194/500]\n",
      "Test Loss: 0.3658612166838427 | Test Acc: 0.836832924150407 | Test GradNorm: 0.013408618372415294\n",
      "Train Loss: 0.3167846985151409 | Train Acc: 0.8510903426791278 | Train GradNorm: 0.01732686959881492\n",
      "Epoch [195/500]\n",
      "Test Loss: 0.4401155240050366 | Test Acc: 0.7892169530947151 | Test GradNorm: 0.11789362956225881\n",
      "Train Loss: 0.38155775853032375 | Train Acc: 0.8137071651090343 | Train GradNorm: 0.11208377741113083\n",
      "Epoch [196/500]\n",
      "Test Loss: 0.35675841383753426 | Test Acc: 0.8405801783176121 | Test GradNorm: 0.0015643518311223845\n",
      "Train Loss: 0.3048122312295503 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.0023824245277838554\n",
      "Epoch [197/500]\n",
      "Test Loss: 0.39181488709131196 | Test Acc: 0.8144786148081148 | Test GradNorm: 0.05179374068543213\n",
      "Train Loss: 0.3346090144436205 | Train Acc: 0.8392523364485981 | Train GradNorm: 0.045736832983376226\n",
      "Epoch [198/500]\n",
      "Test Loss: 0.3565227731543632 | Test Acc: 0.839772580436749 | Test GradNorm: 0.0005484850309209724\n",
      "Train Loss: 0.3032942247260348 | Train Acc: 0.8604361370716511 | Train GradNorm: 0.0004209353885116386\n",
      "Epoch [199/500]\n",
      "Test Loss: 0.3569143663997134 | Test Acc: 0.8395787569453418 | Test GradNorm: 0.0009153740209226328\n",
      "Train Loss: 0.30448452739727394 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.0014966607941443145\n",
      "Epoch [200/500]\n",
      "Test Loss: 0.36536812680132136 | Test Acc: 0.8347331696601629 | Test GradNorm: 0.013521067306159117\n",
      "Train Loss: 0.31000179657914884 | Train Acc: 0.8523364485981308 | Train GradNorm: 0.009368351597212905\n",
      "Epoch [201/500]\n",
      "Test Loss: 0.36695999829574927 | Test Acc: 0.8323749838480424 | Test GradNorm: 0.014267374141943524\n",
      "Train Loss: 0.3105703287296643 | Train Acc: 0.8504672897196262 | Train GradNorm: 0.01025663862799036\n",
      "Epoch [202/500]\n",
      "Test Loss: 0.3591961266334357 | Test Acc: 0.8389326786406512 | Test GradNorm: 0.004287657011352508\n",
      "Train Loss: 0.30758383998116673 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.006272704575230218\n",
      "Epoch [203/500]\n",
      "Test Loss: 0.36181074046668643 | Test Acc: 0.8359930223543094 | Test GradNorm: 0.0075625326231453585\n",
      "Train Loss: 0.3062584458773035 | Train Acc: 0.854828660436137 | Train GradNorm: 0.004739645914417803\n",
      "Epoch [204/500]\n",
      "Test Loss: 0.37570330793904305 | Test Acc: 0.8267541025972348 | Test GradNorm: 0.027336712006051454\n",
      "Train Loss: 0.31874001737900526 | Train Acc: 0.8454828660436137 | Train GradNorm: 0.022226630021717694\n",
      "Epoch [205/500]\n",
      "Test Loss: 0.3571305092766042 | Test Acc: 0.839901796097687 | Test GradNorm: 0.0010604440550471324\n",
      "Train Loss: 0.30422642369270897 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.0017695608257941762\n",
      "Epoch [206/500]\n",
      "Test Loss: 0.35742630004322457 | Test Acc: 0.8405155704871431 | Test GradNorm: 0.002077131840475467\n",
      "Train Loss: 0.3056174648626002 | Train Acc: 0.8554517133956386 | Train GradNorm: 0.0034964484296143338\n",
      "Epoch [207/500]\n",
      "Test Loss: 0.37227272035716724 | Test Acc: 0.8294030236464659 | Test GradNorm: 0.02175796148286045\n",
      "Train Loss: 0.31561740732345117 | Train Acc: 0.8504672897196262 | Train GradNorm: 0.017385959646168323\n",
      "Epoch [208/500]\n",
      "Test Loss: 0.37745081403248304 | Test Acc: 0.834377826592583 | Test GradNorm: 0.0255900572683326\n",
      "Train Loss: 0.32847468577289124 | Train Acc: 0.8454828660436137 | Train GradNorm: 0.031101063825571852\n",
      "Epoch [209/500]\n",
      "Test Loss: 0.35772777327058725 | Test Acc: 0.8388680708101822 | Test GradNorm: 0.0011571821140930862\n",
      "Train Loss: 0.30289007146124725 | Train Acc: 0.864797507788162 | Train GradNorm: 0.00013715696342058174\n",
      "Epoch [210/500]\n",
      "Test Loss: 0.4644803849960566 | Test Acc: 0.8107636645561442 | Test GradNorm: 0.11036200207710027\n",
      "Train Loss: 0.4183660704868323 | Train Acc: 0.8161993769470405 | Train GradNorm: 0.12117959411460626\n",
      "Epoch [211/500]\n",
      "Test Loss: 0.3565941295913498 | Test Acc: 0.8393849334539346 | Test GradNorm: 0.0004757850425158247\n",
      "Train Loss: 0.30348742976159965 | Train Acc: 0.8598130841121495 | Train GradNorm: 0.0005788324321766002\n",
      "Epoch [212/500]\n",
      "Test Loss: 0.3710897727536186 | Test Acc: 0.8348946892363355 | Test GradNorm: 0.019045732363975637\n",
      "Train Loss: 0.32179978785558017 | Train Acc: 0.8442367601246106 | Train GradNorm: 0.02384416753783092\n",
      "Epoch [213/500]\n",
      "Test Loss: 0.3561435717486226 | Test Acc: 0.8381573846750227 | Test GradNorm: 0.0006045210440977411\n",
      "Train Loss: 0.3027993423735848 | Train Acc: 0.8641744548286604 | Train GradNorm: 4.886329684299407e-05\n",
      "Epoch [214/500]\n",
      "Test Loss: 0.36741261350991666 | Test Acc: 0.835540767541026 | Test GradNorm: 0.013848673658065502\n",
      "Train Loss: 0.31708817486533764 | Train Acc: 0.8517133956386292 | Train GradNorm: 0.018205666183935855\n",
      "Epoch [215/500]\n",
      "Test Loss: 0.3567175349618565 | Test Acc: 0.8384158159968988 | Test GradNorm: 0.0006415454418844961\n",
      "Train Loss: 0.3032499488895302 | Train Acc: 0.8616822429906542 | Train GradNorm: 0.0008816612238439237\n",
      "Epoch [216/500]\n",
      "Test Loss: 0.3604025302892675 | Test Acc: 0.8370590515570487 | Test GradNorm: 0.0049814913284334595\n",
      "Train Loss: 0.30452783140811873 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.0027763850193113647\n",
      "Epoch [217/500]\n",
      "Test Loss: 0.3587226513508026 | Test Acc: 0.839223413877762 | Test GradNorm: 0.0037038464984056546\n",
      "Train Loss: 0.3068560631368395 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.005462759824787037\n",
      "Epoch [218/500]\n",
      "Test Loss: 0.3559247481853368 | Test Acc: 0.839288021708231 | Test GradNorm: 0.000426134406309057\n",
      "Train Loss: 0.3029122760708622 | Train Acc: 0.8598130841121495 | Train GradNorm: 0.00019796783125824852\n",
      "Epoch [219/500]\n",
      "Test Loss: 0.35885609264078366 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.0015314988860474646\n",
      "Train Loss: 0.30292499321518734 | Train Acc: 0.8598130841121495 | Train GradNorm: 0.00031550308509397187\n",
      "Epoch [220/500]\n",
      "Test Loss: 0.3808874083333242 | Test Acc: 0.8237821423956584 | Test GradNorm: 0.03427592027533946\n",
      "Train Loss: 0.3228285810428258 | Train Acc: 0.8392523364485981 | Train GradNorm: 0.029164914051572406\n",
      "Epoch [221/500]\n",
      "Test Loss: 0.35671607839684666 | Test Acc: 0.8390618943015894 | Test GradNorm: 0.0006605099079483612\n",
      "Train Loss: 0.3034192478838388 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.00110700436312195\n",
      "Epoch [222/500]\n",
      "Test Loss: 0.3593242194323444 | Test Acc: 0.8402894430805014 | Test GradNorm: 0.0028967664387351774\n",
      "Train Loss: 0.3060700558571565 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.004499736740511436\n",
      "Epoch [223/500]\n",
      "Test Loss: 0.36177602621649474 | Test Acc: 0.8388357668949477 | Test GradNorm: 0.005854293896358264\n",
      "Train Loss: 0.30890827701661006 | Train Acc: 0.854828660436137 | Train GradNorm: 0.008162583798606866\n",
      "Epoch [224/500]\n",
      "Test Loss: 0.3573512209654672 | Test Acc: 0.8387065512340096 | Test GradNorm: 0.0005080327678353491\n",
      "Train Loss: 0.3027750942531865 | Train Acc: 0.864797507788162 | Train GradNorm: 0.00042082636760119564\n",
      "Epoch [225/500]\n",
      "Test Loss: 0.3724344793236743 | Test Acc: 0.833634836542189 | Test GradNorm: 0.019425428437874134\n",
      "Train Loss: 0.32221445143428845 | Train Acc: 0.8467289719626169 | Train GradNorm: 0.02442579376532648\n",
      "Epoch [226/500]\n",
      "Test Loss: 0.3613246634741728 | Test Acc: 0.836154541930482 | Test GradNorm: 0.0047288413418326534\n",
      "Train Loss: 0.30462008457703177 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.0025317418452151927\n",
      "Epoch [227/500]\n",
      "Test Loss: 0.35717158909957397 | Test Acc: 0.8396110608605764 | Test GradNorm: 0.0005647253773894649\n",
      "Train Loss: 0.30291931864874 | Train Acc: 0.8598130841121495 | Train GradNorm: 0.000607876997168514\n",
      "Epoch [228/500]\n",
      "Test Loss: 0.37864675379515367 | Test Acc: 0.8259142008011371 | Test GradNorm: 0.03197318307749799\n",
      "Train Loss: 0.32201325671012065 | Train Acc: 0.8423676012461059 | Train GradNorm: 0.026485635446790057\n",
      "Epoch [229/500]\n",
      "Test Loss: 0.376926495040111 | Test Acc: 0.826398759529655 | Test GradNorm: 0.02876507003934588\n",
      "Train Loss: 0.3200086935600598 | Train Acc: 0.8442367601246106 | Train GradNorm: 0.023814971841291586\n",
      "Epoch [230/500]\n",
      "Test Loss: 0.3866368059390625 | Test Acc: 0.8196149373304045 | Test GradNorm: 0.04170789449276644\n",
      "Train Loss: 0.32821056719543545 | Train Acc: 0.838006230529595 | Train GradNorm: 0.03585210581095184\n",
      "Epoch [231/500]\n",
      "Test Loss: 0.36900860410632147 | Test Acc: 0.8369298358961106 | Test GradNorm: 0.015661939742017947\n",
      "Train Loss: 0.31884671050865426 | Train Acc: 0.8498442367601247 | Train GradNorm: 0.02017458856001539\n",
      "Epoch [232/500]\n",
      "Test Loss: 0.3608537954190784 | Test Acc: 0.8388034629797131 | Test GradNorm: 0.006015682237424237\n",
      "Train Loss: 0.3088589147012753 | Train Acc: 0.8554517133956386 | Train GradNorm: 0.0085320908266605\n",
      "Epoch [233/500]\n",
      "Test Loss: 0.3565568509816153 | Test Acc: 0.8401279235043287 | Test GradNorm: 0.001045384166319989\n",
      "Train Loss: 0.3024546056676661 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.0001275590148820762\n",
      "Epoch [234/500]\n",
      "Test Loss: 0.43173042800733114 | Test Acc: 0.7943532756170048 | Test GradNorm: 0.10645193442521331\n",
      "Train Loss: 0.37058419571249895 | Train Acc: 0.8205607476635514 | Train GradNorm: 0.09922371183618992\n",
      "Epoch [235/500]\n",
      "Test Loss: 0.36518763415005967 | Test Acc: 0.8369944437265796 | Test GradNorm: 0.011100724384688915\n",
      "Train Loss: 0.31379836743766526 | Train Acc: 0.8535825545171339 | Train GradNorm: 0.014585957872786066\n",
      "Epoch [236/500]\n",
      "Test Loss: 0.4441154086608182 | Test Acc: 0.7886677865357281 | Test GradNorm: 0.12108798985949074\n",
      "Train Loss: 0.38146961483998326 | Train Acc: 0.8149532710280374 | Train GradNorm: 0.11385460437770652\n",
      "Epoch [237/500]\n",
      "Test Loss: 0.35810196825052887 | Test Acc: 0.8393849334539346 | Test GradNorm: 0.0004938467079362013\n",
      "Train Loss: 0.30242347690106375 | Train Acc: 0.8641744548286604 | Train GradNorm: 0.00017361546221530272\n",
      "Epoch [238/500]\n",
      "Test Loss: 0.35955934914985543 | Test Acc: 0.8388680708101822 | Test GradNorm: 0.0022161301695648185\n",
      "Train Loss: 0.3049927397203843 | Train Acc: 0.8554517133956386 | Train GradNorm: 0.0031926823545858057\n",
      "Epoch [239/500]\n",
      "Test Loss: 0.3631503779230788 | Test Acc: 0.8348300814058663 | Test GradNorm: 0.00784131540931907\n",
      "Train Loss: 0.30643032756924266 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.005187080304284342\n",
      "Epoch [240/500]\n",
      "Test Loss: 0.3569796283429042 | Test Acc: 0.8393526295387 | Test GradNorm: 0.00047299082968276884\n",
      "Train Loss: 0.30331751030383214 | Train Acc: 0.859190031152648 | Train GradNorm: 0.0006301755557008399\n",
      "Epoch [241/500]\n",
      "Test Loss: 0.35814246626040364 | Test Acc: 0.8386419434035405 | Test GradNorm: 0.0005241236463727644\n",
      "Train Loss: 0.30261890830329596 | Train Acc: 0.8604361370716511 | Train GradNorm: 0.00048694805467664625\n",
      "Epoch [242/500]\n",
      "Test Loss: 0.39453181624588735 | Test Acc: 0.8293384158159969 | Test GradNorm: 0.041719703566512535\n",
      "Train Loss: 0.34422223129837 | Train Acc: 0.8386292834890966 | Train GradNorm: 0.04874807454756598\n",
      "Epoch [243/500]\n",
      "Test Loss: 0.3590690021427471 | Test Acc: 0.8393849334539346 | Test GradNorm: 0.0007768681411961844\n",
      "Train Loss: 0.30245876180211784 | Train Acc: 0.8610591900311526 | Train GradNorm: 4.724214461766773e-05\n",
      "Epoch [244/500]\n",
      "Test Loss: 0.3633778336994957 | Test Acc: 0.8377374337769737 | Test GradNorm: 0.006264012133778173\n",
      "Train Loss: 0.30873864232860715 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.00838891790032471\n",
      "Epoch [245/500]\n",
      "Test Loss: 0.3578294820017209 | Test Acc: 0.8390295903863548 | Test GradNorm: 0.0007009011378391072\n",
      "Train Loss: 0.3029137773384845 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.00013805916412345425\n",
      "Epoch [246/500]\n",
      "Test Loss: 0.3600708664034777 | Test Acc: 0.8365098849980618 | Test GradNorm: 0.002986150557278679\n",
      "Train Loss: 0.3030075349138936 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.0011697499110458422\n",
      "Epoch [247/500]\n",
      "Test Loss: 0.37687862239329617 | Test Acc: 0.8333764052203128 | Test GradNorm: 0.024576433051685432\n",
      "Train Loss: 0.32645893479022114 | Train Acc: 0.8473520249221184 | Train GradNorm: 0.030184353560080524\n",
      "Epoch [248/500]\n",
      "Test Loss: 0.37662298535872635 | Test Acc: 0.8268833182581729 | Test GradNorm: 0.026096484636240678\n",
      "Train Loss: 0.31772796963635147 | Train Acc: 0.8454828660436137 | Train GradNorm: 0.021676393511457608\n",
      "Epoch [249/500]\n",
      "Test Loss: 0.36522170485272226 | Test Acc: 0.8363483654218892 | Test GradNorm: 0.009190475737602042\n",
      "Train Loss: 0.31173587818670945 | Train Acc: 0.8498442367601247 | Train GradNorm: 0.011898161559537079\n",
      "Epoch [250/500]\n",
      "Test Loss: 0.3662457777548814 | Test Acc: 0.8339255717792996 | Test GradNorm: 0.013159695922872148\n",
      "Train Loss: 0.3093442094655662 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.009947418554619936\n",
      "Epoch [251/500]\n",
      "Test Loss: 0.3573445487283855 | Test Acc: 0.8389649825558858 | Test GradNorm: 0.000599780767492396\n",
      "Train Loss: 0.3026974715050643 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.0007789509404806818\n",
      "Epoch [252/500]\n",
      "Test Loss: 0.35822114656476933 | Test Acc: 0.8385450316578369 | Test GradNorm: 0.0011690351463808455\n",
      "Train Loss: 0.3033677163292793 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.0016923712010064851\n",
      "Epoch [253/500]\n",
      "Test Loss: 0.3575504807235084 | Test Acc: 0.8389003747254167 | Test GradNorm: 0.0008605105157610841\n",
      "Train Loss: 0.30222318722728997 | Train Acc: 0.8660436137071651 | Train GradNorm: 5.7584264733099886e-05\n",
      "Epoch [254/500]\n",
      "Test Loss: 0.36132304151177363 | Test Acc: 0.8370913554722832 | Test GradNorm: 0.004162915923345194\n",
      "Train Loss: 0.3065419227068987 | Train Acc: 0.8535825545171339 | Train GradNorm: 0.005833380092995007\n",
      "Epoch [255/500]\n",
      "Test Loss: 0.360007063946204 | Test Acc: 0.8388680708101822 | Test GradNorm: 0.003912544690856369\n",
      "Train Loss: 0.3068984575693218 | Train Acc: 0.8523364485981308 | Train GradNorm: 0.0059974185934853945\n",
      "Epoch [256/500]\n",
      "Test Loss: 0.3571999274453073 | Test Acc: 0.8388034629797131 | Test GradNorm: 0.0007889970781928135\n",
      "Train Loss: 0.30243431403585125 | Train Acc: 0.8579439252336448 | Train GradNorm: 8.901457680187846e-05\n",
      "Epoch [257/500]\n",
      "Test Loss: 0.3738579507876939 | Test Acc: 0.8273032691562218 | Test GradNorm: 0.023299578786046063\n",
      "Train Loss: 0.31554456454464125 | Train Acc: 0.8510903426791278 | Train GradNorm: 0.01890049269876382\n",
      "Epoch [258/500]\n",
      "Test Loss: 0.3577057213753565 | Test Acc: 0.8389649825558858 | Test GradNorm: 0.0004431083909922962\n",
      "Train Loss: 0.30200815899965916 | Train Acc: 0.8610591900311526 | Train GradNorm: 0.00025629816573767154\n",
      "Epoch [259/500]\n",
      "Test Loss: 0.36145464527169835 | Test Acc: 0.8379312572683809 | Test GradNorm: 0.0056835906149449505\n",
      "Train Loss: 0.30811413632741536 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.008115006719151283\n",
      "Epoch [260/500]\n",
      "Test Loss: 0.3654064434714241 | Test Acc: 0.8370267476418142 | Test GradNorm: 0.010259178800707229\n",
      "Train Loss: 0.3125248762680591 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.01363563356123675\n",
      "Epoch [261/500]\n",
      "Test Loss: 0.3767147029445192 | Test Acc: 0.8342163070164104 | Test GradNorm: 0.021971730364068583\n",
      "Train Loss: 0.3236368270735465 | Train Acc: 0.8473520249221184 | Train GradNorm: 0.026744987365375884\n",
      "Epoch [262/500]\n",
      "Test Loss: 0.3585135036767635 | Test Acc: 0.8390941982168238 | Test GradNorm: 0.0010456962498855656\n",
      "Train Loss: 0.3020248773567871 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.00012104204014874697\n",
      "Epoch [263/500]\n",
      "Test Loss: 0.3673484874357156 | Test Acc: 0.83321488564414 | Test GradNorm: 0.013079050914368298\n",
      "Train Loss: 0.3087977557754307 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.00972629613853872\n",
      "Epoch [264/500]\n",
      "Test Loss: 0.37492547461419573 | Test Acc: 0.8339901796097687 | Test GradNorm: 0.020733406707729368\n",
      "Train Loss: 0.3219938782705619 | Train Acc: 0.8442367601246106 | Train GradNorm: 0.025163924608523548\n",
      "Epoch [265/500]\n",
      "Test Loss: 0.3746064346469837 | Test Acc: 0.828240082698023 | Test GradNorm: 0.023263691868521556\n",
      "Train Loss: 0.315017939357153 | Train Acc: 0.8517133956386292 | Train GradNorm: 0.018573917640727574\n",
      "Epoch [266/500]\n",
      "Test Loss: 0.35885543479113224 | Test Acc: 0.8386419434035405 | Test GradNorm: 0.0011124192265238055\n",
      "Train Loss: 0.30325583006170265 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.0017937855576986676\n",
      "Epoch [267/500]\n",
      "Test Loss: 0.3976497987015721 | Test Acc: 0.813574105181548 | Test GradNorm: 0.05537922578926139\n",
      "Train Loss: 0.336456819382335 | Train Acc: 0.838006230529595 | Train GradNorm: 0.04906478869479085\n",
      "Epoch [268/500]\n",
      "Test Loss: 0.3936327549271 | Test Acc: 0.8289184649179481 | Test GradNorm: 0.04139117242946319\n",
      "Train Loss: 0.34290366156198476 | Train Acc: 0.8355140186915888 | Train GradNorm: 0.049120632733019164\n",
      "Epoch [269/500]\n",
      "Test Loss: 0.3595959871844203 | Test Acc: 0.8381896885902571 | Test GradNorm: 0.0010059072379875924\n",
      "Train Loss: 0.3032193208351882 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.0017236712779884998\n",
      "Epoch [270/500]\n",
      "Test Loss: 0.36102911037635477 | Test Acc: 0.8394172373691692 | Test GradNorm: 0.002014066899619786\n",
      "Train Loss: 0.3043124523743432 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.0031856537171473186\n",
      "Epoch [271/500]\n",
      "Test Loss: 0.39605825382882887 | Test Acc: 0.829467631476935 | Test GradNorm: 0.04177458141533522\n",
      "Train Loss: 0.3447770061989699 | Train Acc: 0.8386292834890966 | Train GradNorm: 0.04931911441172779\n",
      "Epoch [272/500]\n",
      "Test Loss: 0.3609165393528132 | Test Acc: 0.8397079726062798 | Test GradNorm: 0.0018928106962501888\n",
      "Train Loss: 0.3026139606323782 | Train Acc: 0.859190031152648 | Train GradNorm: 0.0005125398377256809\n",
      "Epoch [273/500]\n",
      "Test Loss: 0.36182731448673006 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.004419928338213181\n",
      "Train Loss: 0.30331945736467597 | Train Acc: 0.8604361370716511 | Train GradNorm: 0.0022070626035410167\n",
      "Epoch [274/500]\n",
      "Test Loss: 0.3616074501188673 | Test Acc: 0.8381250807597881 | Test GradNorm: 0.0037264050067610265\n",
      "Train Loss: 0.3039194235519104 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.0017446115952154022\n",
      "Epoch [275/500]\n",
      "Test Loss: 0.3625576656180137 | Test Acc: 0.8389649825558858 | Test GradNorm: 0.0058342752428200435\n",
      "Train Loss: 0.308327676543596 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.008430944718484653\n",
      "Epoch [276/500]\n",
      "Test Loss: 0.41594835771060196 | Test Acc: 0.8044644010854115 | Test GradNorm: 0.08175306381583893\n",
      "Train Loss: 0.352548796879012 | Train Acc: 0.8336448598130841 | Train GradNorm: 0.07285328516802479\n",
      "Epoch [277/500]\n",
      "Test Loss: 0.3606505347652984 | Test Acc: 0.838674247318775 | Test GradNorm: 0.002204609086785083\n",
      "Train Loss: 0.3022820926079284 | Train Acc: 0.8598130841121495 | Train GradNorm: 0.0006374128330017291\n",
      "Epoch [278/500]\n",
      "Test Loss: 0.3647858777554062 | Test Acc: 0.8386419434035405 | Test GradNorm: 0.0072322445959973536\n",
      "Train Loss: 0.3097560007135772 | Train Acc: 0.8554517133956386 | Train GradNorm: 0.010000643083954788\n",
      "Epoch [279/500]\n",
      "Test Loss: 0.3612664610519944 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.0033015551211967352\n",
      "Train Loss: 0.30267804473477455 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.0014161566799198069\n",
      "Epoch [280/500]\n",
      "Test Loss: 0.37813232875971076 | Test Acc: 0.8262372399534824 | Test GradNorm: 0.026565455337790467\n",
      "Train Loss: 0.317141456339936 | Train Acc: 0.84797507788162 | Train GradNorm: 0.021729133642056597\n",
      "Epoch [281/500]\n",
      "Test Loss: 0.38120019321236087 | Test Acc: 0.8246866520222251 | Test GradNorm: 0.03231594221977729\n",
      "Train Loss: 0.3204978429837723 | Train Acc: 0.8467289719626169 | Train GradNorm: 0.026610440463355298\n",
      "Epoch [282/500]\n",
      "Test Loss: 0.36558979880677733 | Test Acc: 0.8364775810828272 | Test GradNorm: 0.008258010255234507\n",
      "Train Loss: 0.3103884760216717 | Train Acc: 0.8517133956386292 | Train GradNorm: 0.01094353973597877\n",
      "Epoch [283/500]\n",
      "Test Loss: 0.3623768888558346 | Test Acc: 0.8351854244734462 | Test GradNorm: 0.0026423127245960402\n",
      "Train Loss: 0.30295369899971564 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.0009990696957609645\n",
      "Epoch [284/500]\n",
      "Test Loss: 0.3639567160163803 | Test Acc: 0.8370590515570487 | Test GradNorm: 0.005519564663645724\n",
      "Train Loss: 0.30783914010132774 | Train Acc: 0.8554517133956386 | Train GradNorm: 0.007386804179113781\n",
      "Epoch [285/500]\n",
      "Test Loss: 0.35850826908497857 | Test Acc: 0.8390618943015894 | Test GradNorm: 0.0004793554272738603\n",
      "Train Loss: 0.30165428288277296 | Train Acc: 0.8610591900311526 | Train GradNorm: 0.00011103613274746254\n",
      "Epoch [286/500]\n",
      "Test Loss: 0.4346212783791391 | Test Acc: 0.7930611190076238 | Test GradNorm: 0.10817881496049127\n",
      "Train Loss: 0.3718439587128386 | Train Acc: 0.8230529595015577 | Train GradNorm: 0.10093181693451231\n",
      "Epoch [287/500]\n",
      "Test Loss: 0.3596893236970316 | Test Acc: 0.8384158159968988 | Test GradNorm: 0.0030488464487277516\n",
      "Train Loss: 0.30256121074058795 | Train Acc: 0.8629283489096573 | Train GradNorm: 0.0012815354238367097\n",
      "Epoch [288/500]\n",
      "Test Loss: 0.35978460102325627 | Test Acc: 0.8394172373691692 | Test GradNorm: 0.0019352619266601804\n",
      "Train Loss: 0.3051399746098119 | Train Acc: 0.854828660436137 | Train GradNorm: 0.003341448643267944\n",
      "Epoch [289/500]\n",
      "Test Loss: 0.3585695462006382 | Test Acc: 0.8389003747254167 | Test GradNorm: 0.0008392938093642155\n",
      "Train Loss: 0.30162963074806265 | Train Acc: 0.8598130841121495 | Train GradNorm: 5.0769332004918574e-05\n",
      "Epoch [290/500]\n",
      "Test Loss: 0.37405044034407386 | Test Acc: 0.8303075332730326 | Test GradNorm: 0.02142137477379849\n",
      "Train Loss: 0.31407279409416966 | Train Acc: 0.8510903426791278 | Train GradNorm: 0.016902722422471594\n",
      "Epoch [291/500]\n",
      "Test Loss: 0.4245795681951387 | Test Acc: 0.8188073394495413 | Test GradNorm: 0.0736608849437051\n",
      "Train Loss: 0.3760814484817531 | Train Acc: 0.8255451713395638 | Train GradNorm: 0.08366943014998542\n",
      "Epoch [292/500]\n",
      "Test Loss: 0.35955173613444635 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.0008612863628113704\n",
      "Train Loss: 0.30155088858847834 | Train Acc: 0.864797507788162 | Train GradNorm: 2.8656665689964037e-05\n",
      "Epoch [293/500]\n",
      "Test Loss: 0.36441062485593606 | Test Acc: 0.8355084636257915 | Test GradNorm: 0.007834400941900627\n",
      "Train Loss: 0.3051080428511274 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.00513071219109135\n",
      "Epoch [294/500]\n",
      "Test Loss: 0.3708869833186351 | Test Acc: 0.8370590515570487 | Test GradNorm: 0.0149410744115662\n",
      "Train Loss: 0.3164524985746345 | Train Acc: 0.8492211838006231 | Train GradNorm: 0.01877913018437423\n",
      "Epoch [295/500]\n",
      "Test Loss: 0.36057853376959964 | Test Acc: 0.8394172373691692 | Test GradNorm: 0.0022487721528381996\n",
      "Train Loss: 0.3040821539201555 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.003465663220415583\n",
      "Epoch [296/500]\n",
      "Test Loss: 0.3611363906624954 | Test Acc: 0.8381573846750227 | Test GradNorm: 0.003171752947001339\n",
      "Train Loss: 0.30256099969640804 | Train Acc: 0.8604361370716511 | Train GradNorm: 0.0013767772360718117\n",
      "Epoch [297/500]\n",
      "Test Loss: 0.371333160853987 | Test Acc: 0.8366391006589998 | Test GradNorm: 0.016785815837570866\n",
      "Train Loss: 0.3185906552742798 | Train Acc: 0.8454828660436137 | Train GradNorm: 0.021277170317787742\n",
      "Epoch [298/500]\n",
      "Test Loss: 0.3626377108464223 | Test Acc: 0.836154541930482 | Test GradNorm: 0.003035468914726885\n",
      "Train Loss: 0.30347096544730773 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.0013625303358557259\n",
      "Epoch [299/500]\n",
      "Test Loss: 0.385253594249446 | Test Acc: 0.8206486626179093 | Test GradNorm: 0.03540857013730306\n",
      "Train Loss: 0.32316660152379595 | Train Acc: 0.8423676012461059 | Train GradNorm: 0.030619968580835898\n",
      "Epoch [300/500]\n",
      "Test Loss: 0.36015728585498413 | Test Acc: 0.839288021708231 | Test GradNorm: 0.000613603483773265\n",
      "Train Loss: 0.30192486129398577 | Train Acc: 0.8616822429906542 | Train GradNorm: 0.00010821778495967849\n",
      "Epoch [301/500]\n",
      "Test Loss: 0.36700900909638123 | Test Acc: 0.8375759142008011 | Test GradNorm: 0.008958848162896951\n",
      "Train Loss: 0.31122643612468404 | Train Acc: 0.84797507788162 | Train GradNorm: 0.012182287243516485\n",
      "Epoch [302/500]\n",
      "Test Loss: 0.3651241174661526 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.007192134968041778\n",
      "Train Loss: 0.3092615576515664 | Train Acc: 0.8523364485981308 | Train GradNorm: 0.009920912937886195\n",
      "Epoch [303/500]\n",
      "Test Loss: 0.3636503408653342 | Test Acc: 0.8371559633027523 | Test GradNorm: 0.004201882174036084\n",
      "Train Loss: 0.30660441348811496 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.0058901612117418555\n",
      "Epoch [304/500]\n",
      "Test Loss: 0.39716595734401183 | Test Acc: 0.8127018994702158 | Test GradNorm: 0.05275016494666614\n",
      "Train Loss: 0.33453819037085625 | Train Acc: 0.838006230529595 | Train GradNorm: 0.04702188981966903\n",
      "Epoch [305/500]\n",
      "Test Loss: 0.3609292739259568 | Test Acc: 0.8384481199121333 | Test GradNorm: 0.0027812172250637254\n",
      "Train Loss: 0.3045523006939377 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.0041646966339056075\n",
      "Epoch [306/500]\n",
      "Test Loss: 0.42990243966964375 | Test Acc: 0.7961945987853728 | Test GradNorm: 0.10083826823378486\n",
      "Train Loss: 0.36571380225422495 | Train Acc: 0.8249221183800624 | Train GradNorm: 0.0933273365528818\n",
      "Epoch [307/500]\n",
      "Test Loss: 0.3611730725136166 | Test Acc: 0.8391265021320584 | Test GradNorm: 0.0015041360123033533\n",
      "Train Loss: 0.3044121351899698 | Train Acc: 0.8535825545171339 | Train GradNorm: 0.0025695484000505457\n",
      "Epoch [308/500]\n",
      "Test Loss: 0.36074151270968774 | Test Acc: 0.8372851789636904 | Test GradNorm: 0.0010224819373102175\n",
      "Train Loss: 0.3027842487622822 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.0016384206316668378\n",
      "Epoch [309/500]\n",
      "Test Loss: 0.3591740199729303 | Test Acc: 0.8391265021320584 | Test GradNorm: 0.0004212468505063301\n",
      "Train Loss: 0.3016027872370927 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.0002856087308370468\n",
      "Epoch [310/500]\n",
      "Test Loss: 0.3901469213855525 | Test Acc: 0.818355084636258 | Test GradNorm: 0.044764532161987114\n",
      "Train Loss: 0.32828222736741663 | Train Acc: 0.8417445482866044 | Train GradNorm: 0.037979885624243044\n",
      "Epoch [311/500]\n",
      "Test Loss: 0.3615928201176899 | Test Acc: 0.839223413877762 | Test GradNorm: 0.004027652009633429\n",
      "Train Loss: 0.3062545555249638 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.006046332189731446\n",
      "Epoch [312/500]\n",
      "Test Loss: 0.3589239897613993 | Test Acc: 0.8397402765215144 | Test GradNorm: 0.0005336833020044077\n",
      "Train Loss: 0.30290107266269944 | Train Acc: 0.8535825545171339 | Train GradNorm: 0.000310348952943026\n",
      "Epoch [313/500]\n",
      "Test Loss: 0.3657382143821964 | Test Acc: 0.8351531205582117 | Test GradNorm: 0.009180845792987743\n",
      "Train Loss: 0.30589310438904843 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.0064582941596154205\n",
      "Epoch [314/500]\n",
      "Test Loss: 0.3610036009845103 | Test Acc: 0.8389326786406512 | Test GradNorm: 0.0021756752793089433\n",
      "Train Loss: 0.30256014130940156 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.0007828171576026859\n",
      "Epoch [315/500]\n",
      "Test Loss: 0.3675385665285348 | Test Acc: 0.8344424344230521 | Test GradNorm: 0.01028235831921374\n",
      "Train Loss: 0.3065600456891067 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.007107470125474693\n",
      "Epoch [316/500]\n",
      "Test Loss: 0.36319245587419696 | Test Acc: 0.836219149760951 | Test GradNorm: 0.004264015386939302\n",
      "Train Loss: 0.3028809927078803 | Train Acc: 0.8604361370716511 | Train GradNorm: 0.0020904921966440355\n",
      "Epoch [317/500]\n",
      "Test Loss: 0.38606192635348185 | Test Acc: 0.8309859154929577 | Test GradNorm: 0.031455513762699984\n",
      "Train Loss: 0.33272258414136 | Train Acc: 0.8429906542056075 | Train GradNorm: 0.03798138426011832\n",
      "Epoch [318/500]\n",
      "Test Loss: 0.4054534210744406 | Test Acc: 0.8097299392686393 | Test GradNorm: 0.06599208103705252\n",
      "Train Loss: 0.34147072841755577 | Train Acc: 0.8361370716510903 | Train GradNorm: 0.057828637859300515\n",
      "Epoch [319/500]\n",
      "Test Loss: 0.36027659482463165 | Test Acc: 0.838674247318775 | Test GradNorm: 0.000570372038903289\n",
      "Train Loss: 0.3020884435787906 | Train Acc: 0.8610591900311526 | Train GradNorm: 0.000787192368502371\n",
      "Epoch [320/500]\n",
      "Test Loss: 0.3623128636630626 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.004684853895485135\n",
      "Train Loss: 0.3030834408502742 | Train Acc: 0.8616822429906542 | Train GradNorm: 0.002333898800039319\n",
      "Epoch [321/500]\n",
      "Test Loss: 0.35898558185215906 | Test Acc: 0.8385450316578369 | Test GradNorm: 0.0009931606045192934\n",
      "Train Loss: 0.3012359445044073 | Train Acc: 0.8629283489096573 | Train GradNorm: 8.19726290527138e-05\n",
      "Epoch [322/500]\n",
      "Test Loss: 0.36183751602070446 | Test Acc: 0.8372205711332213 | Test GradNorm: 0.0060459983447690565\n",
      "Train Loss: 0.30397014728081934 | Train Acc: 0.8616822429906542 | Train GradNorm: 0.0036351799299246795\n",
      "Epoch [323/500]\n",
      "Test Loss: 0.3695332392908276 | Test Acc: 0.8310828272386613 | Test GradNorm: 0.014571328075838691\n",
      "Train Loss: 0.3094684746006748 | Train Acc: 0.8510903426791278 | Train GradNorm: 0.011284770992099636\n",
      "Epoch [324/500]\n",
      "Test Loss: 0.361938463754852 | Test Acc: 0.836219149760951 | Test GradNorm: 0.004631445174908798\n",
      "Train Loss: 0.30365062162106016 | Train Acc: 0.8598130841121495 | Train GradNorm: 0.0025879095070085443\n",
      "Epoch [325/500]\n",
      "Test Loss: 0.35870047628315477 | Test Acc: 0.8393203256234656 | Test GradNorm: 0.0020042719976627755\n",
      "Train Loss: 0.30197038620469835 | Train Acc: 0.8598130841121495 | Train GradNorm: 0.0005805631628596234\n",
      "Epoch [326/500]\n",
      "Test Loss: 0.3584163746447904 | Test Acc: 0.8400633156738597 | Test GradNorm: 0.0010443631553619663\n",
      "Train Loss: 0.30356544857165424 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.0021418462988897174\n",
      "Epoch [327/500]\n",
      "Test Loss: 0.36489285650882686 | Test Acc: 0.8360576301847784 | Test GradNorm: 0.010184179160716183\n",
      "Train Loss: 0.30643133059552163 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.006778945063346453\n",
      "Epoch [328/500]\n",
      "Test Loss: 0.3623575812930718 | Test Acc: 0.8382866003359607 | Test GradNorm: 0.0052239786180616156\n",
      "Train Loss: 0.3067904065595835 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.007377265252322669\n",
      "Epoch [329/500]\n",
      "Test Loss: 0.36027942217373643 | Test Acc: 0.8391911099625274 | Test GradNorm: 0.0025191474267753176\n",
      "Train Loss: 0.3041679119396967 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.004025123632327623\n",
      "Epoch [330/500]\n",
      "Test Loss: 0.3616682192694923 | Test Acc: 0.8387388551492441 | Test GradNorm: 0.0024829784332931907\n",
      "Train Loss: 0.3040231435893349 | Train Acc: 0.8554517133956386 | Train GradNorm: 0.0037482696755366455\n",
      "Epoch [331/500]\n",
      "Test Loss: 0.3609933441221811 | Test Acc: 0.839223413877762 | Test GradNorm: 0.0015717342808933938\n",
      "Train Loss: 0.30324793884148177 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.0025789214569903376\n",
      "Epoch [332/500]\n",
      "Test Loss: 0.3828318537064977 | Test Acc: 0.8330210621527329 | Test GradNorm: 0.027510359913842358\n",
      "Train Loss: 0.3284601451449907 | Train Acc: 0.8436137071651091 | Train GradNorm: 0.033205500744272025\n",
      "Epoch [333/500]\n",
      "Test Loss: 0.36327059306093995 | Test Acc: 0.8380927768445535 | Test GradNorm: 0.004185545358735442\n",
      "Train Loss: 0.30291360610754486 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.0020925778599324135\n",
      "Epoch [334/500]\n",
      "Test Loss: 0.36581775296032654 | Test Acc: 0.8352177283886807 | Test GradNorm: 0.007476651894973335\n",
      "Train Loss: 0.3045666442240508 | Train Acc: 0.8554517133956386 | Train GradNorm: 0.004685721259788835\n",
      "Epoch [335/500]\n",
      "Test Loss: 0.37589027508361206 | Test Acc: 0.8351854244734462 | Test GradNorm: 0.018998278579357472\n",
      "Train Loss: 0.32080504803883503 | Train Acc: 0.8485981308411215 | Train GradNorm: 0.02384219979774472\n",
      "Epoch [336/500]\n",
      "Test Loss: 0.3611976196931809 | Test Acc: 0.838609639488306 | Test GradNorm: 0.0021250656503190696\n",
      "Train Loss: 0.301848862478389 | Train Acc: 0.8641744548286604 | Train GradNorm: 0.0006829335230616549\n",
      "Epoch [337/500]\n",
      "Test Loss: 0.3721235083380597 | Test Acc: 0.8304690528492054 | Test GradNorm: 0.01645475606677589\n",
      "Train Loss: 0.3101481923251313 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.012550581481911481\n",
      "Epoch [338/500]\n",
      "Test Loss: 0.3680029799249325 | Test Acc: 0.8334087091355472 | Test GradNorm: 0.010153672640261699\n",
      "Train Loss: 0.30636834808002744 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.007019469729289468\n",
      "Epoch [339/500]\n",
      "Test Loss: 0.381590673190607 | Test Acc: 0.8228130249386225 | Test GradNorm: 0.030187305868324524\n",
      "Train Loss: 0.3192608325922755 | Train Acc: 0.8448598130841122 | Train GradNorm: 0.0254406718763245\n",
      "Epoch [340/500]\n",
      "Test Loss: 0.38246129127872935 | Test Acc: 0.8231360640909678 | Test GradNorm: 0.03244203890331096\n",
      "Train Loss: 0.32032605686084187 | Train Acc: 0.8423676012461059 | Train GradNorm: 0.02745119992298657\n",
      "Epoch [341/500]\n",
      "Test Loss: 0.39004980830990094 | Test Acc: 0.8304367489339708 | Test GradNorm: 0.03514685678125732\n",
      "Train Loss: 0.3356658215655776 | Train Acc: 0.8367601246105919 | Train GradNorm: 0.04081566186637396\n",
      "Epoch [342/500]\n",
      "Test Loss: 0.38097571515855855 | Test Acc: 0.827012533919111 | Test GradNorm: 0.030707971394595298\n",
      "Train Loss: 0.32047298855586365 | Train Acc: 0.8461059190031153 | Train GradNorm: 0.025119478023792233\n",
      "Epoch [343/500]\n",
      "Test Loss: 0.361636048047217 | Test Acc: 0.8375759142008011 | Test GradNorm: 0.0034783739677874096\n",
      "Train Loss: 0.30229410614547625 | Train Acc: 0.8623052959501558 | Train GradNorm: 0.0016312650092758518\n",
      "Epoch [344/500]\n",
      "Test Loss: 0.3592349034610576 | Test Acc: 0.8393203256234656 | Test GradNorm: 0.00047294764768553827\n",
      "Train Loss: 0.30149102929697746 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.0004239195784985193\n",
      "Epoch [345/500]\n",
      "Test Loss: 0.38849543300874595 | Test Acc: 0.8181612611448508 | Test GradNorm: 0.04086875865920888\n",
      "Train Loss: 0.3263861058961696 | Train Acc: 0.8392523364485981 | Train GradNorm: 0.035696112570979974\n",
      "Epoch [346/500]\n",
      "Test Loss: 0.3604802005602099 | Test Acc: 0.8381250807597881 | Test GradNorm: 0.0005787549397584769\n",
      "Train Loss: 0.3017594940464369 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.00037449586941850553\n",
      "Epoch [347/500]\n",
      "Test Loss: 0.3871128374066277 | Test Acc: 0.8324395916785114 | Test GradNorm: 0.033009502214322736\n",
      "Train Loss: 0.3340618419257779 | Train Acc: 0.8417445482866044 | Train GradNorm: 0.03923632959449701\n",
      "Epoch [348/500]\n",
      "Test Loss: 0.36462278660979935 | Test Acc: 0.835540767541026 | Test GradNorm: 0.007221752283681959\n",
      "Train Loss: 0.30440437758281164 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.004584478364368436\n",
      "Epoch [349/500]\n",
      "Test Loss: 0.3899817470524378 | Test Acc: 0.8297906706292802 | Test GradNorm: 0.036983572304884145\n",
      "Train Loss: 0.33751611881980015 | Train Acc: 0.8404984423676013 | Train GradNorm: 0.04396901152366438\n",
      "Epoch [350/500]\n",
      "Test Loss: 0.36870613122549034 | Test Acc: 0.8326980230003876 | Test GradNorm: 0.01393570977646079\n",
      "Train Loss: 0.30819576862921505 | Train Acc: 0.8554517133956386 | Train GradNorm: 0.01003972709381475\n",
      "Epoch [351/500]\n",
      "Test Loss: 0.3650732280672397 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.007671538019038121\n",
      "Train Loss: 0.30969114595503977 | Train Acc: 0.8498442367601247 | Train GradNorm: 0.010736626208284524\n",
      "Epoch [352/500]\n",
      "Test Loss: 0.38141082409901883 | Test Acc: 0.8227161131929189 | Test GradNorm: 0.03015975507565359\n",
      "Train Loss: 0.31943287046302354 | Train Acc: 0.8454828660436137 | Train GradNorm: 0.02598562223569082\n",
      "Epoch [353/500]\n",
      "Test Loss: 0.36400134691413905 | Test Acc: 0.8366067967437654 | Test GradNorm: 0.004475592402148125\n",
      "Train Loss: 0.3037037889609594 | Train Acc: 0.8604361370716511 | Train GradNorm: 0.002682425394511852\n",
      "Epoch [354/500]\n",
      "Test Loss: 0.3827392126998452 | Test Acc: 0.823329887582375 | Test GradNorm: 0.03198573307852236\n",
      "Train Loss: 0.31958969909691365 | Train Acc: 0.8461059190031153 | Train GradNorm: 0.026715548906554667\n",
      "Epoch [355/500]\n",
      "Test Loss: 0.3666880507282842 | Test Acc: 0.8344424344230521 | Test GradNorm: 0.007910278892406369\n",
      "Train Loss: 0.304896414049326 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.005295575142572022\n",
      "Epoch [356/500]\n",
      "Test Loss: 0.3758999369355912 | Test Acc: 0.834313218762114 | Test GradNorm: 0.019241704811500792\n",
      "Train Loss: 0.3203784266702692 | Train Acc: 0.8467289719626169 | Train GradNorm: 0.023941740134706818\n",
      "Epoch [357/500]\n",
      "Test Loss: 0.36823196249699847 | Test Acc: 0.8338609639488306 | Test GradNorm: 0.009837377015667094\n",
      "Train Loss: 0.30589256076190907 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.006795566132939419\n",
      "Epoch [358/500]\n",
      "Test Loss: 0.3727933678671792 | Test Acc: 0.8305982685101434 | Test GradNorm: 0.016560511378028473\n",
      "Train Loss: 0.3101132442501594 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.012933498937188332\n",
      "Epoch [359/500]\n",
      "Test Loss: 0.41312774432711 | Test Acc: 0.8048843519834604 | Test GradNorm: 0.07085778374876654\n",
      "Train Loss: 0.34709310645324765 | Train Acc: 0.8317757009345794 | Train GradNorm: 0.06432822532747405\n",
      "Epoch [360/500]\n",
      "Test Loss: 0.36986134688548794 | Test Acc: 0.8331825817289056 | Test GradNorm: 0.010936836941304963\n",
      "Train Loss: 0.307082157320402 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.00797174897669833\n",
      "Epoch [361/500]\n",
      "Test Loss: 0.3880387809155725 | Test Acc: 0.8298875823749838 | Test GradNorm: 0.032527854367500206\n",
      "Train Loss: 0.3326463584247091 | Train Acc: 0.8392523364485981 | Train GradNorm: 0.038457098644574875\n",
      "Epoch [362/500]\n",
      "Test Loss: 0.36117705842145 | Test Acc: 0.8389649825558858 | Test GradNorm: 0.0017579867785761946\n",
      "Train Loss: 0.30146794656885934 | Train Acc: 0.8654205607476636 | Train GradNorm: 0.0004091049808625367\n",
      "Epoch [363/500]\n",
      "Test Loss: 0.38751079822254586 | Test Acc: 0.8309859154929577 | Test GradNorm: 0.0329761658550992\n",
      "Train Loss: 0.33416469956381 | Train Acc: 0.8423676012461059 | Train GradNorm: 0.03957582187504811\n",
      "Epoch [364/500]\n",
      "Test Loss: 0.40412051353492984 | Test Acc: 0.8088900374725416 | Test GradNorm: 0.06246750454286895\n",
      "Train Loss: 0.3396144287473858 | Train Acc: 0.8367601246105919 | Train GradNorm: 0.056164304246118825\n",
      "Epoch [365/500]\n",
      "Test Loss: 0.35960379523342423 | Test Acc: 0.8384804238273679 | Test GradNorm: 0.0005418973647080318\n",
      "Train Loss: 0.30215903974186953 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.0009495678624581697\n",
      "Epoch [366/500]\n",
      "Test Loss: 0.3864037674348012 | Test Acc: 0.831308954645303 | Test GradNorm: 0.03209034859751843\n",
      "Train Loss: 0.3325210346693769 | Train Acc: 0.8423676012461059 | Train GradNorm: 0.038187321694752335\n",
      "Epoch [367/500]\n",
      "Test Loss: 0.39872980658381085 | Test Acc: 0.8119912133350562 | Test GradNorm: 0.053486475096691424\n",
      "Train Loss: 0.3344004984703565 | Train Acc: 0.8404984423676013 | Train GradNorm: 0.04794483566063101\n",
      "Epoch [368/500]\n",
      "Test Loss: 0.379231204538314 | Test Acc: 0.8336025326269544 | Test GradNorm: 0.02304245299144175\n",
      "Train Loss: 0.3236074084130021 | Train Acc: 0.8454828660436137 | Train GradNorm: 0.028126330890579927\n",
      "Epoch [369/500]\n",
      "Test Loss: 0.38556026672984944 | Test Acc: 0.8227484171081535 | Test GradNorm: 0.036863789581980444\n",
      "Train Loss: 0.32228765447880786 | Train Acc: 0.8454828660436137 | Train GradNorm: 0.030496100906143467\n",
      "Epoch [370/500]\n",
      "Test Loss: 0.3660489493366981 | Test Acc: 0.83628375759142 | Test GradNorm: 0.008854733316390487\n",
      "Train Loss: 0.30521205559165066 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.005668461675621352\n",
      "Epoch [371/500]\n",
      "Test Loss: 0.3711955044762541 | Test Acc: 0.8324072877632769 | Test GradNorm: 0.014299755862152774\n",
      "Train Loss: 0.30872030017395374 | Train Acc: 0.8523364485981308 | Train GradNorm: 0.01056842268430574\n",
      "Epoch [372/500]\n",
      "Test Loss: 0.38588409590507033 | Test Acc: 0.8331179738984364 | Test GradNorm: 0.02961508213199778\n",
      "Train Loss: 0.33166378294361915 | Train Acc: 0.8448598130841122 | Train GradNorm: 0.03580468487491496\n",
      "Epoch [373/500]\n",
      "Test Loss: 0.40083204118000315 | Test Acc: 0.8109897919627859 | Test GradNorm: 0.05658872340545148\n",
      "Train Loss: 0.33574885741647514 | Train Acc: 0.8398753894080997 | Train GradNorm: 0.049836433488808035\n",
      "Epoch [374/500]\n",
      "Test Loss: 0.3772341016575488 | Test Acc: 0.8347977774906319 | Test GradNorm: 0.020510731626318383\n",
      "Train Loss: 0.3225243501883491 | Train Acc: 0.8454828660436137 | Train GradNorm: 0.025802484707205555\n",
      "Epoch [375/500]\n",
      "Test Loss: 0.36415924844817277 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.004173994475176465\n",
      "Train Loss: 0.30578184356310273 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.006045153624776212\n",
      "Epoch [376/500]\n",
      "Test Loss: 0.387918242637379 | Test Acc: 0.8204225352112676 | Test GradNorm: 0.03816579528898561\n",
      "Train Loss: 0.32399998473413694 | Train Acc: 0.8423676012461059 | Train GradNorm: 0.03251728321112972\n",
      "Epoch [377/500]\n",
      "Test Loss: 0.3687586165762007 | Test Acc: 0.8368006202351725 | Test GradNorm: 0.010838973852790033\n",
      "Train Loss: 0.31184724773435396 | Train Acc: 0.8504672897196262 | Train GradNorm: 0.014138101959960287\n",
      "Epoch [378/500]\n",
      "Test Loss: 0.36071763961214665 | Test Acc: 0.8395141491148728 | Test GradNorm: 0.0004812869702322223\n",
      "Train Loss: 0.3011507694680098 | Train Acc: 0.8629283489096573 | Train GradNorm: 0.0001115388286177073\n",
      "Epoch [379/500]\n",
      "Test Loss: 0.37968808520652797 | Test Acc: 0.8336671404574234 | Test GradNorm: 0.02342672753211515\n",
      "Train Loss: 0.32388909333361715 | Train Acc: 0.8448598130841122 | Train GradNorm: 0.028468822595802173\n",
      "Epoch [380/500]\n",
      "Test Loss: 0.36765219626683554 | Test Acc: 0.8376082181160357 | Test GradNorm: 0.009160346937393656\n",
      "Train Loss: 0.3106706545324142 | Train Acc: 0.8504672897196262 | Train GradNorm: 0.012285599899297905\n",
      "Epoch [381/500]\n",
      "Test Loss: 0.3610941706737987 | Test Acc: 0.8392557177929965 | Test GradNorm: 0.001153697532499994\n",
      "Train Loss: 0.3008757723365936 | Train Acc: 0.8610591900311526 | Train GradNorm: 0.00014400811470641697\n",
      "Epoch [382/500]\n",
      "Test Loss: 0.4253768993419199 | Test Acc: 0.7985204806822587 | Test GradNorm: 0.09198104616053089\n",
      "Train Loss: 0.3592602719654022 | Train Acc: 0.8267912772585669 | Train GradNorm: 0.08436578298139973\n",
      "Epoch [383/500]\n",
      "Test Loss: 0.3673042125185428 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.009460736882227882\n",
      "Train Loss: 0.31110834592734676 | Train Acc: 0.8523364485981308 | Train GradNorm: 0.012760390160034742\n",
      "Epoch [384/500]\n",
      "Test Loss: 0.37033069527835044 | Test Acc: 0.8321811603566353 | Test GradNorm: 0.014396827780395818\n",
      "Train Loss: 0.3083762688959312 | Train Acc: 0.854828660436137 | Train GradNorm: 0.010604645220093962\n",
      "Epoch [385/500]\n",
      "Test Loss: 0.3926430590770891 | Test Acc: 0.8299521902054529 | Test GradNorm: 0.037621067976372324\n",
      "Train Loss: 0.3378878619834633 | Train Acc: 0.8367601246105919 | Train GradNorm: 0.04410569560561354\n",
      "Epoch [386/500]\n",
      "Test Loss: 0.4361270854157729 | Test Acc: 0.7918658741439463 | Test GradNorm: 0.10557746953063628\n",
      "Train Loss: 0.36982431804027904 | Train Acc: 0.8180685358255452 | Train GradNorm: 0.09928762640314562\n",
      "Epoch [387/500]\n",
      "Test Loss: 0.4023208738757039 | Test Acc: 0.8106667528104406 | Test GradNorm: 0.05884233732980007\n",
      "Train Loss: 0.33776966827532856 | Train Acc: 0.8367601246105919 | Train GradNorm: 0.0528151081883773\n",
      "Epoch [388/500]\n",
      "Test Loss: 0.3603966642158243 | Test Acc: 0.839158806047293 | Test GradNorm: 0.0007851125244972601\n",
      "Train Loss: 0.3019894675899228 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.0013343737434178132\n",
      "Epoch [389/500]\n",
      "Test Loss: 0.36063277568429797 | Test Acc: 0.8393203256234656 | Test GradNorm: 0.0004369056588037894\n",
      "Train Loss: 0.30128845723137937 | Train Acc: 0.859190031152648 | Train GradNorm: 0.0005597402818110933\n",
      "Epoch [390/500]\n",
      "Test Loss: 0.3804838167714629 | Test Acc: 0.8275940043933324 | Test GradNorm: 0.02697844441825511\n",
      "Train Loss: 0.31680740086776926 | Train Acc: 0.8492211838006231 | Train GradNorm: 0.022124291787754755\n",
      "Epoch [391/500]\n",
      "Test Loss: 0.37362807699465284 | Test Acc: 0.8310182194081923 | Test GradNorm: 0.016629659392377136\n",
      "Train Loss: 0.30995954393018654 | Train Acc: 0.8504672897196262 | Train GradNorm: 0.012690937951583802\n",
      "Epoch [392/500]\n",
      "Test Loss: 0.3733985404359538 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.014726206682966321\n",
      "Train Loss: 0.31631781329774666 | Train Acc: 0.8492211838006231 | Train GradNorm: 0.01864570303190365\n",
      "Epoch [393/500]\n",
      "Test Loss: 0.36853061192915865 | Test Acc: 0.8347008657449283 | Test GradNorm: 0.00988267505859227\n",
      "Train Loss: 0.3055724809239752 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.006627630413991793\n",
      "Epoch [394/500]\n",
      "Test Loss: 0.4188000917368168 | Test Acc: 0.8236852306499548 | Test GradNorm: 0.06394364481256765\n",
      "Train Loss: 0.3673592798445107 | Train Acc: 0.8330218068535825 | Train GradNorm: 0.07362927145715713\n",
      "Epoch [395/500]\n",
      "Test Loss: 0.3647850957897287 | Test Acc: 0.8373497867941595 | Test GradNorm: 0.005305056288539053\n",
      "Train Loss: 0.3028960861775442 | Train Acc: 0.8623052959501558 | Train GradNorm: 0.002888271939288905\n",
      "Epoch [396/500]\n",
      "Test Loss: 0.428316528465595 | Test Acc: 0.7957423439720894 | Test GradNorm: 0.09281626298917035\n",
      "Train Loss: 0.3609962983247628 | Train Acc: 0.821183800623053 | Train GradNorm: 0.08616824743300265\n",
      "Epoch [397/500]\n",
      "Test Loss: 0.3872630536055814 | Test Acc: 0.8325041995089805 | Test GradNorm: 0.031105287572078703\n",
      "Train Loss: 0.33229199126602854 | Train Acc: 0.8429906542056075 | Train GradNorm: 0.03698985108743228\n",
      "Epoch [398/500]\n",
      "Test Loss: 0.36206495186590404 | Test Acc: 0.8397402765215144 | Test GradNorm: 0.0009425171202936213\n",
      "Train Loss: 0.3021309310363164 | Train Acc: 0.859190031152648 | Train GradNorm: 0.001550692853114996\n",
      "Epoch [399/500]\n",
      "Test Loss: 0.41840098744337095 | Test Acc: 0.8026230779170436 | Test GradNorm: 0.07983693841928212\n",
      "Train Loss: 0.3525493495522542 | Train Acc: 0.8317757009345794 | Train GradNorm: 0.07287044866781707\n",
      "Epoch [400/500]\n",
      "Test Loss: 0.36524948824938036 | Test Acc: 0.838609639488306 | Test GradNorm: 0.004423766747496322\n",
      "Train Loss: 0.3065993565658568 | Train Acc: 0.8535825545171339 | Train GradNorm: 0.006499210544604797\n",
      "Epoch [401/500]\n",
      "Test Loss: 0.36617264324034016 | Test Acc: 0.8357991988629022 | Test GradNorm: 0.00654700755763179\n",
      "Train Loss: 0.30348618470096245 | Train Acc: 0.859190031152648 | Train GradNorm: 0.0038668694691913236\n",
      "Epoch [402/500]\n",
      "Test Loss: 0.3734350971999469 | Test Acc: 0.8318258172890555 | Test GradNorm: 0.016740669557011995\n",
      "Train Loss: 0.3098681320313137 | Train Acc: 0.8510903426791278 | Train GradNorm: 0.012408552092988483\n",
      "Epoch [403/500]\n",
      "Test Loss: 0.3866166735334343 | Test Acc: 0.8325688073394495 | Test GradNorm: 0.03071482023973469\n",
      "Train Loss: 0.33242279038401346 | Train Acc: 0.8429906542056075 | Train GradNorm: 0.03725968671343626\n",
      "Epoch [404/500]\n",
      "Test Loss: 0.3838169756846572 | Test Acc: 0.8337963561183616 | Test GradNorm: 0.027194627542192366\n",
      "Train Loss: 0.32964262376436027 | Train Acc: 0.8448598130841122 | Train GradNorm: 0.03347117943250588\n",
      "Epoch [405/500]\n",
      "Test Loss: 0.36900429968365495 | Test Acc: 0.8339901796097687 | Test GradNorm: 0.010452618006313601\n",
      "Train Loss: 0.30591018699435524 | Train Acc: 0.8554517133956386 | Train GradNorm: 0.007005806445502349\n",
      "Epoch [406/500]\n",
      "Test Loss: 0.36557944727411984 | Test Acc: 0.8367360124047034 | Test GradNorm: 0.006030049312480618\n",
      "Train Loss: 0.30320896142521514 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.003384922972313788\n",
      "Epoch [407/500]\n",
      "Test Loss: 0.36195738311050357 | Test Acc: 0.8393849334539346 | Test GradNorm: 0.0009607558483744444\n",
      "Train Loss: 0.3008568539205886 | Train Acc: 0.8623052959501558 | Train GradNorm: 6.906855441995586e-05\n",
      "Epoch [408/500]\n",
      "Test Loss: 0.3722465594091641 | Test Acc: 0.8311797389843649 | Test GradNorm: 0.01579902167799842\n",
      "Train Loss: 0.30927759556041357 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.012101983587028402\n",
      "Epoch [409/500]\n",
      "Test Loss: 0.36342666027881987 | Test Acc: 0.8363483654218892 | Test GradNorm: 0.0016998490344059072\n",
      "Train Loss: 0.3012254921921408 | Train Acc: 0.8623052959501558 | Train GradNorm: 0.000399022281490762\n",
      "Epoch [410/500]\n",
      "Test Loss: 0.3647781348659576 | Test Acc: 0.8392557177929965 | Test GradNorm: 0.005664276896048191\n",
      "Train Loss: 0.3075696239716815 | Train Acc: 0.8554517133956386 | Train GradNorm: 0.008302369600990497\n",
      "Epoch [411/500]\n",
      "Test Loss: 0.4090501370222767 | Test Acc: 0.8268510143429384 | Test GradNorm: 0.05453281029165123\n",
      "Train Loss: 0.35817784683783765 | Train Acc: 0.8386292834890966 | Train GradNorm: 0.06370978466860482\n",
      "Epoch [412/500]\n",
      "Test Loss: 0.3818387070106966 | Test Acc: 0.8335702287117198 | Test GradNorm: 0.026352753219307794\n",
      "Train Loss: 0.3280576262360291 | Train Acc: 0.8461059190031153 | Train GradNorm: 0.03218813922022328\n",
      "Epoch [413/500]\n",
      "Test Loss: 0.4828792034817927 | Test Acc: 0.7736464659516733 | Test GradNorm: 0.17012916883401938\n",
      "Train Loss: 0.4161125947925913 | Train Acc: 0.8024922118380062 | Train GradNorm: 0.16229065238854878\n",
      "Epoch [414/500]\n",
      "Test Loss: 0.3622362644530458 | Test Acc: 0.8385773355730715 | Test GradNorm: 0.0007924582570433945\n",
      "Train Loss: 0.30218535545715225 | Train Acc: 0.8598130841121495 | Train GradNorm: 0.0001871096933064118\n",
      "Epoch [415/500]\n",
      "Test Loss: 0.36418457065173404 | Test Acc: 0.8380927768445535 | Test GradNorm: 0.004542996373253356\n",
      "Train Loss: 0.3058567565404463 | Train Acc: 0.854828660436137 | Train GradNorm: 0.006352787428644154\n",
      "Epoch [416/500]\n",
      "Test Loss: 0.3939908634543416 | Test Acc: 0.8160938105698411 | Test GradNorm: 0.047597981157362794\n",
      "Train Loss: 0.32953125001146133 | Train Acc: 0.838006230529595 | Train GradNorm: 0.041026462675723574\n",
      "Epoch [417/500]\n",
      "Test Loss: 0.3627194890716161 | Test Acc: 0.8391265021320584 | Test GradNorm: 0.002601138010692828\n",
      "Train Loss: 0.3013510845718573 | Train Acc: 0.864797507788162 | Train GradNorm: 0.0009472613954885004\n",
      "Epoch [418/500]\n",
      "Test Loss: 0.3612950002420476 | Test Acc: 0.8387711590644786 | Test GradNorm: 0.0005794115476257162\n",
      "Train Loss: 0.30114784286895135 | Train Acc: 0.8598130841121495 | Train GradNorm: 0.0008340039206398225\n",
      "Epoch [419/500]\n",
      "Test Loss: 0.37643921321697993 | Test Acc: 0.8349269931515699 | Test GradNorm: 0.018770046914585194\n",
      "Train Loss: 0.32033064759137436 | Train Acc: 0.8467289719626169 | Train GradNorm: 0.023799408776728422\n",
      "Epoch [420/500]\n",
      "Test Loss: 0.3648542977807651 | Test Acc: 0.8375759142008011 | Test GradNorm: 0.006095636167711296\n",
      "Train Loss: 0.30352040458294094 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.003494034898609486\n",
      "Epoch [421/500]\n",
      "Test Loss: 0.3606683108485982 | Test Acc: 0.839288021708231 | Test GradNorm: 0.00041301390933599203\n",
      "Train Loss: 0.3007484036906344 | Train Acc: 0.864797507788162 | Train GradNorm: 0.0001886980235561704\n",
      "Epoch [422/500]\n",
      "Test Loss: 0.3802889170052919 | Test Acc: 0.8265602791058276 | Test GradNorm: 0.02763812714440131\n",
      "Train Loss: 0.3168945922103134 | Train Acc: 0.8448598130841122 | Train GradNorm: 0.022748602268671567\n",
      "Epoch [423/500]\n",
      "Test Loss: 0.3685880248378452 | Test Acc: 0.833150277813671 | Test GradNorm: 0.011104190877528631\n",
      "Train Loss: 0.30619339706353904 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.007876208155739734\n",
      "Epoch [424/500]\n",
      "Test Loss: 0.36809826591005773 | Test Acc: 0.8352823362191497 | Test GradNorm: 0.010152555694799953\n",
      "Train Loss: 0.30652370552770747 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.007090038268162707\n",
      "Epoch [425/500]\n",
      "Test Loss: 0.37514878975455634 | Test Acc: 0.8308243959167851 | Test GradNorm: 0.01926986855197344\n",
      "Train Loss: 0.3115075716518985 | Train Acc: 0.8554517133956386 | Train GradNorm: 0.015030528913197111\n",
      "Epoch [426/500]\n",
      "Test Loss: 0.3649932727097564 | Test Acc: 0.8371882672179868 | Test GradNorm: 0.0028911527931323373\n",
      "Train Loss: 0.3017252188237991 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.0012158897588764905\n",
      "Epoch [427/500]\n",
      "Test Loss: 0.36293156253186726 | Test Acc: 0.8379312572683809 | Test GradNorm: 0.001014349607341675\n",
      "Train Loss: 0.30229941482338324 | Train Acc: 0.8610591900311526 | Train GradNorm: 0.001691849642455664\n",
      "Epoch [428/500]\n",
      "Test Loss: 0.36683072438375947 | Test Acc: 0.8354115518800879 | Test GradNorm: 0.007133881092564547\n",
      "Train Loss: 0.30363737573484123 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.004197771986998543\n",
      "Epoch [429/500]\n",
      "Test Loss: 0.40117679189987693 | Test Acc: 0.8118296937588836 | Test GradNorm: 0.05665452670037202\n",
      "Train Loss: 0.3353032210337956 | Train Acc: 0.8361370716510903 | Train GradNorm: 0.049264801668147486\n",
      "Epoch [430/500]\n",
      "Test Loss: 0.4495678316042402 | Test Acc: 0.8154800361803851 | Test GradNorm: 0.09276284354389222\n",
      "Train Loss: 0.40003666096477775 | Train Acc: 0.8242990654205608 | Train GradNorm: 0.10392267607215597\n",
      "Epoch [431/500]\n",
      "Test Loss: 0.37224009052876716 | Test Acc: 0.8312120428995994 | Test GradNorm: 0.01656257761676953\n",
      "Train Loss: 0.3094735162633145 | Train Acc: 0.8554517133956386 | Train GradNorm: 0.012041764104979672\n",
      "Epoch [432/500]\n",
      "Test Loss: 0.37034102601687335 | Test Acc: 0.8361868458457165 | Test GradNorm: 0.012114497467567596\n",
      "Train Loss: 0.31305556923022204 | Train Acc: 0.8498442367601247 | Train GradNorm: 0.015694257443229283\n",
      "Epoch [433/500]\n",
      "Test Loss: 0.37290273788536504 | Test Acc: 0.8298875823749838 | Test GradNorm: 0.015658315239387273\n",
      "Train Loss: 0.3090357537357287 | Train Acc: 0.8535825545171339 | Train GradNorm: 0.011762108097367331\n",
      "Epoch [434/500]\n",
      "Test Loss: 0.3752572368777855 | Test Acc: 0.8347977774906319 | Test GradNorm: 0.01664020153673393\n",
      "Train Loss: 0.3169931353166723 | Train Acc: 0.8485981308411215 | Train GradNorm: 0.020789202004389044\n",
      "Epoch [435/500]\n",
      "Test Loss: 0.36749314615372314 | Test Acc: 0.8346039539992247 | Test GradNorm: 0.008364632457687074\n",
      "Train Loss: 0.30438412196132203 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.005194513727409026\n",
      "Epoch [436/500]\n",
      "Test Loss: 0.376133686284119 | Test Acc: 0.8283692983589611 | Test GradNorm: 0.019578253436103737\n",
      "Train Loss: 0.3117571252972357 | Train Acc: 0.8535825545171339 | Train GradNorm: 0.015416813540443977\n",
      "Epoch [437/500]\n",
      "Test Loss: 0.3844860246465876 | Test Acc: 0.8333764052203128 | Test GradNorm: 0.026589860382054853\n",
      "Train Loss: 0.3277885978540038 | Train Acc: 0.8442367601246106 | Train GradNorm: 0.03244137191803771\n",
      "Epoch [438/500]\n",
      "Test Loss: 0.365338964673498 | Test Acc: 0.8368652280656416 | Test GradNorm: 0.0052371257367060985\n",
      "Train Loss: 0.3024792875761618 | Train Acc: 0.8616822429906542 | Train GradNorm: 0.002843555900738255\n",
      "Epoch [439/500]\n",
      "Test Loss: 0.36193804608377467 | Test Acc: 0.8388680708101822 | Test GradNorm: 0.0006291944236433606\n",
      "Train Loss: 0.3012579282046583 | Train Acc: 0.8598130841121495 | Train GradNorm: 0.0010207294937396734\n",
      "Epoch [440/500]\n",
      "Test Loss: 0.3653517089704919 | Test Acc: 0.8359930223543094 | Test GradNorm: 0.00542022412909869\n",
      "Train Loss: 0.30258978310261136 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.0029430981006633087\n",
      "Epoch [441/500]\n",
      "Test Loss: 0.37398027056707683 | Test Acc: 0.8352823362191497 | Test GradNorm: 0.01649127525698164\n",
      "Train Loss: 0.3174623192385426 | Train Acc: 0.84797507788162 | Train GradNorm: 0.021200126937326355\n",
      "Epoch [442/500]\n",
      "Test Loss: 0.3609344636810947 | Test Acc: 0.8393203256234656 | Test GradNorm: 0.0005144391441362651\n",
      "Train Loss: 0.30139127419324585 | Train Acc: 0.859190031152648 | Train GradNorm: 0.0009322569653657528\n",
      "Epoch [443/500]\n",
      "Test Loss: 0.36323372316059427 | Test Acc: 0.8400956195890942 | Test GradNorm: 0.003316711880348159\n",
      "Train Loss: 0.3053542537933461 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.005499602653579588\n",
      "Epoch [444/500]\n",
      "Test Loss: 0.3951277811420584 | Test Acc: 0.8176120945858638 | Test GradNorm: 0.046698716194504615\n",
      "Train Loss: 0.32985186866765326 | Train Acc: 0.8411214953271028 | Train GradNorm: 0.03976047748379982\n",
      "Epoch [445/500]\n",
      "Test Loss: 0.42325881959545686 | Test Acc: 0.8005556273420339 | Test GradNorm: 0.0865582339675986\n",
      "Train Loss: 0.35573303321806776 | Train Acc: 0.8317757009345794 | Train GradNorm: 0.07922422029840331\n",
      "Epoch [446/500]\n",
      "Test Loss: 0.3701381153191875 | Test Acc: 0.833150277813671 | Test GradNorm: 0.01065028884320888\n",
      "Train Loss: 0.30589905417820995 | Train Acc: 0.8554517133956386 | Train GradNorm: 0.007431036611011164\n",
      "Epoch [447/500]\n",
      "Test Loss: 0.3785963048158407 | Test Acc: 0.8268833182581729 | Test GradNorm: 0.023237607605850683\n",
      "Train Loss: 0.31363380719237827 | Train Acc: 0.8473520249221184 | Train GradNorm: 0.018589040243141675\n",
      "Epoch [448/500]\n",
      "Test Loss: 0.36492982357675346 | Test Acc: 0.8358638066933712 | Test GradNorm: 0.0039551927218673365\n",
      "Train Loss: 0.30191479779689556 | Train Acc: 0.8604361370716511 | Train GradNorm: 0.0018723728448164119\n",
      "Epoch [449/500]\n",
      "Test Loss: 0.36613715865611335 | Test Acc: 0.8365421889132962 | Test GradNorm: 0.004328000512133139\n",
      "Train Loss: 0.30296528690650115 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.002230311823448195\n",
      "Epoch [450/500]\n",
      "Test Loss: 0.36971375986652516 | Test Acc: 0.8369298358961106 | Test GradNorm: 0.008727520147506008\n",
      "Train Loss: 0.3095689136916189 | Train Acc: 0.8498442367601247 | Train GradNorm: 0.011402525453319204\n",
      "Epoch [451/500]\n",
      "Test Loss: 0.36734426987081154 | Test Acc: 0.8349269931515699 | Test GradNorm: 0.006023218684396747\n",
      "Train Loss: 0.30323513748860464 | Train Acc: 0.854828660436137 | Train GradNorm: 0.003484243485176231\n",
      "Epoch [452/500]\n",
      "Test Loss: 0.364139385509996 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.0016297935249937738\n",
      "Train Loss: 0.3010302823788829 | Train Acc: 0.8623052959501558 | Train GradNorm: 0.000384366532702281\n",
      "Epoch [453/500]\n",
      "Test Loss: 0.3681055580269044 | Test Acc: 0.8349592970668045 | Test GradNorm: 0.007561269807417922\n",
      "Train Loss: 0.3039239424776058 | Train Acc: 0.854828660436137 | Train GradNorm: 0.004741435644386308\n",
      "Epoch [454/500]\n",
      "Test Loss: 0.3844554847177302 | Test Acc: 0.8238467502261274 | Test GradNorm: 0.030511834442759116\n",
      "Train Loss: 0.31940579266177016 | Train Acc: 0.8436137071651091 | Train GradNorm: 0.025764060026578994\n",
      "Epoch [455/500]\n",
      "Test Loss: 0.3648587739384809 | Test Acc: 0.8373174828789249 | Test GradNorm: 0.004647091254632698\n",
      "Train Loss: 0.3026402628198581 | Train Acc: 0.8604361370716511 | Train GradNorm: 0.002561470904850249\n",
      "Epoch [456/500]\n",
      "Test Loss: 0.38981156192991356 | Test Acc: 0.8314704742214757 | Test GradNorm: 0.03332756979834811\n",
      "Train Loss: 0.3336354110297763 | Train Acc: 0.8398753894080997 | Train GradNorm: 0.03946751910393284\n",
      "Epoch [457/500]\n",
      "Test Loss: 0.3694477075070043 | Test Acc: 0.8332471895593746 | Test GradNorm: 0.011178718798288353\n",
      "Train Loss: 0.30608859761928153 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.007925714033259727\n",
      "Epoch [458/500]\n",
      "Test Loss: 0.3623457896049531 | Test Acc: 0.8385773355730715 | Test GradNorm: 0.0006486987798151183\n",
      "Train Loss: 0.3006185813207944 | Train Acc: 0.859190031152648 | Train GradNorm: 4.183495214073482e-05\n",
      "Epoch [459/500]\n",
      "Test Loss: 0.3783909022682136 | Test Acc: 0.828853857087479 | Test GradNorm: 0.023266895498327448\n",
      "Train Loss: 0.31385311936721655 | Train Acc: 0.8504672897196262 | Train GradNorm: 0.018663286611971062\n",
      "Epoch [460/500]\n",
      "Test Loss: 0.36287386403206395 | Test Acc: 0.8375759142008011 | Test GradNorm: 0.0006354526225319719\n",
      "Train Loss: 0.30097098561134744 | Train Acc: 0.8635514018691589 | Train GradNorm: 0.0007226191860128495\n",
      "Epoch [461/500]\n",
      "Test Loss: 0.3799345252288968 | Test Acc: 0.834313218762114 | Test GradNorm: 0.02211822101548327\n",
      "Train Loss: 0.32243410339884415 | Train Acc: 0.8485981308411215 | Train GradNorm: 0.026950799389336914\n",
      "Epoch [462/500]\n",
      "Test Loss: 0.4338822833566512 | Test Acc: 0.8201317999741569 | Test GradNorm: 0.07771526004897503\n",
      "Train Loss: 0.3818440126540577 | Train Acc: 0.8267912772585669 | Train GradNorm: 0.08768085291066001\n",
      "Epoch [463/500]\n",
      "Test Loss: 0.3833474726591245 | Test Acc: 0.8329241504070294 | Test GradNorm: 0.02686522438252045\n",
      "Train Loss: 0.3267565053704882 | Train Acc: 0.8442367601246106 | Train GradNorm: 0.03243871501139177\n",
      "Epoch [464/500]\n",
      "Test Loss: 0.42137780384196094 | Test Acc: 0.8021385191885256 | Test GradNorm: 0.08545444315571141\n",
      "Train Loss: 0.35433415215177594 | Train Acc: 0.8280373831775701 | Train GradNorm: 0.07718239235338338\n",
      "Epoch [465/500]\n",
      "Test Loss: 0.3632961502242139 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.0008106522128693781\n",
      "Train Loss: 0.3015716240315962 | Train Acc: 0.8616822429906542 | Train GradNorm: 0.0012069149929438384\n",
      "Epoch [466/500]\n",
      "Test Loss: 0.36418866061111965 | Test Acc: 0.8371559633027523 | Test GradNorm: 0.0017521675517537289\n",
      "Train Loss: 0.30078323532211493 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.000427856480808909\n",
      "Epoch [467/500]\n",
      "Test Loss: 0.3861738999952197 | Test Acc: 0.8324395916785114 | Test GradNorm: 0.029667146003891167\n",
      "Train Loss: 0.33036559633522633 | Train Acc: 0.8417445482866044 | Train GradNorm: 0.03593149800115465\n",
      "Epoch [468/500]\n",
      "Test Loss: 0.37330127319586925 | Test Acc: 0.830759788086316 | Test GradNorm: 0.01685689475531981\n",
      "Train Loss: 0.3095070872455444 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.012828821530303468\n",
      "Epoch [469/500]\n",
      "Test Loss: 0.37477515406091505 | Test Acc: 0.830016798035922 | Test GradNorm: 0.017220659938404766\n",
      "Train Loss: 0.3098443263146501 | Train Acc: 0.8517133956386292 | Train GradNorm: 0.013378774739112899\n",
      "Epoch [470/500]\n",
      "Test Loss: 0.3624798586945371 | Test Acc: 0.8385127277426024 | Test GradNorm: 0.0006773234073994826\n",
      "Train Loss: 0.3011145796644749 | Train Acc: 0.859190031152648 | Train GradNorm: 0.0001296143495608303\n",
      "Epoch [471/500]\n",
      "Test Loss: 0.40481442893612624 | Test Acc: 0.8273355730714562 | Test GradNorm: 0.049560087927107296\n",
      "Train Loss: 0.3504185779704349 | Train Acc: 0.8386292834890966 | Train GradNorm: 0.05739440679012919\n",
      "Epoch [472/500]\n",
      "Test Loss: 0.3819570333831652 | Test Acc: 0.8265279751905931 | Test GradNorm: 0.029370235272216296\n",
      "Train Loss: 0.31763614593846273 | Train Acc: 0.8485981308411215 | Train GradNorm: 0.02379070906928291\n",
      "Epoch [473/500]\n",
      "Test Loss: 0.3653675172730705 | Test Acc: 0.8371559633027523 | Test GradNorm: 0.005691204924121195\n",
      "Train Loss: 0.3028214110799311 | Train Acc: 0.8598130841121495 | Train GradNorm: 0.0032417023722177397\n",
      "Epoch [474/500]\n",
      "Test Loss: 0.37206274507235815 | Test Acc: 0.832471895593746 | Test GradNorm: 0.01456143179791057\n",
      "Train Loss: 0.30831453983823837 | Train Acc: 0.8523364485981308 | Train GradNorm: 0.010952660309611686\n",
      "Epoch [475/500]\n",
      "Test Loss: 0.36481495385204926 | Test Acc: 0.837446698539863 | Test GradNorm: 0.003358179229029114\n",
      "Train Loss: 0.3022126697320647 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.0014912948971610236\n",
      "Epoch [476/500]\n",
      "Test Loss: 0.38314414106325045 | Test Acc: 0.8256234655640263 | Test GradNorm: 0.03192770078431308\n",
      "Train Loss: 0.31937048551800234 | Train Acc: 0.8454828660436137 | Train GradNorm: 0.026537349256434018\n",
      "Epoch [477/500]\n",
      "Test Loss: 0.3630960605151543 | Test Acc: 0.8385450316578369 | Test GradNorm: 0.0016783944341454316\n",
      "Train Loss: 0.3022914706709913 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.002474186841994019\n",
      "Epoch [478/500]\n",
      "Test Loss: 0.41967776497400816 | Test Acc: 0.8229745445147952 | Test GradNorm: 0.06325211740369739\n",
      "Train Loss: 0.3637253253330679 | Train Acc: 0.8305295950155763 | Train GradNorm: 0.07145090456551971\n",
      "Epoch [479/500]\n",
      "Test Loss: 0.45721336675618157 | Test Acc: 0.7844036697247706 | Test GradNorm: 0.1331808890684306\n",
      "Train Loss: 0.3878206750784703 | Train Acc: 0.8161993769470405 | Train GradNorm: 0.12454134592652746\n",
      "Epoch [480/500]\n",
      "Test Loss: 0.3633322179265667 | Test Acc: 0.8385127277426024 | Test GradNorm: 0.0005269721344387701\n",
      "Train Loss: 0.30115792892465343 | Train Acc: 0.859190031152648 | Train GradNorm: 0.0006498563382090682\n",
      "Epoch [481/500]\n",
      "Test Loss: 0.38151083013065307 | Test Acc: 0.8246866520222251 | Test GradNorm: 0.024554358768446066\n",
      "Train Loss: 0.3151977669173091 | Train Acc: 0.8448598130841122 | Train GradNorm: 0.02015542668522266\n",
      "Epoch [482/500]\n",
      "Test Loss: 0.3877521132214384 | Test Acc: 0.8324072877632769 | Test GradNorm: 0.02848083357963168\n",
      "Train Loss: 0.32869086693620125 | Train Acc: 0.8454828660436137 | Train GradNorm: 0.03386449351253967\n",
      "Epoch [483/500]\n",
      "Test Loss: 0.36271215306149734 | Test Acc: 0.8383835120816643 | Test GradNorm: 0.0006458380559185768\n",
      "Train Loss: 0.30122713879494317 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.0009608279630678298\n",
      "Epoch [484/500]\n",
      "Test Loss: 0.3739882334548845 | Test Acc: 0.8353469440496188 | Test GradNorm: 0.014917650187406865\n",
      "Train Loss: 0.3148471121956228 | Train Acc: 0.8485981308411215 | Train GradNorm: 0.018530259185661313\n",
      "Epoch [485/500]\n",
      "Test Loss: 0.38778294132059116 | Test Acc: 0.8312766507300685 | Test GradNorm: 0.030443804332774337\n",
      "Train Loss: 0.33054186387906165 | Train Acc: 0.8417445482866044 | Train GradNorm: 0.036375983953705775\n",
      "Epoch [486/500]\n",
      "Test Loss: 0.3634273979579689 | Test Acc: 0.8394818451996382 | Test GradNorm: 0.0007497211493934276\n",
      "Train Loss: 0.3004723768537772 | Train Acc: 0.8641744548286604 | Train GradNorm: 4.961611149168626e-05\n",
      "Epoch [487/500]\n",
      "Test Loss: 0.36470415853786203 | Test Acc: 0.8384804238273679 | Test GradNorm: 0.002715813489317581\n",
      "Train Loss: 0.3033660273775765 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.00412405376844825\n",
      "Epoch [488/500]\n",
      "Test Loss: 0.37996439962458284 | Test Acc: 0.8336671404574234 | Test GradNorm: 0.021482124233433254\n",
      "Train Loss: 0.3216807720663681 | Train Acc: 0.84797507788162 | Train GradNorm: 0.026372925139061424\n",
      "Epoch [489/500]\n",
      "Test Loss: 0.3740697189864738 | Test Acc: 0.8354438557953224 | Test GradNorm: 0.014843165475109336\n",
      "Train Loss: 0.315256029908754 | Train Acc: 0.84797507788162 | Train GradNorm: 0.018829849703803005\n",
      "Epoch [490/500]\n",
      "Test Loss: 0.3708346078666593 | Test Acc: 0.8333117973898436 | Test GradNorm: 0.012281015418415465\n",
      "Train Loss: 0.3066811704263274 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.008895735646889506\n",
      "Epoch [491/500]\n",
      "Test Loss: 0.3647739857626178 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.003942453776925364\n",
      "Train Loss: 0.3017811090433577 | Train Acc: 0.8654205607476636 | Train GradNorm: 0.0019003192528078135\n",
      "Epoch [492/500]\n",
      "Test Loss: 0.3710667694741939 | Test Acc: 0.8365744928285308 | Test GradNorm: 0.01029656829654164\n",
      "Train Loss: 0.31085699801658734 | Train Acc: 0.8517133956386292 | Train GradNorm: 0.013470615947033669\n",
      "Epoch [493/500]\n",
      "Test Loss: 0.3661058321898927 | Test Acc: 0.8360576301847784 | Test GradNorm: 0.004528785322489816\n",
      "Train Loss: 0.30192551779654375 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.002304248891629724\n",
      "Epoch [494/500]\n",
      "Test Loss: 0.36877379328643484 | Test Acc: 0.8360253262695438 | Test GradNorm: 0.007560755704443406\n",
      "Train Loss: 0.308096441137569 | Train Acc: 0.8523364485981308 | Train GradNorm: 0.010067358079660749\n",
      "Epoch [495/500]\n",
      "Test Loss: 0.3663795965392566 | Test Acc: 0.8360253262695438 | Test GradNorm: 0.0029843874670659985\n",
      "Train Loss: 0.30146607148857985 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.0013681384271324512\n",
      "Epoch [496/500]\n",
      "Test Loss: 0.3785771833543058 | Test Acc: 0.8289507688331825 | Test GradNorm: 0.02173828953117811\n",
      "Train Loss: 0.3126691001632132 | Train Acc: 0.8504672897196262 | Train GradNorm: 0.017204395537119558\n",
      "Epoch [497/500]\n",
      "Test Loss: 0.3744835808206809 | Test Acc: 0.8312766507300685 | Test GradNorm: 0.01610638145655093\n",
      "Train Loss: 0.3096250108918998 | Train Acc: 0.8517133956386292 | Train GradNorm: 0.011869962096707383\n",
      "Epoch [498/500]\n",
      "Test Loss: 0.36339800600327926 | Test Acc: 0.8398048843519834 | Test GradNorm: 0.0012461302649875972\n",
      "Train Loss: 0.3015766972654438 | Train Acc: 0.8610591900311526 | Train GradNorm: 0.00030493818423438186\n",
      "Epoch [499/500]\n",
      "Test Loss: 0.3741552973653065 | Test Acc: 0.8364775810828272 | Test GradNorm: 0.014663919416272564\n",
      "Train Loss: 0.31545304097097565 | Train Acc: 0.8523364485981308 | Train GradNorm: 0.018735512775911935\n"
     ]
    }
   ],
   "source": [
    "hist1 = train_loop(\"a1a\", 0, 128, 500, methods.SANIA_AdamSQR, eps=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rademacher_old(weights):\n",
    "    return torch.round(torch.rand_like(weights)) * 2 - 1\n",
    "\n",
    "def diag_estimate_old(weights, grad, iters):\n",
    "    Ds = []\n",
    "    for j in range(iters):\n",
    "        z = rademacher_old(weights)\n",
    "        with torch.no_grad():\n",
    "            hvp = torch.autograd.grad(grad, weights, grad_outputs=z, retain_graph=True)[0]\n",
    "        Ds.append((hvp*z))\n",
    "\n",
    "    return torch.mean(torch.stack(Ds), 0)\n",
    "\n",
    "\n",
    "\n",
    "def run_optimizer(optimizer, loss_function, train_data, train_target, train_dataloader, epochs, **kwargs_optimizer):\n",
    "    # parameters\n",
    "    w = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "    opt = optimizer([w], **kwargs_optimizer)\n",
    "\n",
    "    # logging \n",
    "    hist = []\n",
    "    \n",
    "    def compute_loss(w, data, target):\n",
    "        loss = loss_function(w, data, target)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        loss = loss_function(w, train_data.to(device), train_target.to(device))\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "        acc = (np.sign(train_data @ w.detach().numpy()) == train_target).sum() / train_target.shape[0]\n",
    "        print(f\"[{epoch}/{epochs}] | Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()} | Acc: {acc}\")\n",
    "        hist.append([loss.item(), (torch.linalg.norm(g) ** 2).item(), acc])\n",
    "\n",
    "        for i, (batch_data, batch_target) in enumerate(train_dataloader):\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_target = batch_target.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = compute_loss(w, batch_data, batch_target)\n",
    "            opt.step()\n",
    "\n",
    "    return hist\n",
    "\n",
    "\n",
    "def run_psps2(loss_function, train_data, train_target, train_dataloader, epochs, precond_method, eps = 1.0, **kwargs):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    # parameters\n",
    "    w = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "\n",
    "    # save loss and grad size to history\n",
    "    hist = []\n",
    "    \n",
    "\n",
    "    loss = loss_function(w, train_data.to(device), train_target.to(device))\n",
    "    g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "    f_grad = g.clone().detach() \n",
    "\n",
    "\n",
    "    if precond_method == \"none\":\n",
    "        D = torch.ones_like(w)\n",
    "    elif precond_method == \"hutch\":\n",
    "        alpha=0.1\n",
    "        beta=0.999\n",
    "        init_iters = kwargs[\"hutch_init_iters\"]\n",
    "        Dk = diag_estimate_old(w, g, init_iters)\n",
    "    elif precond_method == \"pcg\":\n",
    "        MAX_ITER = train_data.shape[1] * 2\n",
    "\n",
    "    elif precond_method == \"scaling_vec\":\n",
    "        scaling_vec = kwargs[\"scaling_vec\"]\n",
    "        D = (1 / scaling_vec)**2\n",
    "    elif precond_method == \"adam\" or precond_method == \"adam_m\":\n",
    "        D = torch.zeros_like(g)\n",
    "        v = torch.zeros_like(g)\n",
    "        step_t = torch.tensor(0.)\n",
    "        betas = (0.9, 0.999)\n",
    "    elif precond_method == \"adagrad\" or precond_method == \"adagrad_m\":\n",
    "        D = torch.zeros_like(g)\n",
    "        v = torch.zeros_like(g)\n",
    "\n",
    "    pcg_method = kwargs.get(\"pcg_method\")\n",
    "    if pcg_method == \"hutch\":\n",
    "        alpha=0.1\n",
    "        beta=0.999\n",
    "        init_iters = kwargs[\"hutch_init_iters\"]\n",
    "        Dk_pcg = diag_estimate_old(w, g, init_iters)\n",
    "    elif pcg_method == \"adam\" or pcg_method == \"adam_m\":\n",
    "        D_pcg = torch.zeros_like(g)\n",
    "        v_pcg = torch.zeros_like(g)\n",
    "        step_t_pcg = torch.tensor(0.)\n",
    "        betas = (0.9, 0.999)\n",
    "    elif pcg_method == \"adagrad\" or pcg_method == \"adagrad_m\":\n",
    "        D_pcg = torch.zeros_like(g)\n",
    "        v_pcg = torch.zeros_like(g)\n",
    "    elif pcg_method == \"none\":\n",
    "        D_pcg = torch.ones_like(g)\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # eps = eps / 2\n",
    "\n",
    "        loss = loss_function(w, train_data.to(device), train_target.to(device))\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "        grad_norm_sq = torch.linalg.norm(g) ** 2  \n",
    "        acc = (np.sign(train_data @ w.detach().numpy()) == train_target).sum() / train_target.shape[0]\n",
    "\n",
    "        # print(f\"[{epoch}/{epochs}] | Loss: {loss.item()} | GradNorm^2: {grad_norm_sq.item()} | Accuracy: {acc}\")\n",
    "        hist.append([loss.item(), grad_norm_sq.item(), acc])\n",
    "           \n",
    "        for i, (batch_data, batch_target) in enumerate(train_dataloader):  \n",
    "\n",
    "            loss = loss_function(w, batch_data, batch_target)\n",
    "            g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "            f_grad = g.detach().clone()\n",
    "\n",
    "            if precond_method == \"scaling_vec\":\n",
    "                s = D * f_grad\n",
    "\n",
    "            elif precond_method == \"adam\" or precond_method == \"adam_m\":\n",
    "                step_t += 1\n",
    "                v = betas[1] * v + (1 - betas[1]) * g.square()\n",
    "                v_hat = v / (1 - torch.pow(betas[1], step_t))\n",
    "                # sum_regularized = torch.maximum(torch.full_like(v_hat, fill_value=eps), v_hat)\n",
    "                \n",
    "                if precond_method == \"adam\":\n",
    "                    s = f_grad / (torch.sqrt(v_hat) + eps)\n",
    "                    # s = f_grad / torch.sqrt(sum_regularized)\n",
    "                elif precond_method == \"adam_m\":\n",
    "                    s = f_grad / (v_hat + eps) \n",
    "                    # s = f_grad / sum_regularized\n",
    "\n",
    "            elif precond_method == \"adagrad\" or precond_method == \"adagrad_m\":\n",
    "                v.add_(torch.square(g))\n",
    "                sum_regularized = torch.maximum(torch.full_like(v, fill_value=eps), v)\n",
    "\n",
    "                if precond_method == \"adagrad\":\n",
    "                    s = f_grad / (torch.sqrt(v) + eps)\n",
    "                    # s = f_grad / torch.sqrt(sum_regularized)\n",
    "                elif precond_method == \"adagrad_m\":\n",
    "                    s = f_grad / (v + eps)\n",
    "                    # s = f_grad / sum_regularized\n",
    "\n",
    "            elif precond_method == \"none\":\n",
    "                s = D * f_grad\n",
    "\n",
    "            elif precond_method == \"hutch\":\n",
    "                vk = diag_estimate_old(w, g, 1)\n",
    "\n",
    "                # Smoothing and Truncation \n",
    "                Dk = beta * Dk + (1 - beta) * vk\n",
    "                Dk_hat = torch.abs(Dk)\n",
    "                Dk_hat[Dk_hat < alpha] = alpha\n",
    "\n",
    "                D = 1 / Dk_hat\n",
    "                s = D * f_grad\n",
    "\n",
    "            elif precond_method == \"pcg\":\n",
    "\n",
    "                if pcg_method == \"hutch\":\n",
    "                    vk_pcg = diag_estimate_old(w, g, 1)\n",
    "                    # Smoothing and Truncation \n",
    "                    Dk_pcg = beta * Dk_pcg + (1 - beta) * vk_pcg\n",
    "                    Dk_hat = torch.abs(Dk_pcg)\n",
    "                    Dk_hat[Dk_hat < alpha] = alpha\n",
    "                    D_pcg = 1 / Dk_hat\n",
    "\n",
    "                elif pcg_method == \"adam\" or pcg_method == \"adam_m\":\n",
    "                    step_t_pcg += 1\n",
    "                    v_pcg = betas[1] * v_pcg + (1 - betas[1]) * f_grad.square()\n",
    "                    v_hat = v_pcg / (1 - torch.pow(betas[1], step_t_pcg))\n",
    "                    if pcg_method == \"adam\":\n",
    "                        D_pcg = 1 / (torch.sqrt(v_hat) + 1e-6)\n",
    "                    else:\n",
    "                        D_pcg = 1 / (v_hat + 1e-6)\n",
    "\n",
    "                elif pcg_method == \"adagrad\" or pcg_method == \"adagrad_m\":\n",
    "                    v_pcg.add_(f_grad.square())\n",
    "                    if pcg_method == \"adagrad\":\n",
    "                        D_pcg = 1 / (torch.sqrt(v_pcg) + 1e-6)\n",
    "                    else:\n",
    "                        D_pcg = 1 / (v_pcg + 1e-6)\n",
    "\n",
    "                hess_diag_inv = D_pcg.clone()\n",
    "                # Preconditioned CG is here\n",
    "                s = torch.zeros_like(w) # s = H_inv * grad\n",
    "                r = f_grad.clone()\n",
    "                z = hess_diag_inv * r\n",
    "                p = z.detach().clone()\n",
    "\n",
    "                for cg_step in range(MAX_ITER):\n",
    "                    hvp = torch.autograd.grad(g, w, grad_outputs=p, retain_graph=True)[0]\n",
    "                    alpha_k = torch.dot(r, z) / torch.dot(p, hvp)\n",
    "                    s = s + alpha_k * p\n",
    "                    r_prev = r.clone()\n",
    "                    r = r - alpha_k * hvp\n",
    "                    if torch.norm(r) < 1e-4:\n",
    "                        break\n",
    "                    \n",
    "                    z_prev = z.clone()\n",
    "                    z = hess_diag_inv * r\n",
    "                    beta_k = torch.dot(r, z) / torch.dot(r_prev, z_prev)\n",
    "                    p = z + beta_k * p    \n",
    "\n",
    "\n",
    "            grad_norm_sq_scaled = torch.dot(f_grad, s)\n",
    "\n",
    "            if 2 * loss <= grad_norm_sq_scaled:\n",
    "                det = 1 - ( (2 * loss) / ( grad_norm_sq_scaled )) \n",
    "                step_size = 1 - torch.sqrt(det)\n",
    "            else:\n",
    "                # print(f\"[{epoch}, {i}] No solution\")\n",
    "                step_size = 1.0\n",
    "\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                w.sub_(step_size * s)\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg(w, X, y):\n",
    "    return torch.mean(torch.log(1 + torch.exp(-y * (X @ w))))\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "dataset_name = \"a1a\"\n",
    "dataset_scale = 0\n",
    "batch_size = 128\n",
    "train_data, train_target, _= datasets.get_libsvm(name=dataset_name, scale=dataset_scale, seed=0)\n",
    "train_data = torch.tensor(train_data)\n",
    "train_target = torch.tensor(train_target)\n",
    "train_load = TensorDataset(train_data, train_target)\n",
    "train_dataloader = DataLoader(train_load, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    \n",
    "hist2 = run_psps2(logreg, train_data, train_target, train_dataloader, 500, \"adam_m\", eps=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x14a7cf47f3e0>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.semilogy(hist1[\"train/epoch/loss\"], label=\"hist1\")\n",
    "plt.semilogy([x[0] for x in hist2], linestyle=\"--\", label=\"hist2\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/500]\n",
      "Test Loss: 0.6931471805599452 | Test Acc: 0.0 | Test GradNorm: 0.4549499969390798\n",
      "Train Loss: 0.6931471805599452 | Train Acc: 0.0 | Train GradNorm: 0.4359846080686329\n",
      "Epoch [1/500]\n",
      "Test Loss: 0.374287320795298 | Test Acc: 0.8308243959167851 | Test GradNorm: 0.0037326950634616477\n",
      "Train Loss: 0.37381653815358595 | Train Acc: 0.8348909657320872 | Train GradNorm: 0.003425338727719227\n",
      "Epoch [2/500]\n",
      "Test Loss: 0.35507825138842286 | Test Acc: 0.8357668949476676 | Test GradNorm: 0.0016275815273661407\n",
      "Train Loss: 0.35070453210077085 | Train Acc: 0.8392523364485981 | Train GradNorm: 0.0020823008599086057\n",
      "Epoch [3/500]\n",
      "Test Loss: 0.3494059299596812 | Test Acc: 0.8421307662488694 | Test GradNorm: 0.003522924046600861\n",
      "Train Loss: 0.34138845810991203 | Train Acc: 0.8367601246105919 | Train GradNorm: 0.0023203858215436342\n",
      "Epoch [4/500]\n",
      "Test Loss: 0.34423514138357064 | Test Acc: 0.841678511435586 | Test GradNorm: 0.0006240834624278102\n",
      "Train Loss: 0.33459644426859597 | Train Acc: 0.8398753894080997 | Train GradNorm: 0.0005273354278112852\n",
      "Epoch [5/500]\n",
      "Test Loss: 0.3423302043665393 | Test Acc: 0.8414200801137098 | Test GradNorm: 0.0007860041589271591\n",
      "Train Loss: 0.33089569535424534 | Train Acc: 0.8436137071651091 | Train GradNorm: 0.0012665971227961742\n",
      "Epoch [6/500]\n",
      "Test Loss: 0.34144531237305964 | Test Acc: 0.8438751776715337 | Test GradNorm: 0.0010798603697913481\n",
      "Train Loss: 0.32733483471467845 | Train Acc: 0.8454828660436137 | Train GradNorm: 0.0003791831647017258\n",
      "Epoch [7/500]\n",
      "Test Loss: 0.34009792370283254 | Test Acc: 0.8436813541801267 | Test GradNorm: 0.00046861047829522553\n",
      "Train Loss: 0.32501272855805075 | Train Acc: 0.8442367601246106 | Train GradNorm: 0.00021185242582114305\n",
      "Epoch [8/500]\n",
      "Test Loss: 0.339722137708664 | Test Acc: 0.8435521385191885 | Test GradNorm: 0.0004586543656660978\n",
      "Train Loss: 0.32313992432959004 | Train Acc: 0.8454828660436137 | Train GradNorm: 0.0001806123964581274\n",
      "Epoch [9/500]\n",
      "Test Loss: 0.3403582439499905 | Test Acc: 0.8439720894172373 | Test GradNorm: 0.001986570331519028\n",
      "Train Loss: 0.3220004194700208 | Train Acc: 0.8517133956386292 | Train GradNorm: 0.0007954083717019829\n",
      "Epoch [10/500]\n",
      "Test Loss: 0.3393745456680175 | Test Acc: 0.8430029719602016 | Test GradNorm: 0.0005820748440921372\n",
      "Train Loss: 0.320914301704389 | Train Acc: 0.8473520249221184 | Train GradNorm: 0.0009895999899282612\n",
      "Epoch [11/500]\n",
      "Test Loss: 0.33905260018396594 | Test Acc: 0.8439397855020029 | Test GradNorm: 0.0003727109671770206\n",
      "Train Loss: 0.3193134651018164 | Train Acc: 0.8492211838006231 | Train GradNorm: 0.00020403175934673498\n",
      "Epoch [12/500]\n",
      "Test Loss: 0.3398303746251019 | Test Acc: 0.8441659129086445 | Test GradNorm: 0.0014940166067873072\n",
      "Train Loss: 0.31856049881729365 | Train Acc: 0.8523364485981308 | Train GradNorm: 0.0004222509765239081\n",
      "Epoch [13/500]\n",
      "Test Loss: 0.34327523415174355 | Test Acc: 0.8431321876211397 | Test GradNorm: 0.00667514281682454\n",
      "Train Loss: 0.3202390844122397 | Train Acc: 0.8510903426791278 | Train GradNorm: 0.004311186708186373\n",
      "Epoch [14/500]\n",
      "Test Loss: 0.33948706076481144 | Test Acc: 0.8437459620105957 | Test GradNorm: 0.0006597692861024673\n",
      "Train Loss: 0.3168385152445632 | Train Acc: 0.854828660436137 | Train GradNorm: 6.046684785815375e-05\n",
      "Epoch [15/500]\n",
      "Test Loss: 0.33949825907357034 | Test Acc: 0.8430029719602016 | Test GradNorm: 0.0004170390362059589\n",
      "Train Loss: 0.31658971619364906 | Train Acc: 0.84797507788162 | Train GradNorm: 0.0005873100962739513\n",
      "Epoch [16/500]\n",
      "Test Loss: 0.33955404466043576 | Test Acc: 0.8436813541801267 | Test GradNorm: 0.00039247402635560506\n",
      "Train Loss: 0.3157221811195941 | Train Acc: 0.8535825545171339 | Train GradNorm: 0.00014071305927922153\n",
      "Epoch [17/500]\n",
      "Test Loss: 0.33973290365424275 | Test Acc: 0.8433260111125468 | Test GradNorm: 0.0003743039719311803\n",
      "Train Loss: 0.31525116819538046 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.00020314899206582397\n",
      "Epoch [18/500]\n",
      "Test Loss: 0.34032481121866065 | Test Acc: 0.8438105698410647 | Test GradNorm: 0.0011532303493288624\n",
      "Train Loss: 0.3147550421770346 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.00020945618373705864\n",
      "Epoch [19/500]\n",
      "Test Loss: 0.3416949395198039 | Test Acc: 0.843519834603954 | Test GradNorm: 0.002874871141198511\n",
      "Train Loss: 0.3149890296289818 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.0012552134366275102\n",
      "Epoch [20/500]\n",
      "Test Loss: 0.34103986708630063 | Test Acc: 0.8436813541801267 | Test GradNorm: 0.0015763880291732378\n",
      "Train Loss: 0.3140163099021302 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.0004067873534698853\n",
      "Epoch [21/500]\n",
      "Test Loss: 0.3403441137555939 | Test Acc: 0.8430352758754361 | Test GradNorm: 0.0003581563176945926\n",
      "Train Loss: 0.31352298126967526 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.00022182911122099576\n",
      "Epoch [22/500]\n",
      "Test Loss: 0.34067297947510905 | Test Acc: 0.8431321876211397 | Test GradNorm: 0.0004791487518973155\n",
      "Train Loss: 0.3130697455324545 | Train Acc: 0.854828660436137 | Train GradNorm: 5.6224310868838466e-05\n",
      "Epoch [23/500]\n",
      "Test Loss: 0.34077839710704705 | Test Acc: 0.842970668044967 | Test GradNorm: 0.0003826191569742924\n",
      "Train Loss: 0.3127832205714306 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.00015403727088784022\n",
      "Epoch [24/500]\n",
      "Test Loss: 0.3410172204032858 | Test Acc: 0.8430352758754361 | Test GradNorm: 0.0004909432697796952\n",
      "Train Loss: 0.3124083195652629 | Train Acc: 0.854828660436137 | Train GradNorm: 4.849994990968816e-05\n",
      "Epoch [25/500]\n",
      "Test Loss: 0.34271015682650807 | Test Acc: 0.8430029719602016 | Test GradNorm: 0.0027090514787103956\n",
      "Train Loss: 0.31279396979054525 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.0010834285442197936\n",
      "Epoch [26/500]\n",
      "Test Loss: 0.34160559704832083 | Test Acc: 0.8434875306887195 | Test GradNorm: 0.000759450633506938\n",
      "Train Loss: 0.31181651334164956 | Train Acc: 0.854828660436137 | Train GradNorm: 3.315424204585461e-05\n",
      "Epoch [27/500]\n",
      "Test Loss: 0.341468512788688 | Test Acc: 0.84242150148598 | Test GradNorm: 0.0003901772936666548\n",
      "Train Loss: 0.3116430902693343 | Train Acc: 0.8529595015576324 | Train GradNorm: 0.00015106037425184054\n",
      "Epoch [28/500]\n",
      "Test Loss: 0.34174479857554035 | Test Acc: 0.8433583150277814 | Test GradNorm: 0.0007201389645217928\n",
      "Train Loss: 0.311325032487962 | Train Acc: 0.854828660436137 | Train GradNorm: 2.635575363017485e-05\n",
      "Epoch [29/500]\n",
      "Test Loss: 0.3417600206513654 | Test Acc: 0.842970668044967 | Test GradNorm: 0.0004369602414447758\n",
      "Train Loss: 0.3111301788471155 | Train Acc: 0.8554517133956386 | Train GradNorm: 7.924785107947154e-05\n",
      "Epoch [30/500]\n",
      "Test Loss: 0.34192618502345384 | Test Acc: 0.8431321876211397 | Test GradNorm: 0.0004343604648444033\n",
      "Train Loss: 0.31090522176397434 | Train Acc: 0.8554517133956386 | Train GradNorm: 7.944922868217537e-05\n",
      "Epoch [31/500]\n",
      "Test Loss: 0.3425171461620945 | Test Acc: 0.841743119266055 | Test GradNorm: 0.0010508435457725052\n",
      "Train Loss: 0.3119636331178419 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.0018779155255605924\n",
      "Epoch [32/500]\n",
      "Test Loss: 0.3422599538895332 | Test Acc: 0.8428737562992634 | Test GradNorm: 0.0004329228795855915\n",
      "Train Loss: 0.3104862039905068 | Train Acc: 0.854828660436137 | Train GradNorm: 8.224904716357386e-05\n",
      "Epoch [33/500]\n",
      "Test Loss: 0.34264320822670963 | Test Acc: 0.8432290993668433 | Test GradNorm: 0.0007480898678366753\n",
      "Train Loss: 0.31023995402460164 | Train Acc: 0.8566978193146417 | Train GradNorm: 2.5738141868584033e-05\n",
      "Epoch [34/500]\n",
      "Test Loss: 0.3426800481881525 | Test Acc: 0.8430029719602016 | Test GradNorm: 0.0006240022529874889\n",
      "Train Loss: 0.310050839435096 | Train Acc: 0.854828660436137 | Train GradNorm: 1.419011879875814e-05\n",
      "Epoch [35/500]\n",
      "Test Loss: 0.3436859031146684 | Test Acc: 0.8425830210621528 | Test GradNorm: 0.0018705532801016028\n",
      "Train Loss: 0.31022336197948963 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.0005310455633100776\n",
      "Epoch [36/500]\n",
      "Test Loss: 0.3430965403310532 | Test Acc: 0.8430998837059052 | Test GradNorm: 0.0007598961136003331\n",
      "Train Loss: 0.3097009897929683 | Train Acc: 0.8566978193146417 | Train GradNorm: 2.4913468177034088e-05\n",
      "Epoch [37/500]\n",
      "Test Loss: 0.3435411190483519 | Test Acc: 0.8425184132316836 | Test GradNorm: 0.0011847190727521746\n",
      "Train Loss: 0.3096262492558673 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.00016362686401329767\n",
      "Epoch [38/500]\n",
      "Test Loss: 0.34308850291139076 | Test Acc: 0.8420661584184003 | Test GradNorm: 0.00039011686285898644\n",
      "Train Loss: 0.3097143252792915 | Train Acc: 0.8517133956386292 | Train GradNorm: 0.0005049627551394657\n",
      "Epoch [39/500]\n",
      "Test Loss: 0.3435767957934864 | Test Acc: 0.84242150148598 | Test GradNorm: 0.0007856747236809679\n",
      "Train Loss: 0.30922479791796875 | Train Acc: 0.8573208722741433 | Train GradNorm: 2.6449289081846447e-05\n",
      "Epoch [40/500]\n",
      "Test Loss: 0.34351126516952835 | Test Acc: 0.8423245897402766 | Test GradNorm: 0.000413659295705212\n",
      "Train Loss: 0.30914409638207063 | Train Acc: 0.8535825545171339 | Train GradNorm: 0.00011907631995156746\n",
      "Epoch [41/500]\n",
      "Test Loss: 0.34374467369662687 | Test Acc: 0.8425507171469182 | Test GradNorm: 0.0004361536011788189\n",
      "Train Loss: 0.3089770523451799 | Train Acc: 0.8535825545171339 | Train GradNorm: 9.092433637677676e-05\n",
      "Epoch [42/500]\n",
      "Test Loss: 0.34465578786153966 | Test Acc: 0.8417754231812896 | Test GradNorm: 0.0017026268583220222\n",
      "Train Loss: 0.30906011827192886 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.00041507709177495864\n",
      "Epoch [43/500]\n",
      "Test Loss: 0.3442385237668928 | Test Acc: 0.842292285825042 | Test GradNorm: 0.0008070528968852602\n",
      "Train Loss: 0.3086673340944732 | Train Acc: 0.8585669781931464 | Train GradNorm: 2.7621433911745766e-05\n",
      "Epoch [44/500]\n",
      "Test Loss: 0.34410825644693427 | Test Acc: 0.8427122367230908 | Test GradNorm: 0.0004900199494225676\n",
      "Train Loss: 0.30855288523717 | Train Acc: 0.8529595015576324 | Train GradNorm: 4.4343653471617684e-05\n",
      "Epoch [45/500]\n",
      "Test Loss: 0.3442257875582228 | Test Acc: 0.8414846879441789 | Test GradNorm: 0.0004957511637915036\n",
      "Train Loss: 0.3089817693126695 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.0008183949864330528\n",
      "Epoch [46/500]\n",
      "Test Loss: 0.3444140390099423 | Test Acc: 0.8423891975707456 | Test GradNorm: 0.0004562658203942535\n",
      "Train Loss: 0.3083319760687513 | Train Acc: 0.8535825545171339 | Train GradNorm: 7.026865642130823e-05\n",
      "Epoch [47/500]\n",
      "Test Loss: 0.345153795189173 | Test Acc: 0.8417754231812896 | Test GradNorm: 0.0011402801235636632\n",
      "Train Loss: 0.3082428586435932 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.00013204575399974213\n",
      "Epoch [48/500]\n",
      "Test Loss: 0.3458472644792745 | Test Acc: 0.8415815996898824 | Test GradNorm: 0.0018954870201745443\n",
      "Train Loss: 0.3083889115064325 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.0005203722862078532\n",
      "Epoch [49/500]\n",
      "Test Loss: 0.3459142711039767 | Test Acc: 0.8418723349269932 | Test GradNorm: 0.001773317933998746\n",
      "Train Loss: 0.30823235171853103 | Train Acc: 0.8585669781931464 | Train GradNorm: 0.00045714417553923277\n",
      "Epoch [50/500]\n",
      "Test Loss: 0.3450283614370373 | Test Acc: 0.8417754231812896 | Test GradNorm: 0.00038904001710478683\n",
      "Train Loss: 0.3080546975218798 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.0003419855650615164\n",
      "Epoch [51/500]\n",
      "Test Loss: 0.345414834102708 | Test Acc: 0.8423245897402766 | Test GradNorm: 0.0008549119689295487\n",
      "Train Loss: 0.30773552999970816 | Train Acc: 0.8573208722741433 | Train GradNorm: 3.8586807083397706e-05\n",
      "Epoch [52/500]\n",
      "Test Loss: 0.34534679123376694 | Test Acc: 0.842292285825042 | Test GradNorm: 0.0005781277688488382\n",
      "Train Loss: 0.30761767899906167 | Train Acc: 0.8560747663551402 | Train GradNorm: 1.4296196082135314e-05\n",
      "Epoch [53/500]\n",
      "Test Loss: 0.34536000321751587 | Test Acc: 0.8418723349269932 | Test GradNorm: 0.0004089527919587888\n",
      "Train Loss: 0.3076106081656196 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.0001464868457643038\n",
      "Epoch [54/500]\n",
      "Test Loss: 0.34552825220810374 | Test Acc: 0.8419369427574622 | Test GradNorm: 0.00047298234745885487\n",
      "Train Loss: 0.30745362964149936 | Train Acc: 0.8542056074766355 | Train GradNorm: 5.983011805755294e-05\n",
      "Epoch [55/500]\n",
      "Test Loss: 0.3456682013180301 | Test Acc: 0.8419369427574622 | Test GradNorm: 0.0005247012295671261\n",
      "Train Loss: 0.30733624035174756 | Train Acc: 0.854828660436137 | Train GradNorm: 2.949093974785661e-05\n",
      "Epoch [56/500]\n",
      "Test Loss: 0.3457669931830347 | Test Acc: 0.841743119266055 | Test GradNorm: 0.00048350233527625237\n",
      "Train Loss: 0.30725319484460634 | Train Acc: 0.854828660436137 | Train GradNorm: 5.076066461297192e-05\n",
      "Epoch [57/500]\n",
      "Test Loss: 0.34625079862497926 | Test Acc: 0.8418723349269932 | Test GradNorm: 0.0010849387274251118\n",
      "Train Loss: 0.3072061430523188 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.00010945000122091957\n",
      "Epoch [58/500]\n",
      "Test Loss: 0.34583413667755764 | Test Acc: 0.8416139036051169 | Test GradNorm: 0.00041358433610619294\n",
      "Train Loss: 0.30714278977190873 | Train Acc: 0.8542056074766355 | Train GradNorm: 0.00012927399226630174\n",
      "Epoch [59/500]\n",
      "Test Loss: 0.346050218738594 | Test Acc: 0.8418077270965241 | Test GradNorm: 0.000592530452518823\n",
      "Train Loss: 0.30697479644084247 | Train Acc: 0.8560747663551402 | Train GradNorm: 1.1527943982485099e-05\n",
      "Epoch [60/500]\n",
      "Test Loss: 0.3465133719690054 | Test Acc: 0.8418723349269932 | Test GradNorm: 0.0011594064529134838\n",
      "Train Loss: 0.3069724402518561 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.00013851346997230285\n",
      "Epoch [61/500]\n",
      "Test Loss: 0.34624278808434605 | Test Acc: 0.8415815996898824 | Test GradNorm: 0.0005531691343128116\n",
      "Train Loss: 0.3068113407606152 | Train Acc: 0.8560747663551402 | Train GradNorm: 1.8376427149037676e-05\n",
      "Epoch [62/500]\n",
      "Test Loss: 0.3467620114000507 | Test Acc: 0.841743119266055 | Test GradNorm: 0.0011231836912874444\n",
      "Train Loss: 0.30679999903236155 | Train Acc: 0.8566978193146417 | Train GradNorm: 0.00012517867061455877\n",
      "Epoch [63/500]\n",
      "Test Loss: 0.3464532738181055 | Test Acc: 0.8419369427574622 | Test GradNorm: 0.0005718667950014961\n",
      "Train Loss: 0.3066538980195347 | Train Acc: 0.8566978193146417 | Train GradNorm: 1.4432280823332377e-05\n",
      "Epoch [64/500]\n",
      "Test Loss: 0.34659460470698505 | Test Acc: 0.8416462075203515 | Test GradNorm: 0.0005162986607973635\n",
      "Train Loss: 0.30658860554257045 | Train Acc: 0.8566978193146417 | Train GradNorm: 3.383224143740378e-05\n",
      "Epoch [65/500]\n",
      "Test Loss: 0.3466235456810282 | Test Acc: 0.8415169918594133 | Test GradNorm: 0.0004102868275744266\n",
      "Train Loss: 0.30660110916619127 | Train Acc: 0.854828660436137 | Train GradNorm: 0.00015933820828856988\n",
      "Epoch [66/500]\n",
      "Test Loss: 0.34685507927497067 | Test Acc: 0.8414846879441789 | Test GradNorm: 0.0004958600787324136\n",
      "Train Loss: 0.306446550587038 | Train Acc: 0.8566978193146417 | Train GradNorm: 4.708517058491442e-05\n",
      "Epoch [67/500]\n",
      "Test Loss: 0.346957311531925 | Test Acc: 0.8413877761984753 | Test GradNorm: 0.00047036076620440755\n",
      "Train Loss: 0.3063878322467931 | Train Acc: 0.8560747663551402 | Train GradNorm: 6.850924203355364e-05\n",
      "Epoch [68/500]\n",
      "Test Loss: 0.347107275285422 | Test Acc: 0.8412262566223027 | Test GradNorm: 0.0004917551134737997\n",
      "Train Loss: 0.30630590081322046 | Train Acc: 0.8566978193146417 | Train GradNorm: 5.326077967271969e-05\n",
      "Epoch [69/500]\n",
      "Test Loss: 0.3476750131605028 | Test Acc: 0.8411293448765991 | Test GradNorm: 0.0011086885021955929\n",
      "Train Loss: 0.3062720404470012 | Train Acc: 0.859190031152648 | Train GradNorm: 0.00011469690181869119\n",
      "Epoch [70/500]\n",
      "Test Loss: 0.34751232414768113 | Test Acc: 0.8414846879441789 | Test GradNorm: 0.0008162871503674419\n",
      "Train Loss: 0.30614253876784975 | Train Acc: 0.8573208722741433 | Train GradNorm: 2.286464256753806e-05\n",
      "Epoch [71/500]\n",
      "Test Loss: 0.34734839194159123 | Test Acc: 0.8411293448765991 | Test GradNorm: 0.00041661720923444844\n",
      "Train Loss: 0.3061802100424456 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.0001647162331622484\n",
      "Epoch [72/500]\n",
      "Test Loss: 0.3478751360343768 | Test Acc: 0.8411939527070681 | Test GradNorm: 0.0009806227377999854\n",
      "Train Loss: 0.306046361142978 | Train Acc: 0.8579439252336448 | Train GradNorm: 7.007924380312614e-05\n",
      "Epoch [73/500]\n",
      "Test Loss: 0.34785659964254423 | Test Acc: 0.8410001292156609 | Test GradNorm: 0.0007241800712302835\n",
      "Train Loss: 0.30594416574073036 | Train Acc: 0.8560747663551402 | Train GradNorm: 8.501116155101583e-06\n",
      "Epoch [74/500]\n",
      "Test Loss: 0.34795815149957393 | Test Acc: 0.8405801783176121 | Test GradNorm: 0.0006506923568476388\n",
      "Train Loss: 0.30587855471887726 | Train Acc: 0.8566978193146417 | Train GradNorm: 5.915780973454089e-06\n",
      "Epoch [75/500]\n",
      "Test Loss: 0.34799435480689817 | Test Acc: 0.8406447861480811 | Test GradNorm: 0.0006200373855969305\n",
      "Train Loss: 0.30581918044530076 | Train Acc: 0.8566978193146417 | Train GradNorm: 7.650754172219768e-06\n",
      "Epoch [76/500]\n",
      "Test Loss: 0.3480964195350313 | Test Acc: 0.840450962656674 | Test GradNorm: 0.0006205977893178707\n",
      "Train Loss: 0.3057559999272493 | Train Acc: 0.8560747663551402 | Train GradNorm: 7.641516276010431e-06\n",
      "Epoch [77/500]\n",
      "Test Loss: 0.34826434320845867 | Test Acc: 0.8407416978937847 | Test GradNorm: 0.0006991292165517927\n",
      "Train Loss: 0.3056919814306593 | Train Acc: 0.8566978193146417 | Train GradNorm: 5.729374064535897e-06\n",
      "Epoch [78/500]\n",
      "Test Loss: 0.3482098626139173 | Test Acc: 0.8405155704871431 | Test GradNorm: 0.0005259205731266599\n",
      "Train Loss: 0.30565625295998566 | Train Acc: 0.8560747663551402 | Train GradNorm: 3.309182884469339e-05\n",
      "Epoch [79/500]\n",
      "Test Loss: 0.34873963256874246 | Test Acc: 0.8411939527070681 | Test GradNorm: 0.0011330670700448148\n",
      "Train Loss: 0.30566279644051386 | Train Acc: 0.8610591900311526 | Train GradNorm: 0.00012359305605117802\n",
      "Epoch [80/500]\n",
      "Test Loss: 0.3484293481710994 | Test Acc: 0.8405155704871431 | Test GradNorm: 0.0006187046840660951\n",
      "Train Loss: 0.30552626385008985 | Train Acc: 0.8566978193146417 | Train GradNorm: 7.10839951753569e-06\n",
      "Epoch [81/500]\n",
      "Test Loss: 0.3484229464191507 | Test Acc: 0.8404186587414395 | Test GradNorm: 0.0005518154894626667\n",
      "Train Loss: 0.30547409703697515 | Train Acc: 0.8560747663551402 | Train GradNorm: 2.1311274074413734e-05\n",
      "Epoch [82/500]\n",
      "Test Loss: 0.34846618613100516 | Test Acc: 0.840450962656674 | Test GradNorm: 0.0005185305543251989\n",
      "Train Loss: 0.30542537794100155 | Train Acc: 0.8573208722741433 | Train GradNorm: 3.6138232653445505e-05\n",
      "Epoch [83/500]\n",
      "Test Loss: 0.34866640585874337 | Test Acc: 0.8405801783176121 | Test GradNorm: 0.0006924492364996073\n",
      "Train Loss: 0.30535127030377496 | Train Acc: 0.8573208722741433 | Train GradNorm: 5.011450535826868e-06\n",
      "Epoch [84/500]\n",
      "Test Loss: 0.34860645664866974 | Test Acc: 0.8409678253004265 | Test GradNorm: 0.0004137181985086455\n",
      "Train Loss: 0.3054281625314515 | Train Acc: 0.8573208722741433 | Train GradNorm: 0.00018629277568033158\n",
      "Epoch [85/500]\n",
      "Test Loss: 0.3490108628716021 | Test Acc: 0.8408709135547229 | Test GradNorm: 0.0008646034676801323\n",
      "Train Loss: 0.30526484332732456 | Train Acc: 0.8585669781931464 | Train GradNorm: 3.226447532781296e-05\n",
      "Epoch [86/500]\n",
      "Test Loss: 0.349164117656815 | Test Acc: 0.8410001292156609 | Test GradNorm: 0.0009825221443053806\n",
      "Train Loss: 0.30523953191567843 | Train Acc: 0.8598130841121495 | Train GradNorm: 6.778172084849923e-05\n",
      "Epoch [87/500]\n",
      "Test Loss: 0.348841452136205 | Test Acc: 0.8407093939785502 | Test GradNorm: 0.0004196429113247064\n",
      "Train Loss: 0.30526779367176715 | Train Acc: 0.8560747663551402 | Train GradNorm: 0.00016499803883862396\n",
      "Epoch [88/500]\n",
      "Test Loss: 0.3491756119730079 | Test Acc: 0.8405155704871431 | Test GradNorm: 0.0007901822504678541\n",
      "Train Loss: 0.3051147537787762 | Train Acc: 0.8573208722741433 | Train GradNorm: 1.6913443077813187e-05\n",
      "Epoch [89/500]\n",
      "Test Loss: 0.34916056027622056 | Test Acc: 0.8402248352500323 | Test GradNorm: 0.0006623384082134826\n",
      "Train Loss: 0.3050609551558366 | Train Acc: 0.8566978193146417 | Train GradNorm: 4.427647685023427e-06\n",
      "Epoch [90/500]\n",
      "Test Loss: 0.34919447446058 | Test Acc: 0.8400633156738597 | Test GradNorm: 0.0006526803598192751\n",
      "Train Loss: 0.3050136748107165 | Train Acc: 0.8573208722741433 | Train GradNorm: 4.053496706523822e-06\n",
      "Epoch [91/500]\n",
      "Test Loss: 0.3492394795720949 | Test Acc: 0.8402571391652668 | Test GradNorm: 0.0006706608530151041\n",
      "Train Loss: 0.3049691232331454 | Train Acc: 0.8573208722741433 | Train GradNorm: 4.041177691374088e-06\n",
      "Epoch [92/500]\n",
      "Test Loss: 0.3492937019652827 | Test Acc: 0.8403540509109704 | Test GradNorm: 0.0006039829873275451\n",
      "Train Loss: 0.30492691976790265 | Train Acc: 0.8585669781931464 | Train GradNorm: 8.022415668171204e-06\n",
      "Epoch [93/500]\n",
      "Test Loss: 0.34937424218656077 | Test Acc: 0.8402248352500323 | Test GradNorm: 0.0005580308153689932\n",
      "Train Loss: 0.3048926095448732 | Train Acc: 0.859190031152648 | Train GradNorm: 1.8943677755353157e-05\n",
      "Epoch [94/500]\n",
      "Test Loss: 0.3498037498643665 | Test Acc: 0.8406447861480811 | Test GradNorm: 0.0009538904967279522\n",
      "Train Loss: 0.30487335475422106 | Train Acc: 0.8604361370716511 | Train GradNorm: 5.687175644046298e-05\n",
      "Epoch [95/500]\n",
      "Test Loss: 0.34986473859661016 | Test Acc: 0.8400956195890942 | Test GradNorm: 0.0008767184779248439\n",
      "Train Loss: 0.30480789502985034 | Train Acc: 0.8585669781931464 | Train GradNorm: 3.377242294831838e-05\n",
      "Epoch [96/500]\n",
      "Test Loss: 0.3498352702515547 | Test Acc: 0.8400310117586252 | Test GradNorm: 0.0007429570105517994\n",
      "Train Loss: 0.3047518733890416 | Train Acc: 0.8585669781931464 | Train GradNorm: 8.26231736870187e-06\n",
      "Epoch [97/500]\n",
      "Test Loss: 0.3496928138970027 | Test Acc: 0.8399987078433906 | Test GradNorm: 0.0005329087427546771\n",
      "Train Loss: 0.304724387752597 | Train Acc: 0.859190031152648 | Train GradNorm: 2.890203195663004e-05\n",
      "Epoch [98/500]\n",
      "Test Loss: 0.3498010659661991 | Test Acc: 0.8399341000129216 | Test GradNorm: 0.000581863458982328\n",
      "Train Loss: 0.3046696110387241 | Train Acc: 0.8585669781931464 | Train GradNorm: 1.2574952659182616e-05\n",
      "Epoch [99/500]\n",
      "Test Loss: 0.34990188848423426 | Test Acc: 0.8398694921824525 | Test GradNorm: 0.0006377824337322911\n",
      "Train Loss: 0.3046224200456339 | Train Acc: 0.8579439252336448 | Train GradNorm: 4.319621141212626e-06\n",
      "Epoch [100/500]\n",
      "Test Loss: 0.3498194199025476 | Test Acc: 0.8402571391652668 | Test GradNorm: 0.0004760623931113882\n",
      "Train Loss: 0.30463371208536605 | Train Acc: 0.859190031152648 | Train GradNorm: 6.531769242700348e-05\n",
      "Epoch [101/500]\n",
      "Test Loss: 0.3501003042645926 | Test Acc: 0.8399664039281561 | Test GradNorm: 0.0007450904536584825\n",
      "Train Loss: 0.304554175095464 | Train Acc: 0.8579439252336448 | Train GradNorm: 8.705813622994553e-06\n",
      "Epoch [102/500]\n",
      "Test Loss: 0.35032774454077453 | Test Acc: 0.8402571391652668 | Test GradNorm: 0.0009899052422112612\n",
      "Train Loss: 0.3045511840113028 | Train Acc: 0.8585669781931464 | Train GradNorm: 6.87724754317813e-05\n",
      "Epoch [103/500]\n",
      "Test Loss: 0.35016308891862835 | Test Acc: 0.8399664039281561 | Test GradNorm: 0.0007129309013430132\n",
      "Train Loss: 0.3044686420957305 | Train Acc: 0.8566978193146417 | Train GradNorm: 5.6091237103036546e-06\n",
      "Epoch [104/500]\n",
      "Test Loss: 0.3501433643289855 | Test Acc: 0.8398694921824525 | Test GradNorm: 0.0005428758133627598\n",
      "Train Loss: 0.30444131743287783 | Train Acc: 0.8579439252336448 | Train GradNorm: 2.255567127414079e-05\n",
      "Epoch [105/500]\n",
      "Test Loss: 0.35063070639009747 | Test Acc: 0.8400956195890942 | Test GradNorm: 0.001115101520292407\n",
      "Train Loss: 0.30446695320440414 | Train Acc: 0.8604361370716511 | Train GradNorm: 0.00011642645679183998\n",
      "Epoch [106/500]\n",
      "Test Loss: 0.3504802743919318 | Test Acc: 0.839772580436749 | Test GradNorm: 0.0007479313692149291\n",
      "Train Loss: 0.30435582522177007 | Train Acc: 0.8579439252336448 | Train GradNorm: 9.096706347828942e-06\n",
      "Epoch [107/500]\n",
      "Test Loss: 0.35046839171994737 | Test Acc: 0.8397402765215144 | Test GradNorm: 0.0006062896809733945\n",
      "Train Loss: 0.3043191706191876 | Train Acc: 0.8560747663551402 | Train GradNorm: 7.408171456670122e-06\n",
      "Epoch [108/500]\n",
      "Test Loss: 0.3505452977802009 | Test Acc: 0.8397402765215144 | Test GradNorm: 0.0006270396239266135\n",
      "Train Loss: 0.30428040987029004 | Train Acc: 0.8560747663551402 | Train GradNorm: 4.872848687592188e-06\n",
      "Epoch [109/500]\n",
      "Test Loss: 0.3508481572856057 | Test Acc: 0.8402571391652668 | Test GradNorm: 0.0009555845575789607\n",
      "Train Loss: 0.30428300490022525 | Train Acc: 0.8585669781931464 | Train GradNorm: 5.857703687737498e-05\n",
      "Epoch [110/500]\n",
      "Test Loss: 0.3507562660925188 | Test Acc: 0.8398694921824525 | Test GradNorm: 0.0007623014473058181\n",
      "Train Loss: 0.3042130259521963 | Train Acc: 0.8573208722741433 | Train GradNorm: 1.1347268822976557e-05\n",
      "Epoch [111/500]\n",
      "Test Loss: 0.3505737459961455 | Test Acc: 0.839837188267218 | Test GradNorm: 0.00044779507583728043\n",
      "Train Loss: 0.30424130696860496 | Train Acc: 0.8573208722741433 | Train GradNorm: 9.489136543843292e-05\n",
      "Epoch [112/500]\n",
      "Test Loss: 0.3506654795762789 | Test Acc: 0.8400633156738597 | Test GradNorm: 0.0004623429320167363\n",
      "Train Loss: 0.3041960384056708 | Train Acc: 0.8573208722741433 | Train GradNorm: 7.626957778311527e-05\n",
      "Epoch [113/500]\n",
      "Test Loss: 0.35081906138195845 | Test Acc: 0.8397402765215144 | Test GradNorm: 0.0006343523394328355\n",
      "Train Loss: 0.3041100393988986 | Train Acc: 0.8566978193146417 | Train GradNorm: 3.898140402742426e-06\n",
      "Epoch [114/500]\n",
      "Test Loss: 0.35120394230717067 | Test Acc: 0.8399987078433906 | Test GradNorm: 0.0011435856328930688\n",
      "Train Loss: 0.3041665974557397 | Train Acc: 0.8610591900311526 | Train GradNorm: 0.00013209244764555723\n",
      "Epoch [115/500]\n",
      "Test Loss: 0.3509710728483932 | Test Acc: 0.839901796097687 | Test GradNorm: 0.0007467700419947126\n",
      "Train Loss: 0.3040489410421865 | Train Acc: 0.8560747663551402 | Train GradNorm: 1.0660033007569257e-05\n",
      "Epoch [116/500]\n",
      "Test Loss: 0.3510636870500939 | Test Acc: 0.8401279235043287 | Test GradNorm: 0.0008424450903248481\n",
      "Train Loss: 0.30402841772102407 | Train Acc: 0.8585669781931464 | Train GradNorm: 2.9103070449639824e-05\n",
      "Epoch [117/500]\n",
      "Test Loss: 0.35105718250262785 | Test Acc: 0.8398048843519834 | Test GradNorm: 0.0007614758744923358\n",
      "Train Loss: 0.30398631175695773 | Train Acc: 0.8566978193146417 | Train GradNorm: 1.2773121383066526e-05\n",
      "Epoch [118/500]\n",
      "Test Loss: 0.35106097819940774 | Test Acc: 0.8396110608605764 | Test GradNorm: 0.0006683635916696307\n",
      "Train Loss: 0.3039489905822611 | Train Acc: 0.8573208722741433 | Train GradNorm: 4.020173258050287e-06\n",
      "Epoch [119/500]\n",
      "Test Loss: 0.35123888850920937 | Test Acc: 0.839837188267218 | Test GradNorm: 0.0008040654380836864\n",
      "Train Loss: 0.30392801192219543 | Train Acc: 0.8585669781931464 | Train GradNorm: 1.9775256471321475e-05\n",
      "Epoch [120/500]\n",
      "Test Loss: 0.35134441403605793 | Test Acc: 0.839901796097687 | Test GradNorm: 0.000798387147423167\n",
      "Train Loss: 0.30389015852432333 | Train Acc: 0.859190031152648 | Train GradNorm: 1.7181601499809078e-05\n",
      "Epoch [121/500]\n",
      "Test Loss: 0.35115811112417894 | Test Acc: 0.8397402765215144 | Test GradNorm: 0.00047917147691571544\n",
      "Train Loss: 0.3038937772375406 | Train Acc: 0.8579439252336448 | Train GradNorm: 5.882170361578797e-05\n",
      "Epoch [122/500]\n",
      "Test Loss: 0.3515437123328852 | Test Acc: 0.8400633156738597 | Test GradNorm: 0.0008808094914576369\n",
      "Train Loss: 0.30385013724628707 | Train Acc: 0.8585669781931464 | Train GradNorm: 3.563527035222186e-05\n",
      "Epoch [123/500]\n",
      "Test Loss: 0.35130506878537515 | Test Acc: 0.8394172373691692 | Test GradNorm: 0.0005095732764085385\n",
      "Train Loss: 0.3038222379697309 | Train Acc: 0.8585669781931464 | Train GradNorm: 3.640283463488496e-05\n",
      "Epoch [124/500]\n",
      "Test Loss: 0.35181216883359856 | Test Acc: 0.839772580436749 | Test GradNorm: 0.0010725804847508975\n",
      "Train Loss: 0.3038361497579806 | Train Acc: 0.8610591900311526 | Train GradNorm: 0.0001010523248815124\n",
      "Epoch [125/500]\n",
      "Test Loss: 0.3518335404139399 | Test Acc: 0.8397402765215144 | Test GradNorm: 0.0010110172988896885\n",
      "Train Loss: 0.30378909668784476 | Train Acc: 0.8610591900311526 | Train GradNorm: 7.80332261015145e-05\n",
      "Epoch [126/500]\n",
      "Test Loss: 0.35180988113569145 | Test Acc: 0.839837188267218 | Test GradNorm: 0.0008718823510578643\n",
      "Train Loss: 0.3037287093583329 | Train Acc: 0.859190031152648 | Train GradNorm: 3.339067097233171e-05\n",
      "Epoch [127/500]\n",
      "Test Loss: 0.3520022572223924 | Test Acc: 0.8396756686910454 | Test GradNorm: 0.0010474912495931725\n",
      "Train Loss: 0.30374235547844325 | Train Acc: 0.8598130841121495 | Train GradNorm: 8.918280463597123e-05\n",
      "Epoch [128/500]\n",
      "Test Loss: 0.35181278807915156 | Test Acc: 0.8398048843519834 | Test GradNorm: 0.0006833349725742257\n",
      "Train Loss: 0.3036512793950899 | Train Acc: 0.8579439252336448 | Train GradNorm: 3.051740453225644e-06\n",
      "Epoch [129/500]\n",
      "Test Loss: 0.35205143601302474 | Test Acc: 0.8400310117586252 | Test GradNorm: 0.0008984837787050487\n",
      "Train Loss: 0.303649179331391 | Train Acc: 0.8598130841121495 | Train GradNorm: 3.986310455452508e-05\n",
      "Epoch [130/500]\n",
      "Test Loss: 0.3519788887276425 | Test Acc: 0.8394495412844036 | Test GradNorm: 0.0006343542889476937\n",
      "Train Loss: 0.3035956920411211 | Train Acc: 0.8598130841121495 | Train GradNorm: 3.800993696643548e-06\n",
      "Epoch [131/500]\n",
      "Test Loss: 0.35190771313462854 | Test Acc: 0.8393526295387 | Test GradNorm: 0.0005143803739368604\n",
      "Train Loss: 0.30359605881756224 | Train Acc: 0.859190031152648 | Train GradNorm: 3.802219081225423e-05\n",
      "Epoch [132/500]\n",
      "Test Loss: 0.35198001939213835 | Test Acc: 0.8393849334539346 | Test GradNorm: 0.0005329232396757077\n",
      "Train Loss: 0.30356259046512196 | Train Acc: 0.8598130841121495 | Train GradNorm: 2.8077488458853808e-05\n",
      "Epoch [133/500]\n",
      "Test Loss: 0.35204503415335936 | Test Acc: 0.8393203256234656 | Test GradNorm: 0.0005625142997642994\n",
      "Train Loss: 0.3035289243627475 | Train Acc: 0.8598130841121495 | Train GradNorm: 1.6773534662258826e-05\n",
      "Epoch [134/500]\n",
      "Test Loss: 0.35212212370817836 | Test Acc: 0.8396110608605764 | Test GradNorm: 0.0006657612884533174\n",
      "Train Loss: 0.30349499129985574 | Train Acc: 0.8585669781931464 | Train GradNorm: 2.6371184614678507e-06\n",
      "Epoch [135/500]\n",
      "Test Loss: 0.352381134053069 | Test Acc: 0.8396110608605764 | Test GradNorm: 0.0009730117148314352\n",
      "Train Loss: 0.303511773086186 | Train Acc: 0.8616822429906542 | Train GradNorm: 6.411729967439002e-05\n",
      "Epoch [136/500]\n",
      "Test Loss: 0.3521861768102482 | Test Acc: 0.8395464530301072 | Test GradNorm: 0.0005797432967203758\n",
      "Train Loss: 0.3034499122508553 | Train Acc: 0.8598130841121495 | Train GradNorm: 1.1040150418079335e-05\n",
      "Epoch [137/500]\n",
      "Test Loss: 0.352441754798738 | Test Acc: 0.8399341000129216 | Test GradNorm: 0.0008622948130288674\n",
      "Train Loss: 0.30343918702586775 | Train Acc: 0.8585669781931464 | Train GradNorm: 3.1374355668320135e-05\n",
      "Epoch [138/500]\n",
      "Test Loss: 0.35236646730509125 | Test Acc: 0.8394172373691692 | Test GradNorm: 0.0005976995705547715\n",
      "Train Loss: 0.303398742427394 | Train Acc: 0.8579439252336448 | Train GradNorm: 7.835318169517705e-06\n",
      "Epoch [139/500]\n",
      "Test Loss: 0.352323686543246 | Test Acc: 0.8394495412844036 | Test GradNorm: 0.0004957615011681272\n",
      "Train Loss: 0.30340705363148907 | Train Acc: 0.859190031152648 | Train GradNorm: 4.9615507414278995e-05\n",
      "Epoch [140/500]\n",
      "Test Loss: 0.3527237151907061 | Test Acc: 0.8395464530301072 | Test GradNorm: 0.0009547537383334779\n",
      "Train Loss: 0.30338453369368934 | Train Acc: 0.8610591900311526 | Train GradNorm: 5.76032880641565e-05\n",
      "Epoch [141/500]\n",
      "Test Loss: 0.3528009023860834 | Test Acc: 0.8394495412844036 | Test GradNorm: 0.0009304391032099924\n",
      "Train Loss: 0.30335299499926377 | Train Acc: 0.8610591900311526 | Train GradNorm: 4.907060579102888e-05\n",
      "Epoch [142/500]\n",
      "Test Loss: 0.3525397759444788 | Test Acc: 0.8393849334539346 | Test GradNorm: 0.0005618495844819401\n",
      "Train Loss: 0.30331077500957776 | Train Acc: 0.859190031152648 | Train GradNorm: 1.7095785056508802e-05\n",
      "Epoch [143/500]\n",
      "Test Loss: 0.3525043344290875 | Test Acc: 0.8394818451996382 | Test GradNorm: 0.0004181400526654364\n",
      "Train Loss: 0.30341279445467717 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.00019047030250459124\n",
      "Epoch [144/500]\n",
      "Test Loss: 0.3526189809231979 | Test Acc: 0.8394172373691692 | Test GradNorm: 0.0005128919688391322\n",
      "Train Loss: 0.30327686001457504 | Train Acc: 0.8598130841121495 | Train GradNorm: 3.805066635224828e-05\n",
      "Epoch [145/500]\n",
      "Test Loss: 0.3530377238493553 | Test Acc: 0.8393526295387 | Test GradNorm: 0.0009551834302683707\n",
      "Train Loss: 0.3032641211086736 | Train Acc: 0.8604361370716511 | Train GradNorm: 5.611813105025714e-05\n",
      "Epoch [146/500]\n",
      "Test Loss: 0.3530334394029664 | Test Acc: 0.8393203256234656 | Test GradNorm: 0.0007848708931745399\n",
      "Train Loss: 0.3032125015907498 | Train Acc: 0.859190031152648 | Train GradNorm: 1.2422503066764003e-05\n",
      "Epoch [147/500]\n",
      "Test Loss: 0.35317137563348877 | Test Acc: 0.8391265021320584 | Test GradNorm: 0.0008907922957086696\n",
      "Train Loss: 0.3032051953929999 | Train Acc: 0.8604361370716511 | Train GradNorm: 3.602989033336855e-05\n",
      "Epoch [148/500]\n",
      "Test Loss: 0.3530859166142614 | Test Acc: 0.839223413877762 | Test GradNorm: 0.00065779081237131\n",
      "Train Loss: 0.3031600432408546 | Train Acc: 0.859190031152648 | Train GradNorm: 2.406401018930329e-06\n",
      "Epoch [149/500]\n",
      "Test Loss: 0.35311008344792655 | Test Acc: 0.839223413877762 | Test GradNorm: 0.0005970819144437495\n",
      "Train Loss: 0.30314246729292416 | Train Acc: 0.8579439252336448 | Train GradNorm: 9.179603835486273e-06\n",
      "Epoch [150/500]\n",
      "Test Loss: 0.3533370080511388 | Test Acc: 0.8390295903863548 | Test GradNorm: 0.0009011451869426778\n",
      "Train Loss: 0.30314038524416476 | Train Acc: 0.8604361370716511 | Train GradNorm: 3.952328844775929e-05\n",
      "Epoch [151/500]\n",
      "Test Loss: 0.35338933186802385 | Test Acc: 0.839158806047293 | Test GradNorm: 0.0008148076132610246\n",
      "Train Loss: 0.3031039219546467 | Train Acc: 0.859190031152648 | Train GradNorm: 1.8003900580605604e-05\n",
      "Epoch [152/500]\n",
      "Test Loss: 0.35334872722933136 | Test Acc: 0.839288021708231 | Test GradNorm: 0.0006597564388577485\n",
      "Train Loss: 0.30307330604374955 | Train Acc: 0.859190031152648 | Train GradNorm: 2.588670418079813e-06\n",
      "Epoch [153/500]\n",
      "Test Loss: 0.35329866921036185 | Test Acc: 0.839158806047293 | Test GradNorm: 0.0005572159328420984\n",
      "Train Loss: 0.3030656508482771 | Train Acc: 0.8579439252336448 | Train GradNorm: 2.084748216692737e-05\n",
      "Epoch [154/500]\n",
      "Test Loss: 0.3535028338320436 | Test Acc: 0.8389972864711203 | Test GradNorm: 0.0007457309523925321\n",
      "Train Loss: 0.30303486771015525 | Train Acc: 0.859190031152648 | Train GradNorm: 6.470346862757667e-06\n",
      "Epoch [155/500]\n",
      "Test Loss: 0.3535972636118047 | Test Acc: 0.8388357668949477 | Test GradNorm: 0.0007917555958620771\n",
      "Train Loss: 0.30301826395501785 | Train Acc: 0.8598130841121495 | Train GradNorm: 1.2783673828464627e-05\n",
      "Epoch [156/500]\n",
      "Test Loss: 0.35356534083369345 | Test Acc: 0.8391911099625274 | Test GradNorm: 0.0006728457487549353\n",
      "Train Loss: 0.30298894162930085 | Train Acc: 0.8598130841121495 | Train GradNorm: 2.3339447378752677e-06\n",
      "Epoch [157/500]\n",
      "Test Loss: 0.3535714947657723 | Test Acc: 0.8389972864711203 | Test GradNorm: 0.0006554838227605958\n",
      "Train Loss: 0.3029697414480005 | Train Acc: 0.8585669781931464 | Train GradNorm: 2.7755431827383375e-06\n",
      "Epoch [158/500]\n",
      "Test Loss: 0.35359190509482813 | Test Acc: 0.8390618943015894 | Test GradNorm: 0.000662531511538553\n",
      "Train Loss: 0.30295080672191066 | Train Acc: 0.859190031152648 | Train GradNorm: 2.588396426855873e-06\n",
      "Epoch [159/500]\n",
      "Test Loss: 0.3535759345168909 | Test Acc: 0.8390295903863548 | Test GradNorm: 0.0005672372256588812\n",
      "Train Loss: 0.30294242541748045 | Train Acc: 0.859190031152648 | Train GradNorm: 1.7693867199951712e-05\n",
      "Epoch [160/500]\n",
      "Test Loss: 0.3539038125803311 | Test Acc: 0.8391265021320584 | Test GradNorm: 0.0008440975970456174\n",
      "Train Loss: 0.30292507653362166 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.3431686946799855e-05\n",
      "Epoch [161/500]\n",
      "Test Loss: 0.35379058201176905 | Test Acc: 0.839158806047293 | Test GradNorm: 0.0006180827844247447\n",
      "Train Loss: 0.30289295888197076 | Train Acc: 0.859190031152648 | Train GradNorm: 6.404469853612764e-06\n",
      "Epoch [162/500]\n",
      "Test Loss: 0.3538263894285845 | Test Acc: 0.8391911099625274 | Test GradNorm: 0.0006179538971833828\n",
      "Train Loss: 0.3028748165161183 | Train Acc: 0.859190031152648 | Train GradNorm: 6.4215105839709e-06\n",
      "Epoch [163/500]\n",
      "Test Loss: 0.3540077236472805 | Test Acc: 0.8388034629797131 | Test GradNorm: 0.0007831399624324759\n",
      "Train Loss: 0.3028609245730472 | Train Acc: 0.8598130841121495 | Train GradNorm: 1.1355453336560776e-05\n",
      "Epoch [164/500]\n",
      "Test Loss: 0.3538792728008679 | Test Acc: 0.839158806047293 | Test GradNorm: 0.0005758905755510093\n",
      "Train Loss: 0.30284522199367814 | Train Acc: 0.8585669781931464 | Train GradNorm: 1.5749874698680373e-05\n",
      "Epoch [165/500]\n",
      "Test Loss: 0.35412827157089916 | Test Acc: 0.8389649825558858 | Test GradNorm: 0.0008379758212145365\n",
      "Train Loss: 0.3028280469215424 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.2194718698852802e-05\n",
      "Epoch [166/500]\n",
      "Test Loss: 0.3539852190267971 | Test Acc: 0.8390618943015894 | Test GradNorm: 0.0005261001629344971\n",
      "Train Loss: 0.3028224425059197 | Train Acc: 0.8585669781931464 | Train GradNorm: 3.721042483463263e-05\n",
      "Epoch [167/500]\n",
      "Test Loss: 0.3541661632017928 | Test Acc: 0.8389649825558858 | Test GradNorm: 0.0006985794937257671\n",
      "Train Loss: 0.3027784697343346 | Train Acc: 0.8585669781931464 | Train GradNorm: 2.525764504520431e-06\n",
      "Epoch [168/500]\n",
      "Test Loss: 0.3542676231919234 | Test Acc: 0.8389972864711203 | Test GradNorm: 0.0008419563828776488\n",
      "Train Loss: 0.30277203039184813 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.3557580676043e-05\n",
      "Epoch [169/500]\n",
      "Test Loss: 0.3544217049761252 | Test Acc: 0.8387065512340096 | Test GradNorm: 0.0009684938436022675\n",
      "Train Loss: 0.302779669911715 | Train Acc: 0.8598130841121495 | Train GradNorm: 5.852468313954429e-05\n",
      "Epoch [170/500]\n",
      "Test Loss: 0.3543020822059418 | Test Acc: 0.8389326786406512 | Test GradNorm: 0.000769772376757844\n",
      "Train Loss: 0.3027270729153483 | Train Acc: 0.8585669781931464 | Train GradNorm: 9.783938638409107e-06\n",
      "Epoch [171/500]\n",
      "Test Loss: 0.354280540796806 | Test Acc: 0.839158806047293 | Test GradNorm: 0.0006466874463699921\n",
      "Train Loss: 0.302703510685992 | Train Acc: 0.859190031152648 | Train GradNorm: 2.954212774552843e-06\n",
      "Epoch [172/500]\n",
      "Test Loss: 0.354333568584919 | Test Acc: 0.8390941982168238 | Test GradNorm: 0.0006185025682436534\n",
      "Train Loss: 0.3026874285232585 | Train Acc: 0.8585669781931464 | Train GradNorm: 6.071349201070692e-06\n",
      "Epoch [173/500]\n",
      "Test Loss: 0.3545234829325182 | Test Acc: 0.8388034629797131 | Test GradNorm: 0.0008273484089666793\n",
      "Train Loss: 0.3026787504855874 | Train Acc: 0.859190031152648 | Train GradNorm: 1.951516468226136e-05\n",
      "Epoch [174/500]\n",
      "Test Loss: 0.35459936316341384 | Test Acc: 0.8388034629797131 | Test GradNorm: 0.0008729292005093251\n",
      "Train Loss: 0.302669732468485 | Train Acc: 0.8604361370716511 | Train GradNorm: 3.0628528355493585e-05\n",
      "Epoch [175/500]\n",
      "Test Loss: 0.354518519995036 | Test Acc: 0.8389972864711203 | Test GradNorm: 0.0006764705312141863\n",
      "Train Loss: 0.3026333364014344 | Train Acc: 0.8585669781931464 | Train GradNorm: 2.146231507323991e-06\n",
      "Epoch [176/500]\n",
      "Test Loss: 0.35446157378843335 | Test Acc: 0.8389972864711203 | Test GradNorm: 0.0005628680801567818\n",
      "Train Loss: 0.3026274666834357 | Train Acc: 0.8585669781931464 | Train GradNorm: 1.949936477357587e-05\n",
      "Epoch [177/500]\n",
      "Test Loss: 0.3544964881038484 | Test Acc: 0.8390941982168238 | Test GradNorm: 0.0005802708824036856\n",
      "Train Loss: 0.3026046434611436 | Train Acc: 0.8585669781931464 | Train GradNorm: 1.3923605217022374e-05\n",
      "Epoch [178/500]\n",
      "Test Loss: 0.35452854973422665 | Test Acc: 0.8391265021320584 | Test GradNorm: 0.0005644185799137918\n",
      "Train Loss: 0.3025945396910176 | Train Acc: 0.8579439252336448 | Train GradNorm: 1.957994636307764e-05\n",
      "Epoch [179/500]\n",
      "Test Loss: 0.3545461978867973 | Test Acc: 0.8391911099625274 | Test GradNorm: 0.0005433041952247659\n",
      "Train Loss: 0.30258634769005593 | Train Acc: 0.8579439252336448 | Train GradNorm: 2.8525358631305214e-05\n",
      "Epoch [180/500]\n",
      "Test Loss: 0.3545913572513146 | Test Acc: 0.8390941982168238 | Test GradNorm: 0.0005112169650222207\n",
      "Train Loss: 0.3025849696380857 | Train Acc: 0.8579439252336448 | Train GradNorm: 4.8313250071779226e-05\n",
      "Epoch [181/500]\n",
      "Test Loss: 0.3547872718272933 | Test Acc: 0.8387388551492441 | Test GradNorm: 0.000751722870865677\n",
      "Train Loss: 0.30253699039338605 | Train Acc: 0.8604361370716511 | Train GradNorm: 6.854039170038055e-06\n",
      "Epoch [182/500]\n",
      "Test Loss: 0.35478941555368976 | Test Acc: 0.838609639488306 | Test GradNorm: 0.0007196183642367067\n",
      "Train Loss: 0.3025198340051819 | Train Acc: 0.8604361370716511 | Train GradNorm: 3.6189436354975342e-06\n",
      "Epoch [183/500]\n",
      "Test Loss: 0.3546606573284812 | Test Acc: 0.8390295903863548 | Test GradNorm: 0.0005045869265373483\n",
      "Train Loss: 0.3025393551006895 | Train Acc: 0.8579439252336448 | Train GradNorm: 5.268984009316281e-05\n",
      "Epoch [184/500]\n",
      "Test Loss: 0.3548741690073661 | Test Acc: 0.8385127277426024 | Test GradNorm: 0.0006940823210704482\n",
      "Train Loss: 0.3024878018123057 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.570538269921604e-06\n",
      "Epoch [185/500]\n",
      "Test Loss: 0.35497664280428154 | Test Acc: 0.8385127277426024 | Test GradNorm: 0.0007874982211526299\n",
      "Train Loss: 0.30247649111044994 | Train Acc: 0.8604361370716511 | Train GradNorm: 1.2556136347947184e-05\n",
      "Epoch [186/500]\n",
      "Test Loss: 0.3551647354577295 | Test Acc: 0.8384481199121333 | Test GradNorm: 0.0010124170346121227\n",
      "Train Loss: 0.30250363503728916 | Train Acc: 0.8598130841121495 | Train GradNorm: 7.46635685895815e-05\n",
      "Epoch [187/500]\n",
      "Test Loss: 0.35497685159077935 | Test Acc: 0.8388357668949477 | Test GradNorm: 0.0007386193102767083\n",
      "Train Loss: 0.3024397918036587 | Train Acc: 0.8585669781931464 | Train GradNorm: 5.9185180069960375e-06\n",
      "Epoch [188/500]\n",
      "Test Loss: 0.3548747023644663 | Test Acc: 0.8391911099625274 | Test GradNorm: 0.0004913395522516288\n",
      "Train Loss: 0.3024657620285158 | Train Acc: 0.8566978193146417 | Train GradNorm: 6.09322247780711e-05\n",
      "Epoch [189/500]\n",
      "Test Loss: 0.35499655568102023 | Test Acc: 0.8389326786406512 | Test GradNorm: 0.0005986059249609412\n",
      "Train Loss: 0.3024105460259812 | Train Acc: 0.859190031152648 | Train GradNorm: 8.439563064871819e-06\n",
      "Epoch [190/500]\n",
      "Test Loss: 0.3550678124586901 | Test Acc: 0.8388680708101822 | Test GradNorm: 0.0006712642347694978\n",
      "Train Loss: 0.3023885714909019 | Train Acc: 0.8585669781931464 | Train GradNorm: 1.6114336020931248e-06\n",
      "Epoch [191/500]\n",
      "Test Loss: 0.35522722814371893 | Test Acc: 0.8387065512340096 | Test GradNorm: 0.0008460315620692898\n",
      "Train Loss: 0.3023923015140143 | Train Acc: 0.8598130841121495 | Train GradNorm: 2.4456074697131507e-05\n",
      "Epoch [192/500]\n",
      "Test Loss: 0.3551943963153616 | Test Acc: 0.8389326786406512 | Test GradNorm: 0.0007493221378147985\n",
      "Train Loss: 0.3023641579522057 | Train Acc: 0.8585669781931464 | Train GradNorm: 6.744267684400655e-06\n",
      "Epoch [193/500]\n",
      "Test Loss: 0.3552049539492909 | Test Acc: 0.8389326786406512 | Test GradNorm: 0.0007235168263638962\n",
      "Train Loss: 0.30234818702031585 | Train Acc: 0.8585669781931464 | Train GradNorm: 4.092844280702927e-06\n",
      "Epoch [194/500]\n",
      "Test Loss: 0.35514396785048136 | Test Acc: 0.8389649825558858 | Test GradNorm: 0.0006145737730903403\n",
      "Train Loss: 0.30233377004056916 | Train Acc: 0.859190031152648 | Train GradNorm: 5.421666363428613e-06\n",
      "Epoch [195/500]\n",
      "Test Loss: 0.355508871324316 | Test Acc: 0.8385127277426024 | Test GradNorm: 0.0010464720190943848\n",
      "Train Loss: 0.30237931503208454 | Train Acc: 0.8604361370716511 | Train GradNorm: 8.878546695377876e-05\n",
      "Epoch [196/500]\n",
      "Test Loss: 0.3553725787981689 | Test Acc: 0.838674247318775 | Test GradNorm: 0.0007963749224573711\n",
      "Train Loss: 0.30231016086079066 | Train Acc: 0.8585669781931464 | Train GradNorm: 1.4551940999253783e-05\n",
      "Epoch [197/500]\n",
      "Test Loss: 0.35540113713545685 | Test Acc: 0.8386419434035405 | Test GradNorm: 0.0007927718449076001\n",
      "Train Loss: 0.3022961229164157 | Train Acc: 0.8585669781931464 | Train GradNorm: 1.3782218047574087e-05\n",
      "Epoch [198/500]\n",
      "Test Loss: 0.35551431163519226 | Test Acc: 0.8385127277426024 | Test GradNorm: 0.0008400156949902083\n",
      "Train Loss: 0.3022886563424088 | Train Acc: 0.8598130841121495 | Train GradNorm: 2.3175668691487454e-05\n",
      "Epoch [199/500]\n",
      "Test Loss: 0.35547646041674275 | Test Acc: 0.8387388551492441 | Test GradNorm: 0.0007519074975420242\n",
      "Train Loss: 0.3022653160913785 | Train Acc: 0.8579439252336448 | Train GradNorm: 6.967198967601378e-06\n",
      "Epoch [200/500]\n",
      "Test Loss: 0.35549248463415944 | Test Acc: 0.8389326786406512 | Test GradNorm: 0.0007294737305424498\n",
      "Train Loss: 0.30224684147736264 | Train Acc: 0.8579439252336448 | Train GradNorm: 4.473618590855113e-06\n",
      "Epoch [201/500]\n",
      "Test Loss: 0.35555616337598067 | Test Acc: 0.8387065512340096 | Test GradNorm: 0.000735673744752553\n",
      "Train Loss: 0.3022297892001085 | Train Acc: 0.8579439252336448 | Train GradNorm: 4.736117573629668e-06\n",
      "Epoch [202/500]\n",
      "Test Loss: 0.35569775072286586 | Test Acc: 0.8384481199121333 | Test GradNorm: 0.0008440238783738302\n",
      "Train Loss: 0.3022294533303536 | Train Acc: 0.8598130841121495 | Train GradNorm: 2.3354648277984114e-05\n",
      "Epoch [203/500]\n",
      "Test Loss: 0.35581934563957274 | Test Acc: 0.838674247318775 | Test GradNorm: 0.0009212473322740708\n",
      "Train Loss: 0.30222921025973354 | Train Acc: 0.8604361370716511 | Train GradNorm: 4.325144118022205e-05\n",
      "Epoch [204/500]\n",
      "Test Loss: 0.3557926993085574 | Test Acc: 0.8385773355730715 | Test GradNorm: 0.0008505507890099076\n",
      "Train Loss: 0.30220095588300816 | Train Acc: 0.8598130841121495 | Train GradNorm: 2.447665579614525e-05\n",
      "Epoch [205/500]\n",
      "Test Loss: 0.3557654774730315 | Test Acc: 0.8388357668949477 | Test GradNorm: 0.0007566273587254846\n",
      "Train Loss: 0.3021763416505352 | Train Acc: 0.8598130841121495 | Train GradNorm: 6.968616099617713e-06\n",
      "Epoch [206/500]\n",
      "Test Loss: 0.3556988947776316 | Test Acc: 0.8390295903863548 | Test GradNorm: 0.0006296654094625171\n",
      "Train Loss: 0.3021606536935201 | Train Acc: 0.8585669781931464 | Train GradNorm: 3.9421795961418435e-06\n",
      "Epoch [207/500]\n",
      "Test Loss: 0.355806522224599 | Test Acc: 0.8389972864711203 | Test GradNorm: 0.000744734784771772\n",
      "Train Loss: 0.3021492136814016 | Train Acc: 0.859190031152648 | Train GradNorm: 6.070168213717133e-06\n",
      "Epoch [208/500]\n",
      "Test Loss: 0.3557113517415698 | Test Acc: 0.8388357668949477 | Test GradNorm: 0.0005574227974214508\n",
      "Train Loss: 0.3021454341685139 | Train Acc: 0.8585669781931464 | Train GradNorm: 1.9505619870985593e-05\n",
      "Epoch [209/500]\n",
      "Test Loss: 0.35579527061954247 | Test Acc: 0.8388680708101822 | Test GradNorm: 0.0005521834632326785\n",
      "Train Loss: 0.3021330342833197 | Train Acc: 0.8585669781931464 | Train GradNorm: 2.225487372610981e-05\n",
      "Epoch [210/500]\n",
      "Test Loss: 0.3557441756417833 | Test Acc: 0.8390295903863548 | Test GradNorm: 0.00043493237046653285\n",
      "Train Loss: 0.302227723671898 | Train Acc: 0.8579439252336448 | Train GradNorm: 0.00017004678407832115\n",
      "Epoch [211/500]\n",
      "Test Loss: 0.3557855487762412 | Test Acc: 0.838674247318775 | Test GradNorm: 0.0004764240732386989\n",
      "Train Loss: 0.3021461571107065 | Train Acc: 0.8573208722741433 | Train GradNorm: 7.66117555829152e-05\n",
      "Epoch [212/500]\n",
      "Test Loss: 0.35578606616509206 | Test Acc: 0.8387388551492441 | Test GradNorm: 0.00046383400639934925\n",
      "Train Loss: 0.30214348038219085 | Train Acc: 0.8573208722741433 | Train GradNorm: 9.070683618050211e-05\n",
      "Epoch [213/500]\n",
      "Test Loss: 0.3559016928450512 | Test Acc: 0.8387065512340096 | Test GradNorm: 0.0005395025581243734\n",
      "Train Loss: 0.3020817980455043 | Train Acc: 0.8579439252336448 | Train GradNorm: 2.764635037035432e-05\n",
      "Epoch [214/500]\n",
      "Test Loss: 0.356006995206309 | Test Acc: 0.8387711590644786 | Test GradNorm: 0.0006001912544584684\n",
      "Train Loss: 0.30205436897082205 | Train Acc: 0.8585669781931464 | Train GradNorm: 8.521094978271872e-06\n",
      "Epoch [215/500]\n",
      "Test Loss: 0.3560238443206381 | Test Acc: 0.8388034629797131 | Test GradNorm: 0.0006067143402208384\n",
      "Train Loss: 0.30204218635865493 | Train Acc: 0.8604361370716511 | Train GradNorm: 7.343329709807531e-06\n",
      "Epoch [216/500]\n",
      "Test Loss: 0.3561067741204908 | Test Acc: 0.8385127277426024 | Test GradNorm: 0.0006644417418816412\n",
      "Train Loss: 0.3020260104469999 | Train Acc: 0.8604361370716511 | Train GradNorm: 1.959416298155101e-06\n",
      "Epoch [217/500]\n",
      "Test Loss: 0.35603051194897717 | Test Acc: 0.8385127277426024 | Test GradNorm: 0.0005221505847145582\n",
      "Train Loss: 0.3020386627740291 | Train Acc: 0.8585669781931464 | Train GradNorm: 3.625989483484642e-05\n",
      "Epoch [218/500]\n",
      "Test Loss: 0.3561751821954447 | Test Acc: 0.8386419434035405 | Test GradNorm: 0.0006928775495495323\n",
      "Train Loss: 0.30199916093904866 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.997622079396008e-06\n",
      "Epoch [219/500]\n",
      "Test Loss: 0.3562236804186532 | Test Acc: 0.8385127277426024 | Test GradNorm: 0.0006584033976282392\n",
      "Train Loss: 0.3019860736532537 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.784297180818695e-06\n",
      "Epoch [220/500]\n",
      "Test Loss: 0.35648875302983246 | Test Acc: 0.8384158159968988 | Test GradNorm: 0.0009748956542190206\n",
      "Train Loss: 0.30201562017170985 | Train Acc: 0.8598130841121495 | Train GradNorm: 6.239829185183204e-05\n",
      "Epoch [221/500]\n",
      "Test Loss: 0.3563995185371146 | Test Acc: 0.8385450316578369 | Test GradNorm: 0.0008357336657043698\n",
      "Train Loss: 0.30197734595745046 | Train Acc: 0.8610591900311526 | Train GradNorm: 2.3046796310005646e-05\n",
      "Epoch [222/500]\n",
      "Test Loss: 0.3564016535662906 | Test Acc: 0.8384158159968988 | Test GradNorm: 0.0007661010745322472\n",
      "Train Loss: 0.3019548486620067 | Train Acc: 0.8616822429906542 | Train GradNorm: 9.207526354158853e-06\n",
      "Epoch [223/500]\n",
      "Test Loss: 0.3563432407377069 | Test Acc: 0.838674247318775 | Test GradNorm: 0.0006242949512432172\n",
      "Train Loss: 0.30194080212472474 | Train Acc: 0.8610591900311526 | Train GradNorm: 4.061123072421609e-06\n",
      "Epoch [224/500]\n",
      "Test Loss: 0.35647822922130346 | Test Acc: 0.8383835120816643 | Test GradNorm: 0.0007488355206560179\n",
      "Train Loss: 0.3019310271869581 | Train Acc: 0.8610591900311526 | Train GradNorm: 6.387858902472877e-06\n",
      "Epoch [225/500]\n",
      "Test Loss: 0.3563443845276842 | Test Acc: 0.8385450316578369 | Test GradNorm: 0.0005336821428787766\n",
      "Train Loss: 0.30193854149543886 | Train Acc: 0.8598130841121495 | Train GradNorm: 3.1094273591119755e-05\n",
      "Epoch [226/500]\n",
      "Test Loss: 0.356547935666027 | Test Acc: 0.8382866003359607 | Test GradNorm: 0.0006949308824579037\n",
      "Train Loss: 0.3019049726813195 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.7368353510780114e-06\n",
      "Epoch [227/500]\n",
      "Test Loss: 0.35652005055255936 | Test Acc: 0.8384158159968988 | Test GradNorm: 0.000631406720603986\n",
      "Train Loss: 0.30189479920638634 | Train Acc: 0.8604361370716511 | Train GradNorm: 3.6613720457604284e-06\n",
      "Epoch [228/500]\n",
      "Test Loss: 0.3565272594774412 | Test Acc: 0.8383835120816643 | Test GradNorm: 0.0006777236394400816\n",
      "Train Loss: 0.30188129490965177 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.3656576452127614e-06\n",
      "Epoch [229/500]\n",
      "Test Loss: 0.35650441129411997 | Test Acc: 0.8388357668949477 | Test GradNorm: 0.0006308629201445376\n",
      "Train Loss: 0.3018703301931136 | Train Acc: 0.8604361370716511 | Train GradNorm: 3.107081544074523e-06\n",
      "Epoch [230/500]\n",
      "Test Loss: 0.35648103288082966 | Test Acc: 0.8387711590644786 | Test GradNorm: 0.0005668833394861086\n",
      "Train Loss: 0.3018675168024443 | Train Acc: 0.8598130841121495 | Train GradNorm: 1.553489058952774e-05\n",
      "Epoch [231/500]\n",
      "Test Loss: 0.35650268726974677 | Test Acc: 0.8383835120816643 | Test GradNorm: 0.0005141510520438161\n",
      "Train Loss: 0.3018740350283835 | Train Acc: 0.8604361370716511 | Train GradNorm: 3.997395954078995e-05\n",
      "Epoch [232/500]\n",
      "Test Loss: 0.356508934461508 | Test Acc: 0.8384158159968988 | Test GradNorm: 0.0005009996636183924\n",
      "Train Loss: 0.3018699125376638 | Train Acc: 0.8604361370716511 | Train GradNorm: 4.8643578392375364e-05\n",
      "Epoch [233/500]\n",
      "Test Loss: 0.35663338761552743 | Test Acc: 0.8385127277426024 | Test GradNorm: 0.0006440946099339929\n",
      "Train Loss: 0.30182533550853763 | Train Acc: 0.8598130841121495 | Train GradNorm: 2.030464754355532e-06\n",
      "Epoch [234/500]\n",
      "Test Loss: 0.357057997916848 | Test Acc: 0.8382542964207262 | Test GradNorm: 0.001094972848052679\n",
      "Train Loss: 0.30188732399272417 | Train Acc: 0.859190031152648 | Train GradNorm: 0.00010528405287611632\n",
      "Epoch [235/500]\n",
      "Test Loss: 0.3568053912704771 | Test Acc: 0.8383835120816643 | Test GradNorm: 0.0007232222886813332\n",
      "Train Loss: 0.30180334021264155 | Train Acc: 0.8610591900311526 | Train GradNorm: 3.6998724398405155e-06\n",
      "Epoch [236/500]\n",
      "Test Loss: 0.35705100179318067 | Test Acc: 0.8383189042511953 | Test GradNorm: 0.0009220051167773225\n",
      "Train Loss: 0.3018212114775134 | Train Acc: 0.8598130841121495 | Train GradNorm: 4.366532001649966e-05\n",
      "Epoch [237/500]\n",
      "Test Loss: 0.3569154970427231 | Test Acc: 0.8385127277426024 | Test GradNorm: 0.0006919255954882148\n",
      "Train Loss: 0.30178182097601935 | Train Acc: 0.8604361370716511 | Train GradNorm: 1.526506977530163e-06\n",
      "Epoch [238/500]\n",
      "Test Loss: 0.3569576453242679 | Test Acc: 0.8385450316578369 | Test GradNorm: 0.0007528187602685505\n",
      "Train Loss: 0.3017759735946724 | Train Acc: 0.8610591900311526 | Train GradNorm: 7.0038569887952125e-06\n",
      "Epoch [239/500]\n",
      "Test Loss: 0.35689282889245283 | Test Acc: 0.838609639488306 | Test GradNorm: 0.000639168073159758\n",
      "Train Loss: 0.3017647082302164 | Train Acc: 0.8598130841121495 | Train GradNorm: 2.88465988496913e-06\n",
      "Epoch [240/500]\n",
      "Test Loss: 0.35695879803674685 | Test Acc: 0.8385127277426024 | Test GradNorm: 0.0006627150584487179\n",
      "Train Loss: 0.30174968843104877 | Train Acc: 0.8598130841121495 | Train GradNorm: 1.4001122412866004e-06\n",
      "Epoch [241/500]\n",
      "Test Loss: 0.3570514347168765 | Test Acc: 0.8385450316578369 | Test GradNorm: 0.000719255822823697\n",
      "Train Loss: 0.3017411089719525 | Train Acc: 0.8610591900311526 | Train GradNorm: 3.1079594831818874e-06\n",
      "Epoch [242/500]\n",
      "Test Loss: 0.35695973945839815 | Test Acc: 0.8386419434035405 | Test GradNorm: 0.000515215016160814\n",
      "Train Loss: 0.30176037428139796 | Train Acc: 0.8579439252336448 | Train GradNorm: 4.250338876094197e-05\n",
      "Epoch [243/500]\n",
      "Test Loss: 0.3571174132390616 | Test Acc: 0.8386419434035405 | Test GradNorm: 0.0006242575517067016\n",
      "Train Loss: 0.3017204462254311 | Train Acc: 0.859190031152648 | Train GradNorm: 4.486388478597703e-06\n",
      "Epoch [244/500]\n",
      "Test Loss: 0.35710740693786175 | Test Acc: 0.8384481199121333 | Test GradNorm: 0.000556623397241065\n",
      "Train Loss: 0.3017234440635291 | Train Acc: 0.8579439252336448 | Train GradNorm: 2.1294521706044508e-05\n",
      "Epoch [245/500]\n",
      "Test Loss: 0.3571262523176846 | Test Acc: 0.838674247318775 | Test GradNorm: 0.0005447208750487171\n",
      "Train Loss: 0.3017160618664912 | Train Acc: 0.8579439252336448 | Train GradNorm: 2.5353051096613986e-05\n",
      "Epoch [246/500]\n",
      "Test Loss: 0.35723897583137815 | Test Acc: 0.8384158159968988 | Test GradNorm: 0.0006301695246082689\n",
      "Train Loss: 0.3016895141315646 | Train Acc: 0.8585669781931464 | Train GradNorm: 3.865729305547191e-06\n",
      "Epoch [247/500]\n",
      "Test Loss: 0.3572132172332736 | Test Acc: 0.8384481199121333 | Test GradNorm: 0.0005648112016240258\n",
      "Train Loss: 0.30169017214256194 | Train Acc: 0.8585669781931464 | Train GradNorm: 1.834264512432224e-05\n",
      "Epoch [248/500]\n",
      "Test Loss: 0.3573501520941444 | Test Acc: 0.8384804238273679 | Test GradNorm: 0.0007065584360889898\n",
      "Train Loss: 0.3016678464497397 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.069256791493191e-06\n",
      "Epoch [249/500]\n",
      "Test Loss: 0.3572828670784823 | Test Acc: 0.8383512081664297 | Test GradNorm: 0.0006063567734527699\n",
      "Train Loss: 0.3016618766131182 | Train Acc: 0.8585669781931464 | Train GradNorm: 7.264204507693567e-06\n",
      "Epoch [250/500]\n",
      "Test Loss: 0.35736224228851005 | Test Acc: 0.838674247318775 | Test GradNorm: 0.0007189185330486167\n",
      "Train Loss: 0.3016487003433285 | Train Acc: 0.8604361370716511 | Train GradNorm: 3.2583268125831755e-06\n",
      "Epoch [251/500]\n",
      "Test Loss: 0.3573509906701298 | Test Acc: 0.8385450316578369 | Test GradNorm: 0.0006527407748534952\n",
      "Train Loss: 0.30163695890636233 | Train Acc: 0.8598130841121495 | Train GradNorm: 1.800627599242562e-06\n",
      "Epoch [252/500]\n",
      "Test Loss: 0.35736175824214955 | Test Acc: 0.8385773355730715 | Test GradNorm: 0.0006273352605921454\n",
      "Train Loss: 0.3016276441813949 | Train Acc: 0.8598130841121495 | Train GradNorm: 3.6113375355757664e-06\n",
      "Epoch [253/500]\n",
      "Test Loss: 0.3573726517880346 | Test Acc: 0.8385773355730715 | Test GradNorm: 0.0006203024171041706\n",
      "Train Loss: 0.3016183847132068 | Train Acc: 0.8598130841121495 | Train GradNorm: 4.5145458125971e-06\n",
      "Epoch [254/500]\n",
      "Test Loss: 0.35746363177198004 | Test Acc: 0.8383189042511953 | Test GradNorm: 0.0006737531322250963\n",
      "Train Loss: 0.3016062214983296 | Train Acc: 0.8604361370716511 | Train GradNorm: 1.2302003305995172e-06\n",
      "Epoch [255/500]\n",
      "Test Loss: 0.3574897634528222 | Test Acc: 0.8383835120816643 | Test GradNorm: 0.000716398392374816\n",
      "Train Loss: 0.3015964880526784 | Train Acc: 0.8610591900311526 | Train GradNorm: 2.9432936179744706e-06\n",
      "Epoch [256/500]\n",
      "Test Loss: 0.35749347614083876 | Test Acc: 0.8384481199121333 | Test GradNorm: 0.0007141497457808248\n",
      "Train Loss: 0.30158862206432036 | Train Acc: 0.8610591900311526 | Train GradNorm: 2.892608307613867e-06\n",
      "Epoch [257/500]\n",
      "Test Loss: 0.3575863050055041 | Test Acc: 0.8384158159968988 | Test GradNorm: 0.0007962211198098908\n",
      "Train Loss: 0.3015862963446475 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.4123504360931736e-05\n",
      "Epoch [258/500]\n",
      "Test Loss: 0.357449659971959 | Test Acc: 0.8384804238273679 | Test GradNorm: 0.0005984902574006756\n",
      "Train Loss: 0.301573095684963 | Train Acc: 0.8598130841121495 | Train GradNorm: 7.4231166244891415e-06\n",
      "Epoch [259/500]\n",
      "Test Loss: 0.35752696500154413 | Test Acc: 0.8383835120816643 | Test GradNorm: 0.0006488538630439172\n",
      "Train Loss: 0.3015588629179137 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.673944606584506e-06\n",
      "Epoch [260/500]\n",
      "Test Loss: 0.3575081526548402 | Test Acc: 0.8381896885902571 | Test GradNorm: 0.0005736570731179504\n",
      "Train Loss: 0.3015576323287306 | Train Acc: 0.8598130841121495 | Train GradNorm: 1.3328991672759426e-05\n",
      "Epoch [261/500]\n",
      "Test Loss: 0.3575744797681611 | Test Acc: 0.8383835120816643 | Test GradNorm: 0.0005926335975260887\n",
      "Train Loss: 0.3015454059969369 | Train Acc: 0.859190031152648 | Train GradNorm: 9.205297087043587e-06\n",
      "Epoch [262/500]\n",
      "Test Loss: 0.35766206222973035 | Test Acc: 0.8384158159968988 | Test GradNorm: 0.000664579758826986\n",
      "Train Loss: 0.30152880375624314 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.2471237178037132e-06\n",
      "Epoch [263/500]\n",
      "Test Loss: 0.3577671266838561 | Test Acc: 0.8381896885902571 | Test GradNorm: 0.0007454012705091068\n",
      "Train Loss: 0.3015230011273639 | Train Acc: 0.8629283489096573 | Train GradNorm: 6.006200176226362e-06\n",
      "Epoch [264/500]\n",
      "Test Loss: 0.3577160532182745 | Test Acc: 0.8384481199121333 | Test GradNorm: 0.0006362970943869126\n",
      "Train Loss: 0.30151204291281397 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.763768967655385e-06\n",
      "Epoch [265/500]\n",
      "Test Loss: 0.3578313103646779 | Test Acc: 0.8380604729293191 | Test GradNorm: 0.0007332475039905828\n",
      "Train Loss: 0.30150398632579267 | Train Acc: 0.8629283489096573 | Train GradNorm: 4.466057393463575e-06\n",
      "Epoch [266/500]\n",
      "Test Loss: 0.3577786146490725 | Test Acc: 0.8383189042511953 | Test GradNorm: 0.0005968811540284285\n",
      "Train Loss: 0.3015004351593034 | Train Acc: 0.8598130841121495 | Train GradNorm: 9.28886819331072e-06\n",
      "Epoch [267/500]\n",
      "Test Loss: 0.3579881958145666 | Test Acc: 0.8382542964207262 | Test GradNorm: 0.0008466987419597546\n",
      "Train Loss: 0.30149875296024337 | Train Acc: 0.8629283489096573 | Train GradNorm: 2.434593700197404e-05\n",
      "Epoch [268/500]\n",
      "Test Loss: 0.3579101434477963 | Test Acc: 0.8384804238273679 | Test GradNorm: 0.0006077949897166289\n",
      "Train Loss: 0.301479426785501 | Train Acc: 0.8598130841121495 | Train GradNorm: 7.327856371971096e-06\n",
      "Epoch [269/500]\n",
      "Test Loss: 0.35804007901414986 | Test Acc: 0.8379958650988499 | Test GradNorm: 0.0007318231035354719\n",
      "Train Loss: 0.3014657801576031 | Train Acc: 0.8629283489096573 | Train GradNorm: 3.7322920390905345e-06\n",
      "Epoch [270/500]\n",
      "Test Loss: 0.3580616045728099 | Test Acc: 0.8380927768445535 | Test GradNorm: 0.0007086081152236744\n",
      "Train Loss: 0.30145579374973414 | Train Acc: 0.8629283489096573 | Train GradNorm: 1.953172864747709e-06\n",
      "Epoch [271/500]\n",
      "Test Loss: 0.35796329859383885 | Test Acc: 0.8385450316578369 | Test GradNorm: 0.0005283576159012476\n",
      "Train Loss: 0.3014708933283249 | Train Acc: 0.8598130841121495 | Train GradNorm: 3.44742563886898e-05\n",
      "Epoch [272/500]\n",
      "Test Loss: 0.35804519070181834 | Test Acc: 0.8387711590644786 | Test GradNorm: 0.0006208136601566364\n",
      "Train Loss: 0.30144019005411776 | Train Acc: 0.8598130841121495 | Train GradNorm: 4.703137113715295e-06\n",
      "Epoch [273/500]\n",
      "Test Loss: 0.3580927497832179 | Test Acc: 0.8382866003359607 | Test GradNorm: 0.0006207310778983509\n",
      "Train Loss: 0.3014308853587113 | Train Acc: 0.8604361370716511 | Train GradNorm: 4.681886466832752e-06\n",
      "Epoch [274/500]\n",
      "Test Loss: 0.3581506758640103 | Test Acc: 0.8382866003359607 | Test GradNorm: 0.0006775553380200818\n",
      "Train Loss: 0.3014192659578221 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.0078352338679306e-06\n",
      "Epoch [275/500]\n",
      "Test Loss: 0.3581163993165957 | Test Acc: 0.838609639488306 | Test GradNorm: 0.0005817006021540405\n",
      "Train Loss: 0.30141910856232235 | Train Acc: 0.859190031152648 | Train GradNorm: 1.2199477649376043e-05\n",
      "Epoch [276/500]\n",
      "Test Loss: 0.3583177851838539 | Test Acc: 0.8381250807597881 | Test GradNorm: 0.0007947412146929877\n",
      "Train Loss: 0.3014108336297302 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.264557047541285e-05\n",
      "Epoch [277/500]\n",
      "Test Loss: 0.35825678133347044 | Test Acc: 0.8383189042511953 | Test GradNorm: 0.0006719508287022632\n",
      "Train Loss: 0.3013946402530721 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.092074225467039e-06\n",
      "Epoch [278/500]\n",
      "Test Loss: 0.3582608620859134 | Test Acc: 0.8384804238273679 | Test GradNorm: 0.0006495979663695994\n",
      "Train Loss: 0.30138703598742045 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.8316844060852201e-06\n",
      "Epoch [279/500]\n",
      "Test Loss: 0.3582869992387297 | Test Acc: 0.8384804238273679 | Test GradNorm: 0.0006433284757538897\n",
      "Train Loss: 0.30137880734886013 | Train Acc: 0.8610591900311526 | Train GradNorm: 2.292978163097739e-06\n",
      "Epoch [280/500]\n",
      "Test Loss: 0.3582988832943501 | Test Acc: 0.8385450316578369 | Test GradNorm: 0.0006129151618244023\n",
      "Train Loss: 0.30137260891522205 | Train Acc: 0.8604361370716511 | Train GradNorm: 5.772545614359282e-06\n",
      "Epoch [281/500]\n",
      "Test Loss: 0.35850504956575896 | Test Acc: 0.8379958650988499 | Test GradNorm: 0.0008579271887568546\n",
      "Train Loss: 0.30137811953384747 | Train Acc: 0.8610591900311526 | Train GradNorm: 2.612575513077732e-05\n",
      "Epoch [282/500]\n",
      "Test Loss: 0.3585017876141932 | Test Acc: 0.8382219925054917 | Test GradNorm: 0.0007902363675481369\n",
      "Train Loss: 0.30135906913763894 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.1707154467763008e-05\n",
      "Epoch [283/500]\n",
      "Test Loss: 0.3585958672474417 | Test Acc: 0.8378666494379119 | Test GradNorm: 0.0007898159791865644\n",
      "Train Loss: 0.30134875357042895 | Train Acc: 0.8629283489096573 | Train GradNorm: 1.0815564179054666e-05\n",
      "Epoch [284/500]\n",
      "Test Loss: 0.35856291932101164 | Test Acc: 0.8379958650988499 | Test GradNorm: 0.0006973223268132362\n",
      "Train Loss: 0.30133441450499643 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.2933138939540696e-06\n",
      "Epoch [285/500]\n",
      "Test Loss: 0.3585092411888265 | Test Acc: 0.8385127277426024 | Test GradNorm: 0.0005894582031439674\n",
      "Train Loss: 0.30133285844723945 | Train Acc: 0.8598130841121495 | Train GradNorm: 1.123208900925987e-05\n",
      "Epoch [286/500]\n",
      "Test Loss: 0.35874777873310076 | Test Acc: 0.8380281690140845 | Test GradNorm: 0.00087373056822598\n",
      "Train Loss: 0.30133745648438615 | Train Acc: 0.8610591900311526 | Train GradNorm: 2.9555409577496743e-05\n",
      "Epoch [287/500]\n",
      "Test Loss: 0.3586995811027531 | Test Acc: 0.8379635611836155 | Test GradNorm: 0.0008176372937255301\n",
      "Train Loss: 0.30132088771560994 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.6943989213724596e-05\n",
      "Epoch [288/500]\n",
      "Test Loss: 0.3586063924401358 | Test Acc: 0.8382866003359607 | Test GradNorm: 0.0006635795174392665\n",
      "Train Loss: 0.3013024420578496 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.3410274828396304e-06\n",
      "Epoch [289/500]\n",
      "Test Loss: 0.3586384329373652 | Test Acc: 0.8383835120816643 | Test GradNorm: 0.0006643003279178427\n",
      "Train Loss: 0.3012937370688829 | Train Acc: 0.8604361370716511 | Train GradNorm: 1.324306509422895e-06\n",
      "Epoch [290/500]\n",
      "Test Loss: 0.3587158560280465 | Test Acc: 0.8378666494379119 | Test GradNorm: 0.0007569677940542257\n",
      "Train Loss: 0.30128977612395397 | Train Acc: 0.8616822429906542 | Train GradNorm: 6.68180329096382e-06\n",
      "Epoch [291/500]\n",
      "Test Loss: 0.35855750644858975 | Test Acc: 0.8384158159968988 | Test GradNorm: 0.0005095350075525738\n",
      "Train Loss: 0.3013120857054942 | Train Acc: 0.8598130841121495 | Train GradNorm: 4.708175554923486e-05\n",
      "Epoch [292/500]\n",
      "Test Loss: 0.358758790435863 | Test Acc: 0.8380604729293191 | Test GradNorm: 0.0007150440263591676\n",
      "Train Loss: 0.3012704131452672 | Train Acc: 0.8616822429906542 | Train GradNorm: 2.2718539228131285e-06\n",
      "Epoch [293/500]\n",
      "Test Loss: 0.3587700004430611 | Test Acc: 0.8381573846750227 | Test GradNorm: 0.0006980066339724676\n",
      "Train Loss: 0.30126131256067656 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.4685059863063203e-06\n",
      "Epoch [294/500]\n",
      "Test Loss: 0.35876416307081166 | Test Acc: 0.838609639488306 | Test GradNorm: 0.0006381477393701658\n",
      "Train Loss: 0.30125552063800204 | Train Acc: 0.8610591900311526 | Train GradNorm: 2.782366476142647e-06\n",
      "Epoch [295/500]\n",
      "Test Loss: 0.3587998963599596 | Test Acc: 0.8384481199121333 | Test GradNorm: 0.0006417135191289131\n",
      "Train Loss: 0.3012481955056631 | Train Acc: 0.8610591900311526 | Train GradNorm: 2.522378973452227e-06\n",
      "Epoch [296/500]\n",
      "Test Loss: 0.3589055387076779 | Test Acc: 0.8378666494379119 | Test GradNorm: 0.0007631876040500159\n",
      "Train Loss: 0.30124371925965787 | Train Acc: 0.8616822429906542 | Train GradNorm: 7.489432213006082e-06\n",
      "Epoch [297/500]\n",
      "Test Loss: 0.35875651318561025 | Test Acc: 0.8384158159968988 | Test GradNorm: 0.0005933580102134582\n",
      "Train Loss: 0.30123794512277513 | Train Acc: 0.8610591900311526 | Train GradNorm: 9.185726273481484e-06\n",
      "Epoch [298/500]\n",
      "Test Loss: 0.3588493678006256 | Test Acc: 0.8383189042511953 | Test GradNorm: 0.0006429871682402466\n",
      "Train Loss: 0.30122439478564655 | Train Acc: 0.8610591900311526 | Train GradNorm: 2.2553626536221605e-06\n",
      "Epoch [299/500]\n",
      "Test Loss: 0.35894219379573367 | Test Acc: 0.8380604729293191 | Test GradNorm: 0.0006803926558355106\n",
      "Train Loss: 0.3012159774981215 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.0209238989395024e-06\n",
      "Epoch [300/500]\n",
      "Test Loss: 0.35893784992732986 | Test Acc: 0.8381250807597881 | Test GradNorm: 0.000631379946930997\n",
      "Train Loss: 0.3012106354205987 | Train Acc: 0.8610591900311526 | Train GradNorm: 3.6736589658080036e-06\n",
      "Epoch [301/500]\n",
      "Test Loss: 0.35900811099609037 | Test Acc: 0.8380604729293191 | Test GradNorm: 0.0006745423386785654\n",
      "Train Loss: 0.30120079491680374 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.0512881476344341e-06\n",
      "Epoch [302/500]\n",
      "Test Loss: 0.3589452373332283 | Test Acc: 0.8383189042511953 | Test GradNorm: 0.0005537285042870332\n",
      "Train Loss: 0.30121174437301207 | Train Acc: 0.8598130841121495 | Train GradNorm: 2.440872102657477e-05\n",
      "Epoch [303/500]\n",
      "Test Loss: 0.3590663726833295 | Test Acc: 0.8379312572683809 | Test GradNorm: 0.0006735202331887892\n",
      "Train Loss: 0.30118716928249156 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.221775707287805e-06\n",
      "Epoch [304/500]\n",
      "Test Loss: 0.3591482388239573 | Test Acc: 0.8379312572683809 | Test GradNorm: 0.0007247938199634936\n",
      "Train Loss: 0.30118136183226585 | Train Acc: 0.8616822429906542 | Train GradNorm: 2.8438577025611234e-06\n",
      "Epoch [305/500]\n",
      "Test Loss: 0.3592050772879029 | Test Acc: 0.8377374337769737 | Test GradNorm: 0.0007510541244524129\n",
      "Train Loss: 0.3011759311348939 | Train Acc: 0.8623052959501558 | Train GradNorm: 5.497429080082927e-06\n",
      "Epoch [306/500]\n",
      "Test Loss: 0.35950414577886264 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.0011021518232282015\n",
      "Train Loss: 0.3012389523966946 | Train Acc: 0.8604361370716511 | Train GradNorm: 0.00010564147967260112\n",
      "Epoch [307/500]\n",
      "Test Loss: 0.3593475008650437 | Test Acc: 0.8378666494379119 | Test GradNorm: 0.000864002741490967\n",
      "Train Loss: 0.30117605349181586 | Train Acc: 0.8616822429906542 | Train GradNorm: 2.706681924176176e-05\n",
      "Epoch [308/500]\n",
      "Test Loss: 0.35921205703569165 | Test Acc: 0.8379312572683809 | Test GradNorm: 0.0006925112665729775\n",
      "Train Loss: 0.3011504014247564 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.2106833830018142e-06\n",
      "Epoch [309/500]\n",
      "Test Loss: 0.3593112583214224 | Test Acc: 0.8380281690140845 | Test GradNorm: 0.0008353200357116949\n",
      "Train Loss: 0.30115688273689883 | Train Acc: 0.8616822429906542 | Train GradNorm: 2.0805582509625172e-05\n",
      "Epoch [310/500]\n",
      "Test Loss: 0.3593418147564406 | Test Acc: 0.8379635611836155 | Test GradNorm: 0.000841194799295799\n",
      "Train Loss: 0.3011508093763673 | Train Acc: 0.8623052959501558 | Train GradNorm: 2.162532293505822e-05\n",
      "Epoch [311/500]\n",
      "Test Loss: 0.3593406987521531 | Test Acc: 0.8379635611836155 | Test GradNorm: 0.0008145208660352624\n",
      "Train Loss: 0.30114009441891987 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.625549905278059e-05\n",
      "Epoch [312/500]\n",
      "Test Loss: 0.35925942488023 | Test Acc: 0.8380604729293191 | Test GradNorm: 0.0007118104688942651\n",
      "Train Loss: 0.30112488958118544 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.600295822212118e-06\n",
      "Epoch [313/500]\n",
      "Test Loss: 0.3593833085905165 | Test Acc: 0.8379635611836155 | Test GradNorm: 0.0008272747242880059\n",
      "Train Loss: 0.30112834084411527 | Train Acc: 0.8604361370716511 | Train GradNorm: 1.9699038890777984e-05\n",
      "Epoch [314/500]\n",
      "Test Loss: 0.35945956456643985 | Test Acc: 0.8380604729293191 | Test GradNorm: 0.000914279191306672\n",
      "Train Loss: 0.3011407210914821 | Train Acc: 0.8616822429906542 | Train GradNorm: 4.259222904095479e-05\n",
      "Epoch [315/500]\n",
      "Test Loss: 0.3594965018653931 | Test Acc: 0.8380281690140845 | Test GradNorm: 0.0009213530455946239\n",
      "Train Loss: 0.3011339890729248 | Train Acc: 0.8616822429906542 | Train GradNorm: 4.428335504076339e-05\n",
      "Epoch [316/500]\n",
      "Test Loss: 0.3595086367932803 | Test Acc: 0.8379635611836155 | Test GradNorm: 0.000914822645865815\n",
      "Train Loss: 0.3011251151593297 | Train Acc: 0.8616822429906542 | Train GradNorm: 4.2105446850604436e-05\n",
      "Epoch [317/500]\n",
      "Test Loss: 0.35935373558540656 | Test Acc: 0.8382866003359607 | Test GradNorm: 0.0006506620952027988\n",
      "Train Loss: 0.3010895047813578 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.9769142733966965e-06\n",
      "Epoch [318/500]\n",
      "Test Loss: 0.35950864655262227 | Test Acc: 0.8379635611836155 | Test GradNorm: 0.0008371452237746384\n",
      "Train Loss: 0.30109833749723147 | Train Acc: 0.8598130841121495 | Train GradNorm: 2.1990950085603972e-05\n",
      "Epoch [319/500]\n",
      "Test Loss: 0.3593622244838803 | Test Acc: 0.8382542964207262 | Test GradNorm: 0.000608197618945305\n",
      "Train Loss: 0.30108101195507986 | Train Acc: 0.8610591900311526 | Train GradNorm: 6.830929110430726e-06\n",
      "Epoch [320/500]\n",
      "Test Loss: 0.35942925032681455 | Test Acc: 0.8380604729293191 | Test GradNorm: 0.0006747454011164402\n",
      "Train Loss: 0.30106742859882835 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.309660253660259e-06\n",
      "Epoch [321/500]\n",
      "Test Loss: 0.35949121830610886 | Test Acc: 0.8380604729293191 | Test GradNorm: 0.0007345796151131108\n",
      "Train Loss: 0.30106119856966534 | Train Acc: 0.8604361370716511 | Train GradNorm: 4.3580142456929606e-06\n",
      "Epoch [322/500]\n",
      "Test Loss: 0.35953575763023377 | Test Acc: 0.8378666494379119 | Test GradNorm: 0.0008003482819912827\n",
      "Train Loss: 0.30106072205244366 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.4275642362190454e-05\n",
      "Epoch [323/500]\n",
      "Test Loss: 0.3597127459936773 | Test Acc: 0.8376082181160357 | Test GradNorm: 0.0009925249099750244\n",
      "Train Loss: 0.30109061840892654 | Train Acc: 0.8610591900311526 | Train GradNorm: 6.632574268600381e-05\n",
      "Epoch [324/500]\n",
      "Test Loss: 0.3595451134411624 | Test Acc: 0.8378989533531463 | Test GradNorm: 0.0007775245798075719\n",
      "Train Loss: 0.301043956018509 | Train Acc: 0.8610591900311526 | Train GradNorm: 9.939470140653722e-06\n",
      "Epoch [325/500]\n",
      "Test Loss: 0.3594777021668234 | Test Acc: 0.8381573846750227 | Test GradNorm: 0.0006677529465852341\n",
      "Train Loss: 0.3010313101983735 | Train Acc: 0.8610591900311526 | Train GradNorm: 9.695857859553187e-07\n",
      "Epoch [326/500]\n",
      "Test Loss: 0.3594623920180924 | Test Acc: 0.8381250807597881 | Test GradNorm: 0.0006452566087329484\n",
      "Train Loss: 0.3010257604353494 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.7474873177564204e-06\n",
      "Epoch [327/500]\n",
      "Test Loss: 0.35951171815706673 | Test Acc: 0.8381250807597881 | Test GradNorm: 0.0007032886602794158\n",
      "Train Loss: 0.3010193946156738 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.067453007413785e-06\n",
      "Epoch [328/500]\n",
      "Test Loss: 0.3594779003056015 | Test Acc: 0.8385450316578369 | Test GradNorm: 0.0006132742474609409\n",
      "Train Loss: 0.3010152129273683 | Train Acc: 0.8604361370716511 | Train GradNorm: 4.803317628092273e-06\n",
      "Epoch [329/500]\n",
      "Test Loss: 0.35952154270850933 | Test Acc: 0.8384158159968988 | Test GradNorm: 0.000631711951653358\n",
      "Train Loss: 0.3010085785578464 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.7458547323852195e-06\n",
      "Epoch [330/500]\n",
      "Test Loss: 0.359498790060302 | Test Acc: 0.8383189042511953 | Test GradNorm: 0.0005372608037752402\n",
      "Train Loss: 0.30101907782867476 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.6896778413094786e-05\n",
      "Epoch [331/500]\n",
      "Test Loss: 0.3595180869971193 | Test Acc: 0.8383835120816643 | Test GradNorm: 0.0005194066682536127\n",
      "Train Loss: 0.30101934397511537 | Train Acc: 0.8598130841121495 | Train GradNorm: 3.6321022101966145e-05\n",
      "Epoch [332/500]\n",
      "Test Loss: 0.3595366026666981 | Test Acc: 0.8384481199121333 | Test GradNorm: 0.0005016279502842749\n",
      "Train Loss: 0.30102261814678943 | Train Acc: 0.8604361370716511 | Train GradNorm: 4.859209244167392e-05\n",
      "Epoch [333/500]\n",
      "Test Loss: 0.35958307307470116 | Test Acc: 0.8383835120816643 | Test GradNorm: 0.0005224683457131135\n",
      "Train Loss: 0.3010067082862159 | Train Acc: 0.8598130841121495 | Train GradNorm: 3.5053567527972376e-05\n",
      "Epoch [334/500]\n",
      "Test Loss: 0.35972078308543 | Test Acc: 0.8382219925054917 | Test GradNorm: 0.0006430538179745803\n",
      "Train Loss: 0.30097591255685 | Train Acc: 0.8610591900311526 | Train GradNorm: 2.1341024199041658e-06\n",
      "Epoch [335/500]\n",
      "Test Loss: 0.3597006601854762 | Test Acc: 0.8384804238273679 | Test GradNorm: 0.000560761181146303\n",
      "Train Loss: 0.3009812026269746 | Train Acc: 0.8604361370716511 | Train GradNorm: 1.8529141502346632e-05\n",
      "Epoch [336/500]\n",
      "Test Loss: 0.359838055945669 | Test Acc: 0.8379958650988499 | Test GradNorm: 0.0007278692387022536\n",
      "Train Loss: 0.30096388258929957 | Train Acc: 0.8610591900311526 | Train GradNorm: 3.5207723368735466e-06\n",
      "Epoch [337/500]\n",
      "Test Loss: 0.3598732638177414 | Test Acc: 0.8379312572683809 | Test GradNorm: 0.000730121407074429\n",
      "Train Loss: 0.3009570424328958 | Train Acc: 0.8616822429906542 | Train GradNorm: 3.664962156711193e-06\n",
      "Epoch [338/500]\n",
      "Test Loss: 0.3600115076875466 | Test Acc: 0.8379635611836155 | Test GradNorm: 0.000867102931622975\n",
      "Train Loss: 0.30096779945295 | Train Acc: 0.8616822429906542 | Train GradNorm: 2.7850546886879576e-05\n",
      "Epoch [339/500]\n",
      "Test Loss: 0.36006302353304953 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.0009316915761748601\n",
      "Train Loss: 0.3009739940532711 | Train Acc: 0.8616822429906542 | Train GradNorm: 4.548730279973806e-05\n",
      "Epoch [340/500]\n",
      "Test Loss: 0.3600121038555452 | Test Acc: 0.8378989533531463 | Test GradNorm: 0.0008318901887387377\n",
      "Train Loss: 0.3009494284805581 | Train Acc: 0.8629283489096573 | Train GradNorm: 1.9638358155788566e-05\n",
      "Epoch [341/500]\n",
      "Test Loss: 0.359959063375022 | Test Acc: 0.8380604729293191 | Test GradNorm: 0.0007066232177037127\n",
      "Train Loss: 0.3009300805061308 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.7077148566041203e-06\n",
      "Epoch [342/500]\n",
      "Test Loss: 0.3600959965218522 | Test Acc: 0.8378989533531463 | Test GradNorm: 0.0008945660720676343\n",
      "Train Loss: 0.30094956940029066 | Train Acc: 0.8616822429906542 | Train GradNorm: 3.561241830388599e-05\n",
      "Epoch [343/500]\n",
      "Test Loss: 0.36000903373516996 | Test Acc: 0.8380604729293191 | Test GradNorm: 0.0007591353264821511\n",
      "Train Loss: 0.3009234942343762 | Train Acc: 0.8616822429906542 | Train GradNorm: 7.058719989276191e-06\n",
      "Epoch [344/500]\n",
      "Test Loss: 0.3600644114315196 | Test Acc: 0.8378989533531463 | Test GradNorm: 0.0008071443388073557\n",
      "Train Loss: 0.3009235167348595 | Train Acc: 0.8629283489096573 | Train GradNorm: 1.5355830945696095e-05\n",
      "Epoch [345/500]\n",
      "Test Loss: 0.36004877431872184 | Test Acc: 0.8380281690140845 | Test GradNorm: 0.0007403983495226517\n",
      "Train Loss: 0.300909579495474 | Train Acc: 0.8616822429906542 | Train GradNorm: 4.978018402161932e-06\n",
      "Epoch [346/500]\n",
      "Test Loss: 0.35999814928466806 | Test Acc: 0.8380927768445535 | Test GradNorm: 0.000621266342585239\n",
      "Train Loss: 0.30090356800716794 | Train Acc: 0.8604361370716511 | Train GradNorm: 4.127791777692872e-06\n",
      "Epoch [347/500]\n",
      "Test Loss: 0.36001409941403806 | Test Acc: 0.8380604729293191 | Test GradNorm: 0.0006148502471977873\n",
      "Train Loss: 0.3008975887412842 | Train Acc: 0.8604361370716511 | Train GradNorm: 4.8639421661965514e-06\n",
      "Epoch [348/500]\n",
      "Test Loss: 0.3600933391212497 | Test Acc: 0.8378989533531463 | Test GradNorm: 0.0006831691605175172\n",
      "Train Loss: 0.3008884561173493 | Train Acc: 0.8616822429906542 | Train GradNorm: 9.823280316475963e-07\n",
      "Epoch [349/500]\n",
      "Test Loss: 0.360057816072304 | Test Acc: 0.8380281690140845 | Test GradNorm: 0.000611172100470674\n",
      "Train Loss: 0.3008847179931053 | Train Acc: 0.8604361370716511 | Train GradNorm: 5.536182729653995e-06\n",
      "Epoch [350/500]\n",
      "Test Loss: 0.36016186847299836 | Test Acc: 0.8377697376922083 | Test GradNorm: 0.0007293133909685214\n",
      "Train Loss: 0.30087922164675457 | Train Acc: 0.8616822429906542 | Train GradNorm: 3.69591510617085e-06\n",
      "Epoch [351/500]\n",
      "Test Loss: 0.36025930353718894 | Test Acc: 0.8377374337769737 | Test GradNorm: 0.0008267053556088576\n",
      "Train Loss: 0.3008846195667098 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.912210783907244e-05\n",
      "Epoch [352/500]\n",
      "Test Loss: 0.36024248291915195 | Test Acc: 0.8377697376922083 | Test GradNorm: 0.0007860998007556251\n",
      "Train Loss: 0.3008718761232137 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.1454208692429973e-05\n",
      "Epoch [353/500]\n",
      "Test Loss: 0.360283108828535 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.0008044894873013784\n",
      "Train Loss: 0.30086824910304294 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.5068732504705711e-05\n",
      "Epoch [354/500]\n",
      "Test Loss: 0.36038139042023887 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.0008688025942059561\n",
      "Train Loss: 0.3008728851166383 | Train Acc: 0.8623052959501558 | Train GradNorm: 2.8905744065124393e-05\n",
      "Epoch [355/500]\n",
      "Test Loss: 0.36045310005654424 | Test Acc: 0.8376082181160357 | Test GradNorm: 0.0008995091723659478\n",
      "Train Loss: 0.30087245197492957 | Train Acc: 0.8623052959501558 | Train GradNorm: 3.7124882705710316e-05\n",
      "Epoch [356/500]\n",
      "Test Loss: 0.36035315339952473 | Test Acc: 0.8376082181160357 | Test GradNorm: 0.0007058363817343176\n",
      "Train Loss: 0.3008411540706951 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.6749740569505844e-06\n",
      "Epoch [357/500]\n",
      "Test Loss: 0.3604710032386274 | Test Acc: 0.837446698539863 | Test GradNorm: 0.0008035735504650404\n",
      "Train Loss: 0.30084507221998064 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.4033066399226584e-05\n",
      "Epoch [358/500]\n",
      "Test Loss: 0.36059772298464937 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.0008995900972829315\n",
      "Train Loss: 0.30085585411542093 | Train Acc: 0.8623052959501558 | Train GradNorm: 3.685483030282215e-05\n",
      "Epoch [359/500]\n",
      "Test Loss: 0.36068018753690284 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.0009906233298827942\n",
      "Train Loss: 0.3008706020149038 | Train Acc: 0.8623052959501558 | Train GradNorm: 6.527884444462618e-05\n",
      "Epoch [360/500]\n",
      "Test Loss: 0.36066645668370473 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.0009302772731711686\n",
      "Train Loss: 0.30085073170431237 | Train Acc: 0.8623052959501558 | Train GradNorm: 4.594214587860836e-05\n",
      "Epoch [361/500]\n",
      "Test Loss: 0.3605103401051977 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.0006539585026302797\n",
      "Train Loss: 0.30081379002885733 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.7947984719695821e-06\n",
      "Epoch [362/500]\n",
      "Test Loss: 0.3605046066926634 | Test Acc: 0.8377697376922083 | Test GradNorm: 0.0006598316353076765\n",
      "Train Loss: 0.30080738840005383 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.4530244451360978e-06\n",
      "Epoch [363/500]\n",
      "Test Loss: 0.3604693640074855 | Test Acc: 0.8379312572683809 | Test GradNorm: 0.0005861742283732273\n",
      "Train Loss: 0.30080959708209554 | Train Acc: 0.859190031152648 | Train GradNorm: 1.2276924581978076e-05\n",
      "Epoch [364/500]\n",
      "Test Loss: 0.36064481564604806 | Test Acc: 0.8375436102855666 | Test GradNorm: 0.0007946470377262888\n",
      "Train Loss: 0.30080365860820285 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.2469867408864e-05\n",
      "Epoch [365/500]\n",
      "Test Loss: 0.36053944644430913 | Test Acc: 0.8377374337769737 | Test GradNorm: 0.0006440580678634776\n",
      "Train Loss: 0.30079083371002674 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.1328387982709083e-06\n",
      "Epoch [366/500]\n",
      "Test Loss: 0.3604478676986537 | Test Acc: 0.8383512081664297 | Test GradNorm: 0.0005168809600960082\n",
      "Train Loss: 0.3008130909935365 | Train Acc: 0.859190031152648 | Train GradNorm: 4.1510377152544866e-05\n",
      "Epoch [367/500]\n",
      "Test Loss: 0.3606217300667513 | Test Acc: 0.8376082181160357 | Test GradNorm: 0.0006945477268006991\n",
      "Train Loss: 0.30077813803460435 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.2824260043440981e-06\n",
      "Epoch [368/500]\n",
      "Test Loss: 0.36057329984464476 | Test Acc: 0.8377374337769737 | Test GradNorm: 0.0006018689455075631\n",
      "Train Loss: 0.30077802310032214 | Train Acc: 0.859190031152648 | Train GradNorm: 7.883301923564384e-06\n",
      "Epoch [369/500]\n",
      "Test Loss: 0.3606767820804867 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.0007370476760434027\n",
      "Train Loss: 0.30076993019021675 | Train Acc: 0.8610591900311526 | Train GradNorm: 4.202892935072444e-06\n",
      "Epoch [370/500]\n",
      "Test Loss: 0.3606391029540614 | Test Acc: 0.8378989533531463 | Test GradNorm: 0.0006679116461558914\n",
      "Train Loss: 0.30076216009878487 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.0556243896092667e-06\n",
      "Epoch [371/500]\n",
      "Test Loss: 0.3606922660774429 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.0007007955003653168\n",
      "Train Loss: 0.30075707390539047 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.5661400844055639e-06\n",
      "Epoch [372/500]\n",
      "Test Loss: 0.3605528748987866 | Test Acc: 0.8380927768445535 | Test GradNorm: 0.0005171605963596498\n",
      "Train Loss: 0.3007791339636508 | Train Acc: 0.8585669781931464 | Train GradNorm: 3.9472249758541015e-05\n",
      "Epoch [373/500]\n",
      "Test Loss: 0.3607225020606731 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.0007246841533171527\n",
      "Train Loss: 0.3007469233792273 | Train Acc: 0.8616822429906542 | Train GradNorm: 3.16141882732342e-06\n",
      "Epoch [374/500]\n",
      "Test Loss: 0.360661158778215 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.0006160408733011984\n",
      "Train Loss: 0.3007422635822047 | Train Acc: 0.8610591900311526 | Train GradNorm: 4.569001982285192e-06\n",
      "Epoch [375/500]\n",
      "Test Loss: 0.3606530004019872 | Test Acc: 0.8381250807597881 | Test GradNorm: 0.0005682573170219969\n",
      "Train Loss: 0.3007449249310067 | Train Acc: 0.8604361370716511 | Train GradNorm: 1.5288809016958938e-05\n",
      "Epoch [376/500]\n",
      "Test Loss: 0.3608024527614711 | Test Acc: 0.8377697376922083 | Test GradNorm: 0.0007553000999801556\n",
      "Train Loss: 0.30073299179679575 | Train Acc: 0.8610591900311526 | Train GradNorm: 7.03267007609872e-06\n",
      "Epoch [377/500]\n",
      "Test Loss: 0.3607559657770423 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.0006800125534545565\n",
      "Train Loss: 0.3007230024940453 | Train Acc: 0.8610591900311526 | Train GradNorm: 9.740144746169526e-07\n",
      "Epoch [378/500]\n",
      "Test Loss: 0.360749154098505 | Test Acc: 0.8379312572683809 | Test GradNorm: 0.0006695007695430568\n",
      "Train Loss: 0.3007187987519723 | Train Acc: 0.8598130841121495 | Train GradNorm: 9.394073514329921e-07\n",
      "Epoch [379/500]\n",
      "Test Loss: 0.36067025466803615 | Test Acc: 0.8380927768445535 | Test GradNorm: 0.0005469428970163252\n",
      "Train Loss: 0.30072843009121475 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.1183760345108164e-05\n",
      "Epoch [380/500]\n",
      "Test Loss: 0.3607124042345919 | Test Acc: 0.8381896885902571 | Test GradNorm: 0.0005626850271459684\n",
      "Train Loss: 0.3007197473988537 | Train Acc: 0.8604361370716511 | Train GradNorm: 1.5676728827193645e-05\n",
      "Epoch [381/500]\n",
      "Test Loss: 0.36080623281410873 | Test Acc: 0.8378989533531463 | Test GradNorm: 0.0006513064368123194\n",
      "Train Loss: 0.3007033301954243 | Train Acc: 0.8604361370716511 | Train GradNorm: 1.1800734638311656e-06\n",
      "Epoch [382/500]\n",
      "Test Loss: 0.3609049828538854 | Test Acc: 0.8377697376922083 | Test GradNorm: 0.0007744560488166776\n",
      "Train Loss: 0.3007043257999399 | Train Acc: 0.8629283489096573 | Train GradNorm: 1.0261402755513372e-05\n",
      "Epoch [383/500]\n",
      "Test Loss: 0.3608787335780636 | Test Acc: 0.8378666494379119 | Test GradNorm: 0.0007438308158372153\n",
      "Train Loss: 0.3006962582571791 | Train Acc: 0.8623052959501558 | Train GradNorm: 5.856108152337475e-06\n",
      "Epoch [384/500]\n",
      "Test Loss: 0.3608879140785446 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.0007103760817705794\n",
      "Train Loss: 0.3006884396370603 | Train Acc: 0.8610591900311526 | Train GradNorm: 2.3617915890088077e-06\n",
      "Epoch [385/500]\n",
      "Test Loss: 0.3608506713118558 | Test Acc: 0.8378989533531463 | Test GradNorm: 0.00064014728123001\n",
      "Train Loss: 0.3006824713807748 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.7570618224660111e-06\n",
      "Epoch [386/500]\n",
      "Test Loss: 0.36111259153862557 | Test Acc: 0.8377374337769737 | Test GradNorm: 0.000899398905803056\n",
      "Train Loss: 0.30070272063353015 | Train Acc: 0.8623052959501558 | Train GradNorm: 3.821874203813683e-05\n",
      "Epoch [387/500]\n",
      "Test Loss: 0.361238299855388 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.0010415750403564247\n",
      "Train Loss: 0.3007318737561322 | Train Acc: 0.8610591900311526 | Train GradNorm: 8.571333390948128e-05\n",
      "Epoch [388/500]\n",
      "Test Loss: 0.3611177642578082 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.0008602589442170625\n",
      "Train Loss: 0.3006855448615431 | Train Acc: 0.8623052959501558 | Train GradNorm: 2.8034697091528496e-05\n",
      "Epoch [389/500]\n",
      "Test Loss: 0.36110677046490536 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.0007991494436792563\n",
      "Train Loss: 0.30067057483540166 | Train Acc: 0.8629283489096573 | Train GradNorm: 1.4252024661374097e-05\n",
      "Epoch [390/500]\n",
      "Test Loss: 0.36114557680696485 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.0008086720373805835\n",
      "Train Loss: 0.30066753485652004 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.6017696780519068e-05\n",
      "Epoch [391/500]\n",
      "Test Loss: 0.3613080335138914 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.0009772598782103435\n",
      "Train Loss: 0.30069596232448176 | Train Acc: 0.8610591900311526 | Train GradNorm: 6.212547771963977e-05\n",
      "Epoch [392/500]\n",
      "Test Loss: 0.36110080814214324 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.000696466147303543\n",
      "Train Loss: 0.3006470900961646 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.2885478211471707e-06\n",
      "Epoch [393/500]\n",
      "Test Loss: 0.3612091423046654 | Test Acc: 0.8378343455226773 | Test GradNorm: 0.0007756582915771868\n",
      "Train Loss: 0.3006483553719085 | Train Acc: 0.8623052959501558 | Train GradNorm: 9.582045241050348e-06\n",
      "Epoch [394/500]\n",
      "Test Loss: 0.36108400606767227 | Test Acc: 0.8378989533531463 | Test GradNorm: 0.0005834952639040945\n",
      "Train Loss: 0.3006447544689731 | Train Acc: 0.8604361370716511 | Train GradNorm: 1.1212190135821442e-05\n",
      "Epoch [395/500]\n",
      "Test Loss: 0.36112717079051865 | Test Acc: 0.8379958650988499 | Test GradNorm: 0.0005947186043175481\n",
      "Train Loss: 0.30063816468195187 | Train Acc: 0.8604361370716511 | Train GradNorm: 8.775696572776725e-06\n",
      "Epoch [396/500]\n",
      "Test Loss: 0.3613327496436133 | Test Acc: 0.8377697376922083 | Test GradNorm: 0.0007913894883450514\n",
      "Train Loss: 0.3006356723590984 | Train Acc: 0.8629283489096573 | Train GradNorm: 1.1655750486780942e-05\n",
      "Epoch [397/500]\n",
      "Test Loss: 0.36124558582212896 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.000645284974511419\n",
      "Train Loss: 0.30062416904229333 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.8650193504755167e-06\n",
      "Epoch [398/500]\n",
      "Test Loss: 0.3612230449259709 | Test Acc: 0.8378343455226773 | Test GradNorm: 0.0005792596636967901\n",
      "Train Loss: 0.3006276026355332 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.308067638594355e-05\n",
      "Epoch [399/500]\n",
      "Test Loss: 0.36137008538134535 | Test Acc: 0.8378343455226773 | Test GradNorm: 0.0007695922566843454\n",
      "Train Loss: 0.3006200912548051 | Train Acc: 0.8623052959501558 | Train GradNorm: 8.674447191763527e-06\n",
      "Epoch [400/500]\n",
      "Test Loss: 0.3613409753956657 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.0006903158277163848\n",
      "Train Loss: 0.30060946424346585 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.0329534288665345e-06\n",
      "Epoch [401/500]\n",
      "Test Loss: 0.36150377099119047 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.0008409045148550988\n",
      "Train Loss: 0.3006197688668021 | Train Acc: 0.8623052959501558 | Train GradNorm: 2.1906269950601345e-05\n",
      "Epoch [402/500]\n",
      "Test Loss: 0.36146438197539743 | Test Acc: 0.8378989533531463 | Test GradNorm: 0.000790386167326342\n",
      "Train Loss: 0.3006076380624537 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.1622106147301935e-05\n",
      "Epoch [403/500]\n",
      "Test Loss: 0.36138074994815667 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.0006607104060028222\n",
      "Train Loss: 0.30059480647504 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.0242625611739718e-06\n",
      "Epoch [404/500]\n",
      "Test Loss: 0.3613142873156001 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.000565360379781053\n",
      "Train Loss: 0.3006012149495827 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.651371124615117e-05\n",
      "Epoch [405/500]\n",
      "Test Loss: 0.36142274588615986 | Test Acc: 0.8377697376922083 | Test GradNorm: 0.0006832125739997912\n",
      "Train Loss: 0.3005855319601335 | Train Acc: 0.8623052959501558 | Train GradNorm: 9.065766218769851e-07\n",
      "Epoch [406/500]\n",
      "Test Loss: 0.36149872255221793 | Test Acc: 0.8378989533531463 | Test GradNorm: 0.0007839645494965906\n",
      "Train Loss: 0.30058813858710987 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.095268766562622e-05\n",
      "Epoch [407/500]\n",
      "Test Loss: 0.36159784868086164 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.0008584838485587811\n",
      "Train Loss: 0.3005946837895795 | Train Acc: 0.8623052959501558 | Train GradNorm: 2.6734691262414407e-05\n",
      "Epoch [408/500]\n",
      "Test Loss: 0.3615292326427536 | Test Acc: 0.8378666494379119 | Test GradNorm: 0.0007648449591568116\n",
      "Train Loss: 0.30057598747894965 | Train Acc: 0.8635514018691589 | Train GradNorm: 8.059248558583021e-06\n",
      "Epoch [409/500]\n",
      "Test Loss: 0.3615479535358354 | Test Acc: 0.8378666494379119 | Test GradNorm: 0.0007341952183911743\n",
      "Train Loss: 0.300567827063489 | Train Acc: 0.8623052959501558 | Train GradNorm: 3.9458521726690895e-06\n",
      "Epoch [410/500]\n",
      "Test Loss: 0.36148451405808396 | Test Acc: 0.8376082181160357 | Test GradNorm: 0.0006366953312421789\n",
      "Train Loss: 0.3005617536182529 | Train Acc: 0.8616822429906542 | Train GradNorm: 2.2145325130720997e-06\n",
      "Epoch [411/500]\n",
      "Test Loss: 0.3614560879717042 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.0005822868806719996\n",
      "Train Loss: 0.3005639590020876 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.1056205622428275e-05\n",
      "Epoch [412/500]\n",
      "Test Loss: 0.361434327222868 | Test Acc: 0.8378989533531463 | Test GradNorm: 0.0005516034348540986\n",
      "Train Loss: 0.30056629872103663 | Train Acc: 0.8610591900311526 | Train GradNorm: 2.0023088925760084e-05\n",
      "Epoch [413/500]\n",
      "Test Loss: 0.361616934363045 | Test Acc: 0.8378989533531463 | Test GradNorm: 0.0007712175701514528\n",
      "Train Loss: 0.30055314197808913 | Train Acc: 0.8623052959501558 | Train GradNorm: 9.333618642134094e-06\n",
      "Epoch [414/500]\n",
      "Test Loss: 0.3615058496162668 | Test Acc: 0.8378666494379119 | Test GradNorm: 0.0006148590096701691\n",
      "Train Loss: 0.3005463007911978 | Train Acc: 0.8610591900311526 | Train GradNorm: 4.5282384510415766e-06\n",
      "Epoch [415/500]\n",
      "Test Loss: 0.3615581341382218 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.0006161317105590703\n",
      "Train Loss: 0.3005404027781626 | Train Acc: 0.8623052959501558 | Train GradNorm: 4.389192770338826e-06\n",
      "Epoch [416/500]\n",
      "Test Loss: 0.3617708087961861 | Test Acc: 0.8377374337769737 | Test GradNorm: 0.0008708601709601087\n",
      "Train Loss: 0.3005532218973403 | Train Acc: 0.8623052959501558 | Train GradNorm: 2.9720767021419818e-05\n",
      "Epoch [417/500]\n",
      "Test Loss: 0.36166107433783273 | Test Acc: 0.8378343455226773 | Test GradNorm: 0.000709244454311914\n",
      "Train Loss: 0.3005287778674418 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.9630736147724357e-06\n",
      "Epoch [418/500]\n",
      "Test Loss: 0.3616535125700241 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.0006685534574782021\n",
      "Train Loss: 0.3005231680512851 | Train Acc: 0.8623052959501558 | Train GradNorm: 6.967646546556178e-07\n",
      "Epoch [419/500]\n",
      "Test Loss: 0.3615926935700308 | Test Acc: 0.8378343455226773 | Test GradNorm: 0.000571117367768674\n",
      "Train Loss: 0.3005287359030645 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.3789490913170283e-05\n",
      "Epoch [420/500]\n",
      "Test Loss: 0.3616525352808078 | Test Acc: 0.8375113063703321 | Test GradNorm: 0.0006387215368906397\n",
      "Train Loss: 0.30051543569581934 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.8437693043899588e-06\n",
      "Epoch [421/500]\n",
      "Test Loss: 0.3616706685835001 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.0006412573021337429\n",
      "Train Loss: 0.3005111534676768 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.6807527946635088e-06\n",
      "Epoch [422/500]\n",
      "Test Loss: 0.3617570323064658 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.0007259217120804207\n",
      "Train Loss: 0.30050895726933313 | Train Acc: 0.8616822429906542 | Train GradNorm: 3.8240227555498e-06\n",
      "Epoch [423/500]\n",
      "Test Loss: 0.36178198185553945 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.0007457774253880519\n",
      "Train Loss: 0.30050552640653677 | Train Acc: 0.8616822429906542 | Train GradNorm: 6.060115990983262e-06\n",
      "Epoch [424/500]\n",
      "Test Loss: 0.3618488384374471 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.0008255757651497222\n",
      "Train Loss: 0.30051320383731767 | Train Acc: 0.8635514018691589 | Train GradNorm: 2.070905396400871e-05\n",
      "Epoch [425/500]\n",
      "Test Loss: 0.3618207444425669 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.0007537572731797432\n",
      "Train Loss: 0.30049906162864537 | Train Acc: 0.8616822429906542 | Train GradNorm: 7.3734578405664086e-06\n",
      "Epoch [426/500]\n",
      "Test Loss: 0.3618595909758775 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.000739217186241803\n",
      "Train Loss: 0.30049186054748217 | Train Acc: 0.8616822429906542 | Train GradNorm: 5.133725619025881e-06\n",
      "Epoch [427/500]\n",
      "Test Loss: 0.3618700589950667 | Test Acc: 0.8377374337769737 | Test GradNorm: 0.0006909996810145973\n",
      "Train Loss: 0.3004843409385962 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.1531339878289367e-06\n",
      "Epoch [428/500]\n",
      "Test Loss: 0.3619137808704324 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.0007442577324711785\n",
      "Train Loss: 0.3004832171621557 | Train Acc: 0.8616822429906542 | Train GradNorm: 5.3853036474424054e-06\n",
      "Epoch [429/500]\n",
      "Test Loss: 0.3620176666105172 | Test Acc: 0.8377374337769737 | Test GradNorm: 0.0008807550062009353\n",
      "Train Loss: 0.3004990599963019 | Train Acc: 0.8629283489096573 | Train GradNorm: 3.264451208582508e-05\n",
      "Epoch [430/500]\n",
      "Test Loss: 0.3617898299734208 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.0005711081778421885\n",
      "Train Loss: 0.30048214994820904 | Train Acc: 0.8610591900311526 | Train GradNorm: 1.3842710062401337e-05\n",
      "Epoch [431/500]\n",
      "Test Loss: 0.361864440317373 | Test Acc: 0.8377374337769737 | Test GradNorm: 0.0006431643913906532\n",
      "Train Loss: 0.3004679944784229 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.6786916756178261e-06\n",
      "Epoch [432/500]\n",
      "Test Loss: 0.36185828725383107 | Test Acc: 0.8378666494379119 | Test GradNorm: 0.0006074045268954271\n",
      "Train Loss: 0.30046702323395713 | Train Acc: 0.8610591900311526 | Train GradNorm: 5.692135291107752e-06\n",
      "Epoch [433/500]\n",
      "Test Loss: 0.3619660598406315 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.0006904571413995763\n",
      "Train Loss: 0.30045890392521934 | Train Acc: 0.8610591900311526 | Train GradNorm: 9.306644708367895e-07\n",
      "Epoch [434/500]\n",
      "Test Loss: 0.3619523814843457 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.0006321959608379052\n",
      "Train Loss: 0.3004561069327948 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.716659181308671e-06\n",
      "Epoch [435/500]\n",
      "Test Loss: 0.36206284613490447 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.0007511675111714657\n",
      "Train Loss: 0.300454149908431 | Train Acc: 0.8629283489096573 | Train GradNorm: 5.581288101369245e-06\n",
      "Epoch [436/500]\n",
      "Test Loss: 0.36215264499573147 | Test Acc: 0.8375113063703321 | Test GradNorm: 0.0008322556312973823\n",
      "Train Loss: 0.30046018308242534 | Train Acc: 0.8635514018691589 | Train GradNorm: 1.9803393994981056e-05\n",
      "Epoch [437/500]\n",
      "Test Loss: 0.3619670044342754 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.000554991910888908\n",
      "Train Loss: 0.3004575550139168 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.0936594132256272e-05\n",
      "Epoch [438/500]\n",
      "Test Loss: 0.36205091160155806 | Test Acc: 0.8377374337769737 | Test GradNorm: 0.0006366810260593701\n",
      "Train Loss: 0.300439831784874 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.4747715022303343e-06\n",
      "Epoch [439/500]\n",
      "Test Loss: 0.36212470934567 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.0006991428270590346\n",
      "Train Loss: 0.30043490408888956 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.119980558403786e-06\n",
      "Epoch [440/500]\n",
      "Test Loss: 0.36217529516337277 | Test Acc: 0.8377697376922083 | Test GradNorm: 0.000752316528590856\n",
      "Train Loss: 0.3004342559468936 | Train Acc: 0.8623052959501558 | Train GradNorm: 5.57357387303537e-06\n",
      "Epoch [441/500]\n",
      "Test Loss: 0.3620855189409311 | Test Acc: 0.8377697376922083 | Test GradNorm: 0.000629369878571683\n",
      "Train Loss: 0.3004288675841719 | Train Acc: 0.8604361370716511 | Train GradNorm: 3.284091387368145e-06\n",
      "Epoch [442/500]\n",
      "Test Loss: 0.3621385406740659 | Test Acc: 0.8377697376922083 | Test GradNorm: 0.0006627257140590199\n",
      "Train Loss: 0.30042224955280983 | Train Acc: 0.8604361370716511 | Train GradNorm: 9.607917324987028e-07\n",
      "Epoch [443/500]\n",
      "Test Loss: 0.3621580134598844 | Test Acc: 0.8376082181160357 | Test GradNorm: 0.0006741109950465663\n",
      "Train Loss: 0.30041907766242026 | Train Acc: 0.8604361370716511 | Train GradNorm: 8.931249166228578e-07\n",
      "Epoch [444/500]\n",
      "Test Loss: 0.36229404778765606 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.0008300861159227605\n",
      "Train Loss: 0.30043089506766724 | Train Acc: 0.8629283489096573 | Train GradNorm: 1.944486721975925e-05\n",
      "Epoch [445/500]\n",
      "Test Loss: 0.3624023659478674 | Test Acc: 0.8375759142008011 | Test GradNorm: 0.0009235970503355402\n",
      "Train Loss: 0.3004431153231207 | Train Acc: 0.8635514018691589 | Train GradNorm: 4.3223006426692385e-05\n",
      "Epoch [446/500]\n",
      "Test Loss: 0.3623060278366606 | Test Acc: 0.8378343455226773 | Test GradNorm: 0.0007451803361306602\n",
      "Train Loss: 0.30040879612769417 | Train Acc: 0.8623052959501558 | Train GradNorm: 4.795833545296175e-06\n",
      "Epoch [447/500]\n",
      "Test Loss: 0.3623506919986312 | Test Acc: 0.8378343455226773 | Test GradNorm: 0.0007785226211037743\n",
      "Train Loss: 0.30040767864154605 | Train Acc: 0.8629283489096573 | Train GradNorm: 9.050032218370748e-06\n",
      "Epoch [448/500]\n",
      "Test Loss: 0.3624082434892245 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.000817652606358122\n",
      "Train Loss: 0.3004079235539491 | Train Acc: 0.8629283489096573 | Train GradNorm: 1.583605979841953e-05\n",
      "Epoch [449/500]\n",
      "Test Loss: 0.36239775320087947 | Test Acc: 0.8378343455226773 | Test GradNorm: 0.0007693696950044293\n",
      "Train Loss: 0.3003978840931764 | Train Acc: 0.8629283489096573 | Train GradNorm: 7.432866085086631e-06\n",
      "Epoch [450/500]\n",
      "Test Loss: 0.3623643373969943 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.0006649558453443105\n",
      "Train Loss: 0.30038958857539516 | Train Acc: 0.8616822429906542 | Train GradNorm: 9.747549616445147e-07\n",
      "Epoch [451/500]\n",
      "Test Loss: 0.3624627890740382 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.0007231756312392889\n",
      "Train Loss: 0.3003869905980121 | Train Acc: 0.8629283489096573 | Train GradNorm: 2.2208338347724416e-06\n",
      "Epoch [452/500]\n",
      "Test Loss: 0.36246557189974127 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.0006851395623263823\n",
      "Train Loss: 0.30038244436272754 | Train Acc: 0.8623052959501558 | Train GradNorm: 7.163069747459521e-07\n",
      "Epoch [453/500]\n",
      "Test Loss: 0.3625040023030875 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.0007071835607217645\n",
      "Train Loss: 0.30037933465248495 | Train Acc: 0.8635514018691589 | Train GradNorm: 1.2349317385189156e-06\n",
      "Epoch [454/500]\n",
      "Test Loss: 0.36256037126159635 | Test Acc: 0.8377374337769737 | Test GradNorm: 0.0007651993364496525\n",
      "Train Loss: 0.30037850611825423 | Train Acc: 0.8635514018691589 | Train GradNorm: 6.491195938823511e-06\n",
      "Epoch [455/500]\n",
      "Test Loss: 0.36253645070323604 | Test Acc: 0.8377374337769737 | Test GradNorm: 0.0007084413187394714\n",
      "Train Loss: 0.3003708955454292 | Train Acc: 0.8635514018691589 | Train GradNorm: 1.302913998359617e-06\n",
      "Epoch [456/500]\n",
      "Test Loss: 0.3624320313719407 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.000549517684620636\n",
      "Train Loss: 0.3003854172004031 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.609796008464057e-05\n",
      "Epoch [457/500]\n",
      "Test Loss: 0.3625590998326035 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.0006682704180054873\n",
      "Train Loss: 0.300363706026114 | Train Acc: 0.8623052959501558 | Train GradNorm: 9.827644153837884e-07\n",
      "Epoch [458/500]\n",
      "Test Loss: 0.36253075752415886 | Test Acc: 0.8375113063703321 | Test GradNorm: 0.0006081572623512434\n",
      "Train Loss: 0.3003643261928507 | Train Acc: 0.8629283489096573 | Train GradNorm: 7.618169507956593e-06\n",
      "Epoch [459/500]\n",
      "Test Loss: 0.3625677015399277 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.000643180433171607\n",
      "Train Loss: 0.3003566021299876 | Train Acc: 0.8635514018691589 | Train GradNorm: 2.5621262934327288e-06\n",
      "Epoch [460/500]\n",
      "Test Loss: 0.3626460759570316 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.0007270491499099051\n",
      "Train Loss: 0.3003526047496782 | Train Acc: 0.8629283489096573 | Train GradNorm: 2.4329574417206043e-06\n",
      "Epoch [461/500]\n",
      "Test Loss: 0.3626022714781198 | Test Acc: 0.8377374337769737 | Test GradNorm: 0.000672768310137815\n",
      "Train Loss: 0.30034752630283434 | Train Acc: 0.8629283489096573 | Train GradNorm: 8.000324167744644e-07\n",
      "Epoch [462/500]\n",
      "Test Loss: 0.3625131423359482 | Test Acc: 0.8375759142008011 | Test GradNorm: 0.0005591678457735632\n",
      "Train Loss: 0.30035757250865663 | Train Acc: 0.8604361370716511 | Train GradNorm: 2.0611312479594954e-05\n",
      "Epoch [463/500]\n",
      "Test Loss: 0.36249817124487105 | Test Acc: 0.8379635611836155 | Test GradNorm: 0.0005057546829088909\n",
      "Train Loss: 0.30037630172353785 | Train Acc: 0.8598130841121495 | Train GradNorm: 5.1152975643628164e-05\n",
      "Epoch [464/500]\n",
      "Test Loss: 0.3626466099491419 | Test Acc: 0.8378020416074429 | Test GradNorm: 0.0006697336668031508\n",
      "Train Loss: 0.30033520962780913 | Train Acc: 0.8629283489096573 | Train GradNorm: 8.211748631367433e-07\n",
      "Epoch [465/500]\n",
      "Test Loss: 0.3627554227506095 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.0007493279931941281\n",
      "Train Loss: 0.300334379641689 | Train Acc: 0.8635514018691589 | Train GradNorm: 4.63695793217599e-06\n",
      "Epoch [466/500]\n",
      "Test Loss: 0.36276195184825305 | Test Acc: 0.8377374337769737 | Test GradNorm: 0.0007139827128857712\n",
      "Train Loss: 0.3003285896085902 | Train Acc: 0.8635514018691589 | Train GradNorm: 1.589704531939897e-06\n",
      "Epoch [467/500]\n",
      "Test Loss: 0.36269024638047714 | Test Acc: 0.8375436102855666 | Test GradNorm: 0.0006137739700076946\n",
      "Train Loss: 0.30032905761127543 | Train Acc: 0.8635514018691589 | Train GradNorm: 6.350573921169476e-06\n",
      "Epoch [468/500]\n",
      "Test Loss: 0.36284950381418113 | Test Acc: 0.8374143946246285 | Test GradNorm: 0.0007994078836725704\n",
      "Train Loss: 0.30032871863357774 | Train Acc: 0.8635514018691589 | Train GradNorm: 1.2025944660739697e-05\n",
      "Epoch [469/500]\n",
      "Test Loss: 0.3628164233276487 | Test Acc: 0.8376082181160357 | Test GradNorm: 0.0007318177790434714\n",
      "Train Loss: 0.30031798385512204 | Train Acc: 0.8635514018691589 | Train GradNorm: 2.9088232541383098e-06\n",
      "Epoch [470/500]\n",
      "Test Loss: 0.36284916860212213 | Test Acc: 0.8375113063703321 | Test GradNorm: 0.0007320457482093928\n",
      "Train Loss: 0.30031374680468154 | Train Acc: 0.8635514018691589 | Train GradNorm: 2.9225631556811255e-06\n",
      "Epoch [471/500]\n",
      "Test Loss: 0.36277544052016514 | Test Acc: 0.837382090709394 | Test GradNorm: 0.0006068327135864485\n",
      "Train Loss: 0.30031369751468345 | Train Acc: 0.8623052959501558 | Train GradNorm: 7.715836500620348e-06\n",
      "Epoch [472/500]\n",
      "Test Loss: 0.36283515603932753 | Test Acc: 0.8375436102855666 | Test GradNorm: 0.0006844709782577851\n",
      "Train Loss: 0.3003044567350057 | Train Acc: 0.8641744548286604 | Train GradNorm: 6.904301345511649e-07\n",
      "Epoch [473/500]\n",
      "Test Loss: 0.3629498707708169 | Test Acc: 0.8374790024550975 | Test GradNorm: 0.0008106063405942905\n",
      "Train Loss: 0.3003106457162914 | Train Acc: 0.8635514018691589 | Train GradNorm: 1.4401997181692486e-05\n",
      "Epoch [474/500]\n",
      "Test Loss: 0.3629771717970408 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.0008418671975057124\n",
      "Train Loss: 0.30031140859684197 | Train Acc: 0.8635514018691589 | Train GradNorm: 2.1334279249886503e-05\n",
      "Epoch [475/500]\n",
      "Test Loss: 0.362930170158092 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.0007396798715290811\n",
      "Train Loss: 0.3002959639656354 | Train Acc: 0.8635514018691589 | Train GradNorm: 3.941257488645539e-06\n",
      "Epoch [476/500]\n",
      "Test Loss: 0.3629462956783508 | Test Acc: 0.8375759142008011 | Test GradNorm: 0.0007849132589684227\n",
      "Train Loss: 0.3002970955110414 | Train Acc: 0.8635514018691589 | Train GradNorm: 1.0431160238555554e-05\n",
      "Epoch [477/500]\n",
      "Test Loss: 0.36291444744532997 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.0007107501092559886\n",
      "Train Loss: 0.30028715025316705 | Train Acc: 0.8635514018691589 | Train GradNorm: 1.7457495019858343e-06\n",
      "Epoch [478/500]\n",
      "Test Loss: 0.36290640341854624 | Test Acc: 0.8374790024550975 | Test GradNorm: 0.0006523521088713432\n",
      "Train Loss: 0.3002827914960999 | Train Acc: 0.8641744548286604 | Train GradNorm: 1.4544239099572192e-06\n",
      "Epoch [479/500]\n",
      "Test Loss: 0.36310915220337114 | Test Acc: 0.8376405220312702 | Test GradNorm: 0.0008909762896911355\n",
      "Train Loss: 0.3003032408333194 | Train Acc: 0.8635514018691589 | Train GradNorm: 3.3573300070966554e-05\n",
      "Epoch [480/500]\n",
      "Test Loss: 0.3629847369027127 | Test Acc: 0.8375113063703321 | Test GradNorm: 0.0006987607435227189\n",
      "Train Loss: 0.3002770701273934 | Train Acc: 0.8641744548286604 | Train GradNorm: 1.2056469584888088e-06\n",
      "Epoch [481/500]\n",
      "Test Loss: 0.36303420480986454 | Test Acc: 0.8375436102855666 | Test GradNorm: 0.0007240155126108308\n",
      "Train Loss: 0.3002737117855559 | Train Acc: 0.8635514018691589 | Train GradNorm: 2.4896659218621584e-06\n",
      "Epoch [482/500]\n",
      "Test Loss: 0.3630196552590473 | Test Acc: 0.837446698539863 | Test GradNorm: 0.0006507557477774015\n",
      "Train Loss: 0.3002701999155176 | Train Acc: 0.8641744548286604 | Train GradNorm: 1.7534432774188686e-06\n",
      "Epoch [483/500]\n",
      "Test Loss: 0.3630442289894467 | Test Acc: 0.837446698539863 | Test GradNorm: 0.0006768159130374001\n",
      "Train Loss: 0.3002664017317716 | Train Acc: 0.8641744548286604 | Train GradNorm: 8.022090663457415e-07\n",
      "Epoch [484/500]\n",
      "Test Loss: 0.36296779142870245 | Test Acc: 0.8377374337769737 | Test GradNorm: 0.0005695431968231292\n",
      "Train Loss: 0.3002750932370911 | Train Acc: 0.8616822429906542 | Train GradNorm: 1.682938225596055e-05\n",
      "Epoch [485/500]\n",
      "Test Loss: 0.3630044990256882 | Test Acc: 0.8375436102855666 | Test GradNorm: 0.0005763691057917115\n",
      "Train Loss: 0.300270362581566 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.4780590316070803e-05\n",
      "Epoch [486/500]\n",
      "Test Loss: 0.36306391266787075 | Test Acc: 0.8375113063703321 | Test GradNorm: 0.0006343835142647568\n",
      "Train Loss: 0.30025659256178505 | Train Acc: 0.8623052959501558 | Train GradNorm: 3.0212978315743545e-06\n",
      "Epoch [487/500]\n",
      "Test Loss: 0.3630462560350715 | Test Acc: 0.8375113063703321 | Test GradNorm: 0.0005826247634823185\n",
      "Train Loss: 0.30026025424720076 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.3055795008987883e-05\n",
      "Epoch [488/500]\n",
      "Test Loss: 0.36301490543489445 | Test Acc: 0.8377697376922083 | Test GradNorm: 0.0005337481928851085\n",
      "Train Loss: 0.3002714744425281 | Train Acc: 0.8610591900311526 | Train GradNorm: 3.309543786030162e-05\n",
      "Epoch [489/500]\n",
      "Test Loss: 0.363053633146024 | Test Acc: 0.8377051298617393 | Test GradNorm: 0.0005397200595100619\n",
      "Train Loss: 0.30026621574751844 | Train Acc: 0.8616822429906542 | Train GradNorm: 3.0324088658085305e-05\n",
      "Epoch [490/500]\n",
      "Test Loss: 0.36311469415081715 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.0006118777337503271\n",
      "Train Loss: 0.30024449539613945 | Train Acc: 0.8623052959501558 | Train GradNorm: 6.182712778350391e-06\n",
      "Epoch [491/500]\n",
      "Test Loss: 0.3631832314722031 | Test Acc: 0.8375436102855666 | Test GradNorm: 0.0006812659954781634\n",
      "Train Loss: 0.30023688278919347 | Train Acc: 0.8635514018691589 | Train GradNorm: 6.587835373863399e-07\n",
      "Epoch [492/500]\n",
      "Test Loss: 0.3632118121196762 | Test Acc: 0.8375759142008011 | Test GradNorm: 0.0006808131038075314\n",
      "Train Loss: 0.3002338570915942 | Train Acc: 0.8635514018691589 | Train GradNorm: 6.664397802437087e-07\n",
      "Epoch [493/500]\n",
      "Test Loss: 0.3632986345678704 | Test Acc: 0.837446698539863 | Test GradNorm: 0.0007489009299932339\n",
      "Train Loss: 0.3002337248184418 | Train Acc: 0.8635514018691589 | Train GradNorm: 4.688925424951915e-06\n",
      "Epoch [494/500]\n",
      "Test Loss: 0.36327812084098904 | Test Acc: 0.8375436102855666 | Test GradNorm: 0.0006929772785540218\n",
      "Train Loss: 0.30022752307170103 | Train Acc: 0.8635514018691589 | Train GradNorm: 8.026477815161987e-07\n",
      "Epoch [495/500]\n",
      "Test Loss: 0.3633426291262937 | Test Acc: 0.8374143946246285 | Test GradNorm: 0.0006964320797295106\n",
      "Train Loss: 0.30022446579084405 | Train Acc: 0.8641744548286604 | Train GradNorm: 9.126842057550347e-07\n",
      "Epoch [496/500]\n",
      "Test Loss: 0.3633709663093965 | Test Acc: 0.8375759142008011 | Test GradNorm: 0.0007377041814597158\n",
      "Train Loss: 0.3002231594998977 | Train Acc: 0.8641744548286604 | Train GradNorm: 3.60214730609395e-06\n",
      "Epoch [497/500]\n",
      "Test Loss: 0.3634887813316093 | Test Acc: 0.8375759142008011 | Test GradNorm: 0.0008833876917637124\n",
      "Train Loss: 0.30024032856047467 | Train Acc: 0.8635514018691589 | Train GradNorm: 3.1087824999599095e-05\n",
      "Epoch [498/500]\n",
      "Test Loss: 0.3633692677780792 | Test Acc: 0.8376728259465047 | Test GradNorm: 0.0007269680441580309\n",
      "Train Loss: 0.3002172717615748 | Train Acc: 0.8641744548286604 | Train GradNorm: 2.9863096628586494e-06\n",
      "Epoch [499/500]\n",
      "Test Loss: 0.36328025959806864 | Test Acc: 0.8375436102855666 | Test GradNorm: 0.0005863741002530033\n",
      "Train Loss: 0.3002205362708448 | Train Acc: 0.8623052959501558 | Train GradNorm: 1.2022732205590295e-05\n"
     ]
    }
   ],
   "source": [
    "hist_adam1 = train_loop(\"a1a\", 0, 128, 500, methods.Adagrad, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/500] | Loss: 0.6931471805599453 | GradNorm^2: 0.7733304714186172 | Acc: 0.0\n",
      "[1/500] | Loss: 0.37490747865397916 | GradNorm^2: 0.008910386048902331 | Acc: 0.8305295950155763\n",
      "[2/500] | Loss: 0.35156821459985155 | GradNorm^2: 0.0022040508504374535 | Acc: 0.8336448598130841\n",
      "[3/500] | Loss: 0.34281508058522037 | GradNorm^2: 0.004263236895754371 | Acc: 0.8404984423676013\n",
      "[4/500] | Loss: 0.33693905897615106 | GradNorm^2: 0.0029019813152215915 | Acc: 0.8398753894080997\n",
      "[5/500] | Loss: 0.3324356074529611 | GradNorm^2: 0.0007497290021224474 | Acc: 0.838006230529595\n",
      "[6/500] | Loss: 0.33190301039256404 | GradNorm^2: 0.007819601368112536 | Acc: 0.8467289719626169\n",
      "[7/500] | Loss: 0.32727628996909364 | GradNorm^2: 0.00020648164698832552 | Acc: 0.8411214953271028\n",
      "[8/500] | Loss: 0.32617344829330525 | GradNorm^2: 0.0028519302173063937 | Acc: 0.8448598130841122\n",
      "[9/500] | Loss: 0.32419537112382335 | GradNorm^2: 0.0011891597406113452 | Acc: 0.8454828660436137\n",
      "[10/500] | Loss: 0.3226607888248423 | GradNorm^2: 0.00023148626267516576 | Acc: 0.8461059190031153\n",
      "[11/500] | Loss: 0.322120293786805 | GradNorm^2: 0.0023704728743352286 | Acc: 0.84797507788162\n",
      "[12/500] | Loss: 0.32332958753713087 | GradNorm^2: 0.010001152599662465 | Acc: 0.8473520249221184\n",
      "[13/500] | Loss: 0.3204312523301247 | GradNorm^2: 0.00264798898361746 | Acc: 0.8517133956386292\n",
      "[14/500] | Loss: 0.3188414748950265 | GradNorm^2: 7.459425440833901e-05 | Acc: 0.8510903426791278\n",
      "[15/500] | Loss: 0.3185268625262819 | GradNorm^2: 0.0013054956332502574 | Acc: 0.8498442367601247\n",
      "[16/500] | Loss: 0.3183832390135949 | GradNorm^2: 0.0030840619814601062 | Acc: 0.8504672897196262\n",
      "[17/500] | Loss: 0.3170675880783051 | GradNorm^2: 0.0004907001027711378 | Acc: 0.8492211838006231\n",
      "[18/500] | Loss: 0.3167160515043238 | GradNorm^2: 0.0011346825133408825 | Acc: 0.8535825545171339\n",
      "[19/500] | Loss: 0.31630442520509433 | GradNorm^2: 0.0013955245267599347 | Acc: 0.8523364485981308\n",
      "[20/500] | Loss: 0.31646662617968435 | GradNorm^2: 0.0030987199118788023 | Acc: 0.8523364485981308\n",
      "[21/500] | Loss: 0.31523734112430873 | GradNorm^2: 0.0005921927355618719 | Acc: 0.8529595015576324\n",
      "[22/500] | Loss: 0.3147005565873336 | GradNorm^2: 6.505854567384927e-05 | Acc: 0.8535825545171339\n",
      "[23/500] | Loss: 0.3143466172382748 | GradNorm^2: 9.605381806939669e-05 | Acc: 0.8542056074766355\n",
      "[24/500] | Loss: 0.3140876335677473 | GradNorm^2: 0.00037336368722345824 | Acc: 0.8523364485981308\n",
      "[25/500] | Loss: 0.31396691135880817 | GradNorm^2: 0.0008850550588522121 | Acc: 0.8504672897196262\n",
      "[26/500] | Loss: 0.31471768961520585 | GradNorm^2: 0.004045647767937883 | Acc: 0.8523364485981308\n",
      "[27/500] | Loss: 0.31350491660395613 | GradNorm^2: 0.0011623706257527678 | Acc: 0.8517133956386292\n",
      "[28/500] | Loss: 0.3128559264865481 | GradNorm^2: 3.186163386121328e-05 | Acc: 0.8523364485981308\n",
      "[29/500] | Loss: 0.3128477431030345 | GradNorm^2: 0.0007506942174316078 | Acc: 0.8542056074766355\n",
      "[30/500] | Loss: 0.3123786254338457 | GradNorm^2: 2.2110955386955387e-05 | Acc: 0.8542056074766355\n",
      "[31/500] | Loss: 0.31215254368189754 | GradNorm^2: 4.974512744311404e-05 | Acc: 0.8529595015576324\n",
      "[32/500] | Loss: 0.31224263883555126 | GradNorm^2: 0.0009486165670742498 | Acc: 0.8523364485981308\n",
      "[33/500] | Loss: 0.3118659083102357 | GradNorm^2: 0.0003956637917160026 | Acc: 0.854828660436137\n",
      "[34/500] | Loss: 0.3116150009970908 | GradNorm^2: 0.0002284221057278729 | Acc: 0.8535825545171339\n",
      "[35/500] | Loss: 0.3114472008970917 | GradNorm^2: 0.0003020147167774407 | Acc: 0.8529595015576324\n",
      "[36/500] | Loss: 0.3113233493962835 | GradNorm^2: 0.000435701785542156 | Acc: 0.8542056074766355\n",
      "[37/500] | Loss: 0.3110395333698348 | GradNorm^2: 1.4755051302955326e-05 | Acc: 0.8542056074766355\n",
      "[38/500] | Loss: 0.31102585476020095 | GradNorm^2: 0.0004306389963373519 | Acc: 0.8554517133956386\n",
      "[39/500] | Loss: 0.31076293112784364 | GradNorm^2: 9.257747403772664e-05 | Acc: 0.8566978193146417\n",
      "[40/500] | Loss: 0.3106173364678666 | GradNorm^2: 0.00017437894970761994 | Acc: 0.8542056074766355\n",
      "[41/500] | Loss: 0.31042803991649276 | GradNorm^2: 1.8192398997583587e-05 | Acc: 0.8542056074766355\n",
      "[42/500] | Loss: 0.3103813013355412 | GradNorm^2: 0.0002620530496668225 | Acc: 0.8523364485981308\n",
      "[43/500] | Loss: 0.3104553874838613 | GradNorm^2: 0.0008179208159475006 | Acc: 0.8542056074766355\n",
      "[44/500] | Loss: 0.31025785609926076 | GradNorm^2: 0.0005985156961812749 | Acc: 0.854828660436137\n",
      "[45/500] | Loss: 0.31052336241654893 | GradNorm^2: 0.0019274865952519668 | Acc: 0.8585669781931464\n",
      "[46/500] | Loss: 0.3098338836382661 | GradNorm^2: 9.263054732315589e-05 | Acc: 0.8560747663551402\n",
      "[47/500] | Loss: 0.30971669777508404 | GradNorm^2: 5.847417622800954e-05 | Acc: 0.8566978193146417\n",
      "[48/500] | Loss: 0.30964058119255 | GradNorm^2: 0.00014364160019413212 | Acc: 0.8573208722741433\n",
      "[49/500] | Loss: 0.3095065566243756 | GradNorm^2: 7.417773015893762e-05 | Acc: 0.8573208722741433\n",
      "[50/500] | Loss: 0.3094975012544321 | GradNorm^2: 0.0003998414032100961 | Acc: 0.8554517133956386\n",
      "[51/500] | Loss: 0.30953032039345624 | GradNorm^2: 0.0007924950084530494 | Acc: 0.8566978193146417\n",
      "[52/500] | Loss: 0.3092047219877296 | GradNorm^2: 3.980267467093069e-05 | Acc: 0.8554517133956386\n",
      "[53/500] | Loss: 0.3091806853619478 | GradNorm^2: 0.0002292102263059431 | Acc: 0.8573208722741433\n",
      "[54/500] | Loss: 0.30902385764911594 | GradNorm^2: 3.644373932412066e-05 | Acc: 0.8573208722741433\n",
      "[55/500] | Loss: 0.30894476256092224 | GradNorm^2: 7.387277142504847e-05 | Acc: 0.8554517133956386\n",
      "[56/500] | Loss: 0.3091913799846494 | GradNorm^2: 0.00112433868373941 | Acc: 0.8579439252336448\n",
      "[57/500] | Loss: 0.30899806893918924 | GradNorm^2: 0.0007907956638856018 | Acc: 0.8566978193146417\n",
      "[58/500] | Loss: 0.3087324490753335 | GradNorm^2: 0.00016815305141458585 | Acc: 0.8573208722741433\n",
      "[59/500] | Loss: 0.30864352566212483 | GradNorm^2: 0.00014953475286950239 | Acc: 0.8573208722741433\n",
      "[60/500] | Loss: 0.3085553738102843 | GradNorm^2: 0.0001335003680794763 | Acc: 0.8573208722741433\n",
      "[61/500] | Loss: 0.3084386425855105 | GradNorm^2: 1.3756645452231381e-05 | Acc: 0.8573208722741433\n",
      "[62/500] | Loss: 0.3083790155522966 | GradNorm^2: 4.743744612718331e-05 | Acc: 0.8566978193146417\n",
      "[63/500] | Loss: 0.30829461617232834 | GradNorm^2: 6.953995990172408e-06 | Acc: 0.8554517133956386\n",
      "[64/500] | Loss: 0.3083304866254187 | GradNorm^2: 0.0003213370973208068 | Acc: 0.8573208722741433\n",
      "[65/500] | Loss: 0.30823815606060784 | GradNorm^2: 0.00022037652813006707 | Acc: 0.8573208722741433\n",
      "[66/500] | Loss: 0.30810728479237975 | GradNorm^2: 7.033255708793777e-05 | Acc: 0.8560747663551402\n",
      "[67/500] | Loss: 0.3080355116481779 | GradNorm^2: 4.4170330404308424e-05 | Acc: 0.8560747663551402\n",
      "[68/500] | Loss: 0.3080717823310103 | GradNorm^2: 0.00032682477068153495 | Acc: 0.8566978193146417\n",
      "[69/500] | Loss: 0.3079803467897523 | GradNorm^2: 0.0002697369803415432 | Acc: 0.8579439252336448\n",
      "[70/500] | Loss: 0.3078419952997741 | GradNorm^2: 9.785933780932484e-06 | Acc: 0.8566978193146417\n",
      "[71/500] | Loss: 0.3077980972021626 | GradNorm^2: 6.321655574077089e-05 | Acc: 0.8573208722741433\n",
      "[72/500] | Loss: 0.3077684582618329 | GradNorm^2: 0.00016849209079751524 | Acc: 0.8573208722741433\n",
      "[73/500] | Loss: 0.308264682719484 | GradNorm^2: 0.001909197203631148 | Acc: 0.8598130841121495\n",
      "[74/500] | Loss: 0.3077993637478719 | GradNorm^2: 0.0006141707271739498 | Acc: 0.8598130841121495\n",
      "[75/500] | Loss: 0.30801658183509134 | GradNorm^2: 0.0014641589345437364 | Acc: 0.8598130841121495\n",
      "[76/500] | Loss: 0.30769154200141235 | GradNorm^2: 0.0006371445453913447 | Acc: 0.859190031152648\n",
      "[77/500] | Loss: 0.3074800862603135 | GradNorm^2: 0.00013716960018494084 | Acc: 0.8573208722741433\n",
      "[78/500] | Loss: 0.3074292365923437 | GradNorm^2: 0.00013713063796552455 | Acc: 0.8573208722741433\n",
      "[79/500] | Loss: 0.30734559205378714 | GradNorm^2: 2.7235726193375862e-05 | Acc: 0.8566978193146417\n",
      "[80/500] | Loss: 0.30742954949003337 | GradNorm^2: 0.00047296716076442193 | Acc: 0.8585669781931464\n",
      "[81/500] | Loss: 0.30726924940574524 | GradNorm^2: 0.00010693538660407986 | Acc: 0.8573208722741433\n",
      "[82/500] | Loss: 0.307183719290728 | GradNorm^2: 1.1434976105548304e-05 | Acc: 0.8585669781931464\n",
      "[83/500] | Loss: 0.3072062479834537 | GradNorm^2: 0.00020757639167283374 | Acc: 0.8573208722741433\n",
      "[84/500] | Loss: 0.30709202470938496 | GradNorm^2: 1.3413472555727242e-05 | Acc: 0.8573208722741433\n",
      "[85/500] | Loss: 0.30706154411054004 | GradNorm^2: 3.768642177947508e-05 | Acc: 0.8566978193146417\n",
      "[86/500] | Loss: 0.3070967051482931 | GradNorm^2: 0.0002549656929776494 | Acc: 0.8573208722741433\n",
      "[87/500] | Loss: 0.3069635551531927 | GradNorm^2: 7.978785457863288e-06 | Acc: 0.8579439252336448\n",
      "[88/500] | Loss: 0.30696388565081617 | GradNorm^2: 0.00014907417619813485 | Acc: 0.859190031152648\n",
      "[89/500] | Loss: 0.3069896659655729 | GradNorm^2: 0.0003601852976499065 | Acc: 0.8573208722741433\n",
      "[90/500] | Loss: 0.30685731104847364 | GradNorm^2: 6.023498223432506e-05 | Acc: 0.8585669781931464\n",
      "[91/500] | Loss: 0.30686408343075805 | GradNorm^2: 0.0002027511569559074 | Acc: 0.8598130841121495\n",
      "[92/500] | Loss: 0.3067610116774753 | GradNorm^2: 2.9291760095565557e-05 | Acc: 0.859190031152648\n",
      "[93/500] | Loss: 0.3067244315383437 | GradNorm^2: 3.507520742350418e-05 | Acc: 0.8554517133956386\n",
      "[94/500] | Loss: 0.3067927079179834 | GradNorm^2: 0.00033876699587131543 | Acc: 0.8566978193146417\n",
      "[95/500] | Loss: 0.3066433641700436 | GradNorm^2: 1.8157405464376794e-05 | Acc: 0.8554517133956386\n",
      "[96/500] | Loss: 0.30668723596648373 | GradNorm^2: 0.0002664907436249047 | Acc: 0.8573208722741433\n",
      "[97/500] | Loss: 0.30657328118968813 | GradNorm^2: 8.209844100190263e-06 | Acc: 0.8560747663551402\n",
      "[98/500] | Loss: 0.30655134078844587 | GradNorm^2: 6.470781619416006e-05 | Acc: 0.8566978193146417\n",
      "[99/500] | Loss: 0.30653308910832033 | GradNorm^2: 0.00011693246040900873 | Acc: 0.8573208722741433\n",
      "[100/500] | Loss: 0.30645877040263875 | GradNorm^2: 7.168714882411994e-06 | Acc: 0.8579439252336448\n",
      "[101/500] | Loss: 0.3064333934246886 | GradNorm^2: 3.7042215632013325e-05 | Acc: 0.859190031152648\n",
      "[102/500] | Loss: 0.30640903701562183 | GradNorm^2: 5.770969779773709e-05 | Acc: 0.859190031152648\n",
      "[103/500] | Loss: 0.3063844587534441 | GradNorm^2: 7.87009282422443e-05 | Acc: 0.859190031152648\n",
      "[104/500] | Loss: 0.30646262323888285 | GradNorm^2: 0.0004420963327751186 | Acc: 0.8604361370716511\n",
      "[105/500] | Loss: 0.30629982435052405 | GradNorm^2: 1.1879739804022943e-05 | Acc: 0.8579439252336448\n",
      "[106/500] | Loss: 0.306364884172466 | GradNorm^2: 0.0003050594538161156 | Acc: 0.8579439252336448\n",
      "[107/500] | Loss: 0.30623115157858016 | GradNorm^2: 1.9234529603028095e-05 | Acc: 0.8598130841121495\n",
      "[108/500] | Loss: 0.3062490712603019 | GradNorm^2: 0.00016216373797333183 | Acc: 0.8573208722741433\n",
      "[109/500] | Loss: 0.3061638664154603 | GradNorm^2: 1.2093103700223697e-05 | Acc: 0.8585669781931464\n",
      "[110/500] | Loss: 0.30613157870168867 | GradNorm^2: 1.446910231341301e-05 | Acc: 0.8579439252336448\n",
      "[111/500] | Loss: 0.30609576319586773 | GradNorm^2: 3.480090573503908e-06 | Acc: 0.859190031152648\n",
      "[112/500] | Loss: 0.30609648725472877 | GradNorm^2: 8.795565345199655e-05 | Acc: 0.8566978193146417\n",
      "[113/500] | Loss: 0.3060581317243376 | GradNorm^2: 6.049165503957205e-05 | Acc: 0.8579439252336448\n",
      "[114/500] | Loss: 0.30602058123227466 | GradNorm^2: 3.3284071059589645e-05 | Acc: 0.8585669781931464\n",
      "[115/500] | Loss: 0.30606045003511934 | GradNorm^2: 0.00025992209112789125 | Acc: 0.8604361370716511\n",
      "[116/500] | Loss: 0.30607219291493964 | GradNorm^2: 0.0003513235604798942 | Acc: 0.8573208722741433\n",
      "[117/500] | Loss: 0.3059344123139772 | GradNorm^2: 4.331929764054062e-05 | Acc: 0.8579439252336448\n",
      "[118/500] | Loss: 0.30595821136027523 | GradNorm^2: 0.00018136347200979177 | Acc: 0.859190031152648\n",
      "[119/500] | Loss: 0.3059422503128754 | GradNorm^2: 0.0002095591087047093 | Acc: 0.8579439252336448\n",
      "[120/500] | Loss: 0.30586117532855545 | GradNorm^2: 6.097043184695805e-05 | Acc: 0.8598130841121495\n",
      "[121/500] | Loss: 0.30581865077620823 | GradNorm^2: 3.093939859182888e-05 | Acc: 0.8585669781931464\n",
      "[122/500] | Loss: 0.30580497904724085 | GradNorm^2: 7.454331162837741e-05 | Acc: 0.8585669781931464\n",
      "[123/500] | Loss: 0.30577061218746515 | GradNorm^2: 4.340203563927744e-05 | Acc: 0.8573208722741433\n",
      "[124/500] | Loss: 0.3057530157596177 | GradNorm^2: 7.021106907964043e-05 | Acc: 0.8573208722741433\n",
      "[125/500] | Loss: 0.30580889943004685 | GradNorm^2: 0.0002875441755434597 | Acc: 0.8585669781931464\n",
      "[126/500] | Loss: 0.3057844545012907 | GradNorm^2: 0.00028178988553414494 | Acc: 0.859190031152648\n",
      "[127/500] | Loss: 0.3056631351523802 | GradNorm^2: 6.076644476210506e-06 | Acc: 0.8566978193146417\n",
      "[128/500] | Loss: 0.30565538246522117 | GradNorm^2: 5.6023911499211005e-05 | Acc: 0.8579439252336448\n",
      "[129/500] | Loss: 0.3056442409584906 | GradNorm^2: 0.00010432214474620033 | Acc: 0.8585669781931464\n",
      "[130/500] | Loss: 0.30560976898274783 | GradNorm^2: 7.193195683614111e-05 | Acc: 0.8585669781931464\n",
      "[131/500] | Loss: 0.3055981226749111 | GradNorm^2: 0.00010421487946908457 | Acc: 0.8585669781931464\n",
      "[132/500] | Loss: 0.30554236064938123 | GradNorm^2: 1.2140802204513425e-05 | Acc: 0.8560747663551402\n",
      "[133/500] | Loss: 0.3055166711781726 | GradNorm^2: 8.294797902556188e-06 | Acc: 0.8560747663551402\n",
      "[134/500] | Loss: 0.305500739165238 | GradNorm^2: 4.492100807987576e-05 | Acc: 0.8573208722741433\n",
      "[135/500] | Loss: 0.3054664923206594 | GradNorm^2: 7.396456435498576e-06 | Acc: 0.8573208722741433\n",
      "[136/500] | Loss: 0.3054549882389991 | GradNorm^2: 2.1594945092230963e-05 | Acc: 0.8560747663551402\n",
      "[137/500] | Loss: 0.3054309685440906 | GradNorm^2: 3.484443192812485e-05 | Acc: 0.8573208722741433\n",
      "[138/500] | Loss: 0.30540333037353357 | GradNorm^2: 2.2884514025776228e-05 | Acc: 0.8560747663551402\n",
      "[139/500] | Loss: 0.3053955082620174 | GradNorm^2: 7.916862780386945e-05 | Acc: 0.8579439252336448\n",
      "[140/500] | Loss: 0.30538556871535066 | GradNorm^2: 0.00011162729811931235 | Acc: 0.8573208722741433\n",
      "[141/500] | Loss: 0.3053379986891959 | GradNorm^2: 3.4719563625192936e-05 | Acc: 0.8573208722741433\n",
      "[142/500] | Loss: 0.305319377228111 | GradNorm^2: 4.1302156505336345e-05 | Acc: 0.8566978193146417\n",
      "[143/500] | Loss: 0.3052928772627852 | GradNorm^2: 3.948983034045433e-05 | Acc: 0.8579439252336448\n",
      "[144/500] | Loss: 0.3052959185279413 | GradNorm^2: 0.00010204546182914263 | Acc: 0.8573208722741433\n",
      "[145/500] | Loss: 0.30524852494365673 | GradNorm^2: 3.426286839384105e-05 | Acc: 0.8579439252336448\n",
      "[146/500] | Loss: 0.30523880601342557 | GradNorm^2: 6.183569934114064e-05 | Acc: 0.8579439252336448\n",
      "[147/500] | Loss: 0.305196495612051 | GradNorm^2: 8.44584794666156e-06 | Acc: 0.8566978193146417\n",
      "[148/500] | Loss: 0.30517744385255613 | GradNorm^2: 6.392772692342555e-06 | Acc: 0.8566978193146417\n",
      "[149/500] | Loss: 0.3051655254756651 | GradNorm^2: 3.927087757761588e-05 | Acc: 0.8585669781931464\n",
      "[150/500] | Loss: 0.3051948736485158 | GradNorm^2: 0.00018179996173782248 | Acc: 0.8579439252336448\n",
      "[151/500] | Loss: 0.3051160320867148 | GradNorm^2: 1.5817176343908617e-05 | Acc: 0.8566978193146417\n",
      "[152/500] | Loss: 0.30509423412177733 | GradNorm^2: 6.6424213358284585e-06 | Acc: 0.8573208722741433\n",
      "[153/500] | Loss: 0.3051020291357139 | GradNorm^2: 9.973767855644898e-05 | Acc: 0.8598130841121495\n",
      "[154/500] | Loss: 0.305081809563027 | GradNorm^2: 8.09848791576792e-05 | Acc: 0.8566978193146417\n",
      "[155/500] | Loss: 0.3050477869628232 | GradNorm^2: 4.6891080849712886e-05 | Acc: 0.8598130841121495\n",
      "[156/500] | Loss: 0.3050194660541203 | GradNorm^2: 2.5467957233231698e-05 | Acc: 0.8585669781931464\n",
      "[157/500] | Loss: 0.3049990037883722 | GradNorm^2: 1.651641907816964e-05 | Acc: 0.8560747663551402\n",
      "[158/500] | Loss: 0.30497407734834764 | GradNorm^2: 7.1144785812038854e-06 | Acc: 0.8579439252336448\n",
      "[159/500] | Loss: 0.30497905749137366 | GradNorm^2: 8.21589761994159e-05 | Acc: 0.8616822429906542\n",
      "[160/500] | Loss: 0.3049383679400678 | GradNorm^2: 1.0230044670620808e-05 | Acc: 0.8579439252336448\n",
      "[161/500] | Loss: 0.30492582617787356 | GradNorm^2: 1.901261863191788e-05 | Acc: 0.8573208722741433\n",
      "[162/500] | Loss: 0.3049438446498615 | GradNorm^2: 0.00013298064552060614 | Acc: 0.8623052959501558\n",
      "[163/500] | Loss: 0.30492278096642844 | GradNorm^2: 0.00012273912981808162 | Acc: 0.8623052959501558\n",
      "[164/500] | Loss: 0.30504762363463017 | GradNorm^2: 0.000561557167844637 | Acc: 0.8604361370716511\n",
      "[165/500] | Loss: 0.30490722706666384 | GradNorm^2: 0.00017903161639148667 | Acc: 0.8604361370716511\n",
      "[166/500] | Loss: 0.3048476796524468 | GradNorm^2: 5.4357190864140575e-05 | Acc: 0.8604361370716511\n",
      "[167/500] | Loss: 0.30486436245328974 | GradNorm^2: 0.00014395909078001356 | Acc: 0.8585669781931464\n",
      "[168/500] | Loss: 0.3047972846893178 | GradNorm^2: 4.85393823772253e-06 | Acc: 0.8585669781931464\n",
      "[169/500] | Loss: 0.3048132490499845 | GradNorm^2: 9.342460341604968e-05 | Acc: 0.8573208722741433\n",
      "[170/500] | Loss: 0.3047694730782132 | GradNorm^2: 1.802502394304256e-05 | Acc: 0.8566978193146417\n",
      "[171/500] | Loss: 0.30476839830365976 | GradNorm^2: 7.571084147258695e-05 | Acc: 0.8616822429906542\n",
      "[172/500] | Loss: 0.304752041564699 | GradNorm^2: 7.620324656152203e-05 | Acc: 0.8623052959501558\n",
      "[173/500] | Loss: 0.3047434310264608 | GradNorm^2: 8.29681267166933e-05 | Acc: 0.8579439252336448\n",
      "[174/500] | Loss: 0.3046990870168491 | GradNorm^2: 6.885025997631805e-06 | Acc: 0.8585669781931464\n",
      "[175/500] | Loss: 0.3046958723235443 | GradNorm^2: 4.485896764212124e-05 | Acc: 0.8579439252336448\n",
      "[176/500] | Loss: 0.30466778358234414 | GradNorm^2: 1.8151978313414857e-05 | Acc: 0.8604361370716511\n",
      "[177/500] | Loss: 0.3047372034567831 | GradNorm^2: 0.00028785267278914466 | Acc: 0.8629283489096573\n",
      "[178/500] | Loss: 0.30463088526356596 | GradNorm^2: 2.0541896979255596e-06 | Acc: 0.8579439252336448\n",
      "[179/500] | Loss: 0.3046250801339336 | GradNorm^2: 3.0022913242256274e-05 | Acc: 0.8623052959501558\n",
      "[180/500] | Loss: 0.304603389364956 | GradNorm^2: 5.579350450211195e-06 | Acc: 0.8598130841121495\n",
      "[181/500] | Loss: 0.30458905127403446 | GradNorm^2: 3.4496198585097476e-06 | Acc: 0.859190031152648\n",
      "[182/500] | Loss: 0.3046280347173551 | GradNorm^2: 0.00018474060486921157 | Acc: 0.8629283489096573\n",
      "[183/500] | Loss: 0.3045629113154757 | GradNorm^2: 1.7865865300012787e-05 | Acc: 0.8573208722741433\n",
      "[184/500] | Loss: 0.3046673849478991 | GradNorm^2: 0.0003947174596268212 | Acc: 0.8598130841121495\n",
      "[185/500] | Loss: 0.30457855023035757 | GradNorm^2: 0.00016807758747880425 | Acc: 0.8629283489096573\n",
      "[186/500] | Loss: 0.30452331816824385 | GradNorm^2: 4.005526248193372e-05 | Acc: 0.8616822429906542\n",
      "[187/500] | Loss: 0.30449888897817695 | GradNorm^2: 5.056145810345108e-06 | Acc: 0.859190031152648\n",
      "[188/500] | Loss: 0.30448550428493776 | GradNorm^2: 1.2422044353719181e-05 | Acc: 0.859190031152648\n",
      "[189/500] | Loss: 0.3044724922261795 | GradNorm^2: 1.4971282732443312e-05 | Acc: 0.859190031152648\n",
      "[190/500] | Loss: 0.30445376469675844 | GradNorm^2: 3.200776902120332e-06 | Acc: 0.859190031152648\n",
      "[191/500] | Loss: 0.3045582354164347 | GradNorm^2: 0.0003753766294039416 | Acc: 0.8604361370716511\n",
      "[192/500] | Loss: 0.3044255224206607 | GradNorm^2: 6.785347717620347e-06 | Acc: 0.8604361370716511\n",
      "[193/500] | Loss: 0.3044776126535651 | GradNorm^2: 0.00021882664803576143 | Acc: 0.8623052959501558\n",
      "[194/500] | Loss: 0.3044164471917279 | GradNorm^2: 7.553905843473251e-05 | Acc: 0.8616822429906542\n",
      "[195/500] | Loss: 0.3045506518589272 | GradNorm^2: 0.0005009087482232067 | Acc: 0.8566978193146417\n",
      "[196/500] | Loss: 0.3043677277032442 | GradNorm^2: 5.021463085712691e-06 | Acc: 0.8579439252336448\n",
      "[197/500] | Loss: 0.3043651454332636 | GradNorm^2: 2.7697242995108436e-05 | Acc: 0.8579439252336448\n",
      "[198/500] | Loss: 0.3044282723314464 | GradNorm^2: 0.00024899626690796093 | Acc: 0.8573208722741433\n",
      "[199/500] | Loss: 0.3044184724325293 | GradNorm^2: 0.00026699106154127935 | Acc: 0.8573208722741433\n",
      "[200/500] | Loss: 0.30434697541836225 | GradNorm^2: 0.00010540727307282949 | Acc: 0.8579439252336448\n",
      "[201/500] | Loss: 0.3043162104249667 | GradNorm^2: 5.35553734663066e-05 | Acc: 0.8585669781931464\n",
      "[202/500] | Loss: 0.3043034274803706 | GradNorm^2: 4.825868209469084e-05 | Acc: 0.859190031152648\n",
      "[203/500] | Loss: 0.30430358814852293 | GradNorm^2: 8.218317867904329e-05 | Acc: 0.8573208722741433\n",
      "[204/500] | Loss: 0.3042779006423866 | GradNorm^2: 5.0721777562296015e-05 | Acc: 0.8579439252336448\n",
      "[205/500] | Loss: 0.30426984596136847 | GradNorm^2: 6.334379930680521e-05 | Acc: 0.8579439252336448\n",
      "[206/500] | Loss: 0.30433038995986417 | GradNorm^2: 0.00027299217354723816 | Acc: 0.8566978193146417\n",
      "[207/500] | Loss: 0.3042359735821806 | GradNorm^2: 3.6321092905279824e-05 | Acc: 0.8579439252336448\n",
      "[208/500] | Loss: 0.30422127059716836 | GradNorm^2: 2.568285898918113e-05 | Acc: 0.8579439252336448\n",
      "[209/500] | Loss: 0.3042550517093964 | GradNorm^2: 0.00015989848526747454 | Acc: 0.8573208722741433\n",
      "[210/500] | Loss: 0.3041968762487033 | GradNorm^2: 2.5185472434159454e-05 | Acc: 0.859190031152648\n",
      "[211/500] | Loss: 0.30417354831661986 | GradNorm^2: 2.5771851186902396e-06 | Acc: 0.8610591900311526\n",
      "[212/500] | Loss: 0.3041608826591004 | GradNorm^2: 2.9032732762116035e-06 | Acc: 0.8610591900311526\n",
      "[213/500] | Loss: 0.3041496774620945 | GradNorm^2: 9.018853079632743e-06 | Acc: 0.8604361370716511\n",
      "[214/500] | Loss: 0.3041349716357161 | GradNorm^2: 3.007666966542749e-06 | Acc: 0.8604361370716511\n",
      "[215/500] | Loss: 0.30413699693161267 | GradNorm^2: 4.6953810521843465e-05 | Acc: 0.8585669781931464\n",
      "[216/500] | Loss: 0.30411026171281097 | GradNorm^2: 1.7147413005412358e-05 | Acc: 0.8610591900311526\n",
      "[217/500] | Loss: 0.3041767216887903 | GradNorm^2: 0.00025711286698970674 | Acc: 0.8629283489096573\n",
      "[218/500] | Loss: 0.30409030858524994 | GradNorm^2: 3.169472743733197e-05 | Acc: 0.8610591900311526\n",
      "[219/500] | Loss: 0.30406895826517083 | GradNorm^2: 1.7221447583867327e-06 | Acc: 0.859190031152648\n",
      "[220/500] | Loss: 0.30405568252230364 | GradNorm^2: 1.6807781837653195e-06 | Acc: 0.8573208722741433\n",
      "[221/500] | Loss: 0.30404835944197256 | GradNorm^2: 1.3326088589083391e-05 | Acc: 0.8573208722741433\n",
      "[222/500] | Loss: 0.3040321567192634 | GradNorm^2: 1.7959219008675202e-06 | Acc: 0.8579439252336448\n",
      "[223/500] | Loss: 0.3041108964980602 | GradNorm^2: 0.0002791314342580543 | Acc: 0.8629283489096573\n",
      "[224/500] | Loss: 0.30405666416145527 | GradNorm^2: 0.00014970057360518286 | Acc: 0.8629283489096573\n",
      "[225/500] | Loss: 0.30399767117733617 | GradNorm^2: 3.056649142079617e-06 | Acc: 0.859190031152648\n",
      "[226/500] | Loss: 0.3039892969077619 | GradNorm^2: 6.673998930027937e-06 | Acc: 0.8585669781931464\n",
      "[227/500] | Loss: 0.3040177287253296 | GradNorm^2: 0.00012752376218329862 | Acc: 0.8623052959501558\n",
      "[228/500] | Loss: 0.3039734133512434 | GradNorm^2: 1.550365845580948e-05 | Acc: 0.8598130841121495\n",
      "[229/500] | Loss: 0.3040015360485604 | GradNorm^2: 0.0001376520408230803 | Acc: 0.8623052959501558\n",
      "[230/500] | Loss: 0.30396768457666484 | GradNorm^2: 7.518724759274379e-05 | Acc: 0.8616822429906542\n",
      "[231/500] | Loss: 0.30399812772590756 | GradNorm^2: 0.00020683549697571104 | Acc: 0.8629283489096573\n",
      "[232/500] | Loss: 0.3039237037612601 | GradNorm^2: 1.609844723663838e-05 | Acc: 0.8604361370716511\n",
      "[233/500] | Loss: 0.3039677817849319 | GradNorm^2: 0.00018986221815171106 | Acc: 0.8623052959501558\n",
      "[234/500] | Loss: 0.3039044223710336 | GradNorm^2: 2.890245010105235e-05 | Acc: 0.8604361370716511\n",
      "[235/500] | Loss: 0.30390349714459375 | GradNorm^2: 5.865691078183569e-05 | Acc: 0.8610591900311526\n",
      "[236/500] | Loss: 0.30389780926966664 | GradNorm^2: 6.311567220742382e-05 | Acc: 0.8579439252336448\n",
      "[237/500] | Loss: 0.30386847248384313 | GradNorm^2: 1.3385704549535632e-05 | Acc: 0.859190031152648\n",
      "[238/500] | Loss: 0.3038520801965258 | GradNorm^2: 1.8381300075859265e-06 | Acc: 0.859190031152648\n",
      "[239/500] | Loss: 0.3039258459153811 | GradNorm^2: 0.00024437960945099355 | Acc: 0.8573208722741433\n",
      "[240/500] | Loss: 0.3038646347617877 | GradNorm^2: 9.541162882008864e-05 | Acc: 0.8585669781931464\n",
      "[241/500] | Loss: 0.30383143451115613 | GradNorm^2: 3.3146062958445183e-05 | Acc: 0.8579439252336448\n",
      "[242/500] | Loss: 0.3038987033250878 | GradNorm^2: 0.0002542573899005377 | Acc: 0.8579439252336448\n",
      "[243/500] | Loss: 0.3038086320719137 | GradNorm^2: 1.9277908885654755e-05 | Acc: 0.8598130841121495\n",
      "[244/500] | Loss: 0.3038003132235039 | GradNorm^2: 3.1336617696342697e-05 | Acc: 0.8604361370716511\n",
      "[245/500] | Loss: 0.3037820906211826 | GradNorm^2: 4.095328706031633e-06 | Acc: 0.859190031152648\n",
      "[246/500] | Loss: 0.3038025679459478 | GradNorm^2: 9.411761367684904e-05 | Acc: 0.859190031152648\n",
      "[247/500] | Loss: 0.30378559022592555 | GradNorm^2: 8.369586705339843e-05 | Acc: 0.8604361370716511\n",
      "[248/500] | Loss: 0.3037526562879945 | GradNorm^2: 1.3087924982834177e-05 | Acc: 0.859190031152648\n",
      "[249/500] | Loss: 0.3037402628964542 | GradNorm^2: 4.323360499635609e-06 | Acc: 0.859190031152648\n",
      "[250/500] | Loss: 0.3037316226642112 | GradNorm^2: 3.2766798982711946e-06 | Acc: 0.859190031152648\n",
      "[251/500] | Loss: 0.3037228903125238 | GradNorm^2: 6.757074568546934e-06 | Acc: 0.8598130841121495\n",
      "[252/500] | Loss: 0.3037099419170701 | GradNorm^2: 1.415783610213655e-06 | Acc: 0.8598130841121495\n",
      "[253/500] | Loss: 0.30376569915613116 | GradNorm^2: 0.0002004102602248599 | Acc: 0.8623052959501558\n",
      "[254/500] | Loss: 0.30373895327288836 | GradNorm^2: 0.00014595779479412895 | Acc: 0.8604361370716511\n",
      "[255/500] | Loss: 0.3037382137577866 | GradNorm^2: 0.00016268156377211257 | Acc: 0.8585669781931464\n",
      "[256/500] | Loss: 0.3037041909270862 | GradNorm^2: 9.465272598878795e-05 | Acc: 0.8579439252336448\n",
      "[257/500] | Loss: 0.3036780747609008 | GradNorm^2: 4.8956299481347636e-05 | Acc: 0.8579439252336448\n",
      "[258/500] | Loss: 0.30366189848807074 | GradNorm^2: 2.4634575649803065e-05 | Acc: 0.8579439252336448\n",
      "[259/500] | Loss: 0.3036510211460082 | GradNorm^2: 2.02068390611957e-05 | Acc: 0.8573208722741433\n",
      "[260/500] | Loss: 0.30368476568453084 | GradNorm^2: 0.00014579733280649113 | Acc: 0.8585669781931464\n",
      "[261/500] | Loss: 0.3036383770022285 | GradNorm^2: 4.3634207091221656e-05 | Acc: 0.8573208722741433\n",
      "[262/500] | Loss: 0.3036218948028616 | GradNorm^2: 2.3310292192567988e-05 | Acc: 0.8566978193146417\n",
      "[263/500] | Loss: 0.30360608424714775 | GradNorm^2: 4.237207194499112e-06 | Acc: 0.859190031152648\n",
      "[264/500] | Loss: 0.3036236553163556 | GradNorm^2: 8.73341383100398e-05 | Acc: 0.8616822429906542\n",
      "[265/500] | Loss: 0.303587044406208 | GradNorm^2: 3.3067810611643045e-06 | Acc: 0.859190031152648\n",
      "[266/500] | Loss: 0.30357627155548983 | GradNorm^2: 1.5088316828444355e-06 | Acc: 0.859190031152648\n",
      "[267/500] | Loss: 0.3035707334877222 | GradNorm^2: 8.443149591100651e-06 | Acc: 0.859190031152648\n",
      "[268/500] | Loss: 0.3035706341370052 | GradNorm^2: 3.2709603678252516e-05 | Acc: 0.8579439252336448\n",
      "[269/500] | Loss: 0.3035571370850456 | GradNorm^2: 2.9409584607854394e-05 | Acc: 0.8604361370716511\n",
      "[270/500] | Loss: 0.3035547359491368 | GradNorm^2: 5.206242611654404e-05 | Acc: 0.8610591900311526\n",
      "[271/500] | Loss: 0.30355151972069 | GradNorm^2: 6.807128842887609e-05 | Acc: 0.8616822429906542\n",
      "[272/500] | Loss: 0.30355986628729487 | GradNorm^2: 0.00012175893712888416 | Acc: 0.8616822429906542\n",
      "[273/500] | Loss: 0.3035406126823072 | GradNorm^2: 8.927283231025523e-05 | Acc: 0.8616822429906542\n",
      "[274/500] | Loss: 0.30350628793083806 | GradNorm^2: 6.025039470426401e-06 | Acc: 0.859190031152648\n",
      "[275/500] | Loss: 0.3035061669011557 | GradNorm^2: 2.908160947612762e-05 | Acc: 0.8573208722741433\n",
      "[276/500] | Loss: 0.30349618359467817 | GradNorm^2: 2.877025167320977e-05 | Acc: 0.859190031152648\n",
      "[277/500] | Loss: 0.3034853311299991 | GradNorm^2: 2.269536462466507e-05 | Acc: 0.859190031152648\n",
      "[278/500] | Loss: 0.30347248358695417 | GradNorm^2: 7.715453771700069e-06 | Acc: 0.8604361370716511\n",
      "[279/500] | Loss: 0.3034692499033158 | GradNorm^2: 2.375453019465238e-05 | Acc: 0.8610591900311526\n",
      "[280/500] | Loss: 0.3034540358441914 | GradNorm^2: 5.845125107524699e-06 | Acc: 0.8604361370716511\n",
      "[281/500] | Loss: 0.3034622186860936 | GradNorm^2: 6.132784767959533e-05 | Acc: 0.8604361370716511\n",
      "[282/500] | Loss: 0.30348566692661294 | GradNorm^2: 0.00015958593200475428 | Acc: 0.8616822429906542\n",
      "[283/500] | Loss: 0.30343214232541604 | GradNorm^2: 1.973376583848248e-05 | Acc: 0.8616822429906542\n",
      "[284/500] | Loss: 0.30342816764853586 | GradNorm^2: 3.098330954257966e-05 | Acc: 0.8616822429906542\n",
      "[285/500] | Loss: 0.3034106050511481 | GradNorm^2: 1.4991355869224078e-06 | Acc: 0.859190031152648\n",
      "[286/500] | Loss: 0.3034026332264692 | GradNorm^2: 3.0018829481655174e-06 | Acc: 0.859190031152648\n",
      "[287/500] | Loss: 0.3033975496132248 | GradNorm^2: 1.2430878262037499e-05 | Acc: 0.8598130841121495\n",
      "[288/500] | Loss: 0.3033862884475614 | GradNorm^2: 4.835620253818574e-06 | Acc: 0.8598130841121495\n",
      "[289/500] | Loss: 0.30337981655473034 | GradNorm^2: 9.937568499132465e-06 | Acc: 0.8604361370716511\n",
      "[290/500] | Loss: 0.30337418292675405 | GradNorm^2: 1.4414875892044281e-05 | Acc: 0.859190031152648\n",
      "[291/500] | Loss: 0.3033616472453948 | GradNorm^2: 3.7709312165551407e-06 | Acc: 0.8598130841121495\n",
      "[292/500] | Loss: 0.3033531894083459 | GradNorm^2: 1.1224098195188869e-06 | Acc: 0.859190031152648\n",
      "[293/500] | Loss: 0.3033646515548383 | GradNorm^2: 5.241704630704425e-05 | Acc: 0.8610591900311526\n",
      "[294/500] | Loss: 0.3033403895809436 | GradNorm^2: 7.354536252464373e-06 | Acc: 0.859190031152648\n",
      "[295/500] | Loss: 0.30334721616121413 | GradNorm^2: 4.7079715171366e-05 | Acc: 0.8616822429906542\n",
      "[296/500] | Loss: 0.3033466357763045 | GradNorm^2: 6.680522946747411e-05 | Acc: 0.8598130841121495\n",
      "[297/500] | Loss: 0.3033844260248088 | GradNorm^2: 0.00019713322767492215 | Acc: 0.859190031152648\n",
      "[298/500] | Loss: 0.3033253796807024 | GradNorm^2: 5.210933902992086e-05 | Acc: 0.8598130841121495\n",
      "[299/500] | Loss: 0.30330619843418394 | GradNorm^2: 1.758387845388628e-05 | Acc: 0.8604361370716511\n",
      "[300/500] | Loss: 0.3032915256543228 | GradNorm^2: 1.1728701547786481e-06 | Acc: 0.8604361370716511\n",
      "[301/500] | Loss: 0.3033119669120566 | GradNorm^2: 8.845723853144729e-05 | Acc: 0.8616822429906542\n",
      "[302/500] | Loss: 0.30331593110616434 | GradNorm^2: 0.00012025694961490824 | Acc: 0.8610591900311526\n",
      "[303/500] | Loss: 0.3032734752806154 | GradNorm^2: 1.0992749956805293e-05 | Acc: 0.8585669781931464\n",
      "[304/500] | Loss: 0.3032631578181151 | GradNorm^2: 2.452387841547543e-06 | Acc: 0.8604361370716511\n",
      "[305/500] | Loss: 0.3032590456665572 | GradNorm^2: 1.2875374514353477e-05 | Acc: 0.8598130841121495\n",
      "[306/500] | Loss: 0.30324674998875545 | GradNorm^2: 2.717354589850523e-06 | Acc: 0.8604361370716511\n",
      "[307/500] | Loss: 0.3032482759050411 | GradNorm^2: 2.7492433229548847e-05 | Acc: 0.859190031152648\n",
      "[308/500] | Loss: 0.3032912126604697 | GradNorm^2: 0.0001748828044523195 | Acc: 0.8598130841121495\n",
      "[309/500] | Loss: 0.3032423777595168 | GradNorm^2: 6.062509371982487e-05 | Acc: 0.8623052959501558\n",
      "[310/500] | Loss: 0.303238238346046 | GradNorm^2: 7.092525748361498e-05 | Acc: 0.8616822429906542\n",
      "[311/500] | Loss: 0.3032225824574144 | GradNorm^2: 4.51935365475587e-05 | Acc: 0.8623052959501558\n",
      "[312/500] | Loss: 0.30320619251608627 | GradNorm^2: 1.3388240134853384e-05 | Acc: 0.8598130841121495\n",
      "[313/500] | Loss: 0.3032168190849553 | GradNorm^2: 6.62097984000675e-05 | Acc: 0.8598130841121495\n",
      "[314/500] | Loss: 0.30323422194952737 | GradNorm^2: 0.00013903148465906718 | Acc: 0.8598130841121495\n",
      "[315/500] | Loss: 0.3031974116605451 | GradNorm^2: 5.128993433001039e-05 | Acc: 0.859190031152648\n",
      "[316/500] | Loss: 0.30322909331222836 | GradNorm^2: 0.00016594470179313002 | Acc: 0.8598130841121495\n",
      "[317/500] | Loss: 0.30317209651315963 | GradNorm^2: 2.2075968466847524e-05 | Acc: 0.8629283489096573\n",
      "[318/500] | Loss: 0.30317722781781814 | GradNorm^2: 5.3727336116678824e-05 | Acc: 0.8629283489096573\n",
      "[319/500] | Loss: 0.30315488108005506 | GradNorm^2: 4.545290826676904e-06 | Acc: 0.8579439252336448\n",
      "[320/500] | Loss: 0.30314646793691497 | GradNorm^2: 1.1097951430395033e-06 | Acc: 0.8604361370716511\n",
      "[321/500] | Loss: 0.3031415228389024 | GradNorm^2: 1.6431376199123576e-06 | Acc: 0.8610591900311526\n",
      "[322/500] | Loss: 0.3031542273536235 | GradNorm^2: 6.62997707560711e-05 | Acc: 0.8629283489096573\n",
      "[323/500] | Loss: 0.30313311257932885 | GradNorm^2: 2.1558774802365935e-05 | Acc: 0.8629283489096573\n",
      "[324/500] | Loss: 0.3031203659135502 | GradNorm^2: 4.701066950097825e-06 | Acc: 0.8610591900311526\n",
      "[325/500] | Loss: 0.3031314444718346 | GradNorm^2: 6.587188651220389e-05 | Acc: 0.8629283489096573\n",
      "[326/500] | Loss: 0.30313877946516155 | GradNorm^2: 0.00010850755798549031 | Acc: 0.8629283489096573\n",
      "[327/500] | Loss: 0.30314745861542886 | GradNorm^2: 0.00015403742168929442 | Acc: 0.8623052959501558\n",
      "[328/500] | Loss: 0.3030942601709261 | GradNorm^2: 1.348070184623758e-05 | Acc: 0.8610591900311526\n",
      "[329/500] | Loss: 0.3030837866005728 | GradNorm^2: 3.857979532975179e-06 | Acc: 0.8598130841121495\n",
      "[330/500] | Loss: 0.30308200634901683 | GradNorm^2: 2.0547710940303157e-05 | Acc: 0.8629283489096573\n",
      "[331/500] | Loss: 0.30310852015286943 | GradNorm^2: 0.00012167747795900857 | Acc: 0.8629283489096573\n",
      "[332/500] | Loss: 0.3030627662221871 | GradNorm^2: 4.537790612165193e-06 | Acc: 0.8610591900311526\n",
      "[333/500] | Loss: 0.3030553950692367 | GradNorm^2: 1.7614204531201126e-06 | Acc: 0.8604361370716511\n",
      "[334/500] | Loss: 0.3030589578456673 | GradNorm^2: 3.0905589314731505e-05 | Acc: 0.8629283489096573\n",
      "[335/500] | Loss: 0.3030434305840704 | GradNorm^2: 1.7171612381556201e-06 | Acc: 0.8598130841121495\n",
      "[336/500] | Loss: 0.3030371655680154 | GradNorm^2: 1.503066650962278e-06 | Acc: 0.8598130841121495\n",
      "[337/500] | Loss: 0.3030379378719431 | GradNorm^2: 2.1425098187564296e-05 | Acc: 0.8623052959501558\n",
      "[338/500] | Loss: 0.3030251690589927 | GradNorm^2: 2.0926683262349644e-06 | Acc: 0.8604361370716511\n",
      "[339/500] | Loss: 0.3030383908415723 | GradNorm^2: 6.281560683536068e-05 | Acc: 0.8629283489096573\n",
      "[340/500] | Loss: 0.30305926165678493 | GradNorm^2: 0.0001453024741629396 | Acc: 0.8610591900311526\n",
      "[341/500] | Loss: 0.3030200566561504 | GradNorm^2: 4.6958400023756985e-05 | Acc: 0.8623052959501558\n",
      "[342/500] | Loss: 0.30303405225152846 | GradNorm^2: 0.00010864075350163126 | Acc: 0.8623052959501558\n",
      "[343/500] | Loss: 0.30299091401728284 | GradNorm^2: 1.3096927274356417e-06 | Acc: 0.8604361370716511\n",
      "[344/500] | Loss: 0.3030012882429447 | GradNorm^2: 4.7435411806780315e-05 | Acc: 0.8604361370716511\n",
      "[345/500] | Loss: 0.30298923827356333 | GradNorm^2: 3.420165920472452e-05 | Acc: 0.8629283489096573\n",
      "[346/500] | Loss: 0.30304042119110464 | GradNorm^2: 0.00021332634353326237 | Acc: 0.8598130841121495\n",
      "[347/500] | Loss: 0.30296572876173256 | GradNorm^2: 4.576493194027548e-06 | Acc: 0.8598130841121495\n",
      "[348/500] | Loss: 0.30295699438074875 | GradNorm^2: 3.1741880040003495e-06 | Acc: 0.8623052959501558\n",
      "[349/500] | Loss: 0.3029509773241718 | GradNorm^2: 3.2545405280717153e-06 | Acc: 0.8604361370716511\n",
      "[350/500] | Loss: 0.3029515673401289 | GradNorm^2: 2.4307882601608958e-05 | Acc: 0.8623052959501558\n",
      "[351/500] | Loss: 0.30295575024327603 | GradNorm^2: 5.312725805577497e-05 | Acc: 0.8598130841121495\n",
      "[352/500] | Loss: 0.30295209575420573 | GradNorm^2: 6.115462367979554e-05 | Acc: 0.8598130841121495\n",
      "[353/500] | Loss: 0.3029607031448525 | GradNorm^2: 0.00010422597545754288 | Acc: 0.859190031152648\n",
      "[354/500] | Loss: 0.3029367917221564 | GradNorm^2: 5.5387287702916825e-05 | Acc: 0.8598130841121495\n",
      "[355/500] | Loss: 0.3029118230112133 | GradNorm^2: 2.9309598542869212e-06 | Acc: 0.8604361370716511\n",
      "[356/500] | Loss: 0.3029068851809438 | GradNorm^2: 4.537445023996334e-06 | Acc: 0.8623052959501558\n",
      "[357/500] | Loss: 0.30290014121250197 | GradNorm^2: 1.4761159427445e-06 | Acc: 0.8623052959501558\n",
      "[358/500] | Loss: 0.3028955877991529 | GradNorm^2: 2.0757233428582676e-06 | Acc: 0.8623052959501558\n",
      "[359/500] | Loss: 0.3029130272967253 | GradNorm^2: 7.408706183600029e-05 | Acc: 0.8623052959501558\n",
      "[360/500] | Loss: 0.3029061342369547 | GradNorm^2: 7.527315963886456e-05 | Acc: 0.8623052959501558\n",
      "[361/500] | Loss: 0.3028865299430071 | GradNorm^2: 3.650002517034789e-05 | Acc: 0.8629283489096573\n",
      "[362/500] | Loss: 0.3028710186156549 | GradNorm^2: 6.2260905299598495e-06 | Acc: 0.8623052959501558\n",
      "[363/500] | Loss: 0.30292476557224657 | GradNorm^2: 0.00019040631048081825 | Acc: 0.8610591900311526\n",
      "[364/500] | Loss: 0.30286936354248856 | GradNorm^2: 3.8267678357455865e-05 | Acc: 0.8629283489096573\n",
      "[365/500] | Loss: 0.3028666022899317 | GradNorm^2: 4.3911543415052486e-05 | Acc: 0.8623052959501558\n",
      "[366/500] | Loss: 0.30286264305767635 | GradNorm^2: 5.163528801919071e-05 | Acc: 0.8616822429906542\n",
      "[367/500] | Loss: 0.30284392519823566 | GradNorm^2: 1.3695628383125995e-05 | Acc: 0.8629283489096573\n",
      "[368/500] | Loss: 0.3028340933485809 | GradNorm^2: 1.3344771325072204e-06 | Acc: 0.8623052959501558\n",
      "[369/500] | Loss: 0.30283270888756003 | GradNorm^2: 1.3011584148869827e-05 | Acc: 0.8598130841121495\n",
      "[370/500] | Loss: 0.30285733945921545 | GradNorm^2: 0.00010984328713838405 | Acc: 0.8616822429906542\n",
      "[371/500] | Loss: 0.3028286083922417 | GradNorm^2: 3.765760519110941e-05 | Acc: 0.8623052959501558\n",
      "[372/500] | Loss: 0.302813713551017 | GradNorm^2: 8.595750391468469e-06 | Acc: 0.8623052959501558\n",
      "[373/500] | Loss: 0.30282516552174044 | GradNorm^2: 6.215817999930074e-05 | Acc: 0.8623052959501558\n",
      "[374/500] | Loss: 0.3028020443620317 | GradNorm^2: 1.2133303337986139e-05 | Acc: 0.8623052959501558\n",
      "[375/500] | Loss: 0.3028023883862559 | GradNorm^2: 2.6012695361349682e-05 | Acc: 0.8579439252336448\n",
      "[376/500] | Loss: 0.30281103732573905 | GradNorm^2: 6.862157498906863e-05 | Acc: 0.8579439252336448\n",
      "[377/500] | Loss: 0.30278776435877963 | GradNorm^2: 1.723722750578651e-05 | Acc: 0.8579439252336448\n",
      "[378/500] | Loss: 0.30277834204199655 | GradNorm^2: 1.1003985581953125e-05 | Acc: 0.8598130841121495\n",
      "[379/500] | Loss: 0.302768959328917 | GradNorm^2: 2.9402357062077403e-06 | Acc: 0.8629283489096573\n",
      "[380/500] | Loss: 0.30276340195030843 | GradNorm^2: 3.9106045062926265e-06 | Acc: 0.8616822429906542\n",
      "[381/500] | Loss: 0.30276650918980336 | GradNorm^2: 2.7441111921079684e-05 | Acc: 0.8604361370716511\n",
      "[382/500] | Loss: 0.30277181400790554 | GradNorm^2: 5.679273573745219e-05 | Acc: 0.859190031152648\n",
      "[383/500] | Loss: 0.30275780632727334 | GradNorm^2: 3.374558274832942e-05 | Acc: 0.8604361370716511\n",
      "[384/500] | Loss: 0.30274816297896673 | GradNorm^2: 2.2777452034556212e-05 | Acc: 0.8598130841121495\n",
      "[385/500] | Loss: 0.30273731076611354 | GradNorm^2: 4.783533561959013e-06 | Acc: 0.8616822429906542\n",
      "[386/500] | Loss: 0.30273037145984233 | GradNorm^2: 1.9099053272399916e-06 | Acc: 0.8616822429906542\n",
      "[387/500] | Loss: 0.3027243311514816 | GradNorm^2: 1.9248291027682158e-06 | Acc: 0.8629283489096573\n",
      "[388/500] | Loss: 0.3027301078900623 | GradNorm^2: 3.482937021607179e-05 | Acc: 0.8604361370716511\n",
      "[389/500] | Loss: 0.3027313301782619 | GradNorm^2: 5.402530443007556e-05 | Acc: 0.859190031152648\n",
      "[390/500] | Loss: 0.30273347548343627 | GradNorm^2: 8.262508583024435e-05 | Acc: 0.8616822429906542\n",
      "[391/500] | Loss: 0.30272345959487607 | GradNorm^2: 6.598471181652444e-05 | Acc: 0.8616822429906542\n",
      "[392/500] | Loss: 0.3026972394818709 | GradNorm^2: 1.6055438382082519e-06 | Acc: 0.8623052959501558\n",
      "[393/500] | Loss: 0.30270094746324255 | GradNorm^2: 2.6821676073469123e-05 | Acc: 0.8610591900311526\n",
      "[394/500] | Loss: 0.3026879357995561 | GradNorm^2: 3.146201487167053e-06 | Acc: 0.8616822429906542\n",
      "[395/500] | Loss: 0.3026895465913163 | GradNorm^2: 2.2549862090732094e-05 | Acc: 0.8629283489096573\n",
      "[396/500] | Loss: 0.3027063348577543 | GradNorm^2: 9.10745217845927e-05 | Acc: 0.8629283489096573\n",
      "[397/500] | Loss: 0.3026721606378349 | GradNorm^2: 2.2398468542237785e-06 | Acc: 0.8616822429906542\n",
      "[398/500] | Loss: 0.30266818454342875 | GradNorm^2: 5.3845386835795184e-06 | Acc: 0.8604361370716511\n",
      "[399/500] | Loss: 0.30266670801909945 | GradNorm^2: 1.392500535443907e-05 | Acc: 0.8616822429906542\n",
      "[400/500] | Loss: 0.30273266664074727 | GradNorm^2: 0.00021674601513796594 | Acc: 0.8598130841121495\n",
      "[401/500] | Loss: 0.30267247265904373 | GradNorm^2: 5.0626412714741004e-05 | Acc: 0.859190031152648\n",
      "[402/500] | Loss: 0.30270343129229277 | GradNorm^2: 0.0001551780346310913 | Acc: 0.859190031152648\n",
      "[403/500] | Loss: 0.30266744009714064 | GradNorm^2: 5.495638484921781e-05 | Acc: 0.859190031152648\n",
      "[404/500] | Loss: 0.30266395588077105 | GradNorm^2: 5.9856824315542695e-05 | Acc: 0.859190031152648\n",
      "[405/500] | Loss: 0.3026901200969348 | GradNorm^2: 0.0001491674043475201 | Acc: 0.859190031152648\n",
      "[406/500] | Loss: 0.3026393364792805 | GradNorm^2: 2.552163322676518e-05 | Acc: 0.8598130841121495\n",
      "[407/500] | Loss: 0.3026278545744002 | GradNorm^2: 9.405526012961448e-06 | Acc: 0.8604361370716511\n",
      "[408/500] | Loss: 0.30261800073986816 | GradNorm^2: 4.9176928885987405e-06 | Acc: 0.8635514018691589\n",
      "[409/500] | Loss: 0.302631355942626 | GradNorm^2: 5.6090333633984446e-05 | Acc: 0.8598130841121495\n",
      "[410/500] | Loss: 0.3026067019986267 | GradNorm^2: 3.6380584876283056e-06 | Acc: 0.8629283489096573\n",
      "[411/500] | Loss: 0.30262166123715045 | GradNorm^2: 5.981365169759833e-05 | Acc: 0.8629283489096573\n",
      "[412/500] | Loss: 0.3026012040151422 | GradNorm^2: 1.000881024110974e-05 | Acc: 0.8635514018691589\n",
      "[413/500] | Loss: 0.30259806090869457 | GradNorm^2: 2.0033716639055078e-05 | Acc: 0.8604361370716511\n",
      "[414/500] | Loss: 0.3025865996658195 | GradNorm^2: 1.2071157464243432e-06 | Acc: 0.8610591900311526\n",
      "[415/500] | Loss: 0.3025922179855472 | GradNorm^2: 3.453835081793405e-05 | Acc: 0.8604361370716511\n",
      "[416/500] | Loss: 0.30257713750745596 | GradNorm^2: 9.591954949530615e-06 | Acc: 0.8610591900311526\n",
      "[417/500] | Loss: 0.30258343623225903 | GradNorm^2: 4.368448050046233e-05 | Acc: 0.8635514018691589\n",
      "[418/500] | Loss: 0.3025671810501027 | GradNorm^2: 8.752722928132488e-06 | Acc: 0.8629283489096573\n",
      "[419/500] | Loss: 0.3025602785424208 | GradNorm^2: 1.1846651771445344e-06 | Acc: 0.8616822429906542\n",
      "[420/500] | Loss: 0.30255795048759154 | GradNorm^2: 1.1189126102884649e-05 | Acc: 0.8629283489096573\n",
      "[421/500] | Loss: 0.302592221386313 | GradNorm^2: 0.00012251329081027558 | Acc: 0.8629283489096573\n",
      "[422/500] | Loss: 0.30255461117606974 | GradNorm^2: 2.446792688378832e-05 | Acc: 0.8629283489096573\n",
      "[423/500] | Loss: 0.3025467642392049 | GradNorm^2: 1.4146745045854889e-05 | Acc: 0.8629283489096573\n",
      "[424/500] | Loss: 0.3025383052489339 | GradNorm^2: 3.752402210678485e-06 | Acc: 0.8623052959501558\n",
      "[425/500] | Loss: 0.3025335716016256 | GradNorm^2: 7.927354021402387e-06 | Acc: 0.8623052959501558\n",
      "[426/500] | Loss: 0.3025284804955575 | GradNorm^2: 7.3507628066884175e-06 | Acc: 0.8604361370716511\n",
      "[427/500] | Loss: 0.3025234955539511 | GradNorm^2: 1.4457516583545896e-05 | Acc: 0.8629283489096573\n",
      "[428/500] | Loss: 0.3025164937808731 | GradNorm^2: 1.028776034421284e-05 | Acc: 0.8629283489096573\n",
      "[429/500] | Loss: 0.3025265069731563 | GradNorm^2: 5.247863844219205e-05 | Acc: 0.8616822429906542\n",
      "[430/500] | Loss: 0.3025619733287595 | GradNorm^2: 0.00017415707396009888 | Acc: 0.8629283489096573\n",
      "[431/500] | Loss: 0.3025334336072136 | GradNorm^2: 9.016186555929422e-05 | Acc: 0.8604361370716511\n",
      "[432/500] | Loss: 0.3024998034773545 | GradNorm^2: 1.215821152356442e-05 | Acc: 0.8623052959501558\n",
      "[433/500] | Loss: 0.3024941314198579 | GradNorm^2: 3.2638874450637823e-06 | Acc: 0.8623052959501558\n",
      "[434/500] | Loss: 0.3024888383935717 | GradNorm^2: 4.297544431582426e-06 | Acc: 0.8623052959501558\n",
      "[435/500] | Loss: 0.30248385717569926 | GradNorm^2: 1.6502314047677078e-06 | Acc: 0.8629283489096573\n",
      "[436/500] | Loss: 0.3025309126932297 | GradNorm^2: 0.0001606414091698819 | Acc: 0.8623052959501558\n",
      "[437/500] | Loss: 0.3024910196563612 | GradNorm^2: 5.394039545628168e-05 | Acc: 0.8623052959501558\n",
      "[438/500] | Loss: 0.30250213776684287 | GradNorm^2: 0.00010141294074182837 | Acc: 0.8623052959501558\n",
      "[439/500] | Loss: 0.30252258259561576 | GradNorm^2: 0.0001797956818411144 | Acc: 0.8623052959501558\n",
      "[440/500] | Loss: 0.30246110115171293 | GradNorm^2: 6.4774706724938725e-06 | Acc: 0.8623052959501558\n",
      "[441/500] | Loss: 0.3024757422842137 | GradNorm^2: 6.560378615017728e-05 | Acc: 0.8623052959501558\n",
      "[442/500] | Loss: 0.30248279798448396 | GradNorm^2: 9.619116643621357e-05 | Acc: 0.8623052959501558\n",
      "[443/500] | Loss: 0.30245732516329027 | GradNorm^2: 3.5889459689795243e-05 | Acc: 0.8629283489096573\n",
      "[444/500] | Loss: 0.30244601471620064 | GradNorm^2: 1.4368461368487763e-05 | Acc: 0.8623052959501558\n",
      "[445/500] | Loss: 0.3024578463689248 | GradNorm^2: 6.032899462511303e-05 | Acc: 0.8629283489096573\n",
      "[446/500] | Loss: 0.30243486505869355 | GradNorm^2: 6.180012799944893e-06 | Acc: 0.8604361370716511\n",
      "[447/500] | Loss: 0.302427925093179 | GradNorm^2: 1.9372067910843446e-06 | Acc: 0.8604361370716511\n",
      "[448/500] | Loss: 0.302432937925132 | GradNorm^2: 3.0867861153308955e-05 | Acc: 0.8623052959501558\n",
      "[449/500] | Loss: 0.30241879590826326 | GradNorm^2: 4.0613174817819645e-06 | Acc: 0.8610591900311526\n",
      "[450/500] | Loss: 0.30243052671543796 | GradNorm^2: 5.3528562976621674e-05 | Acc: 0.8623052959501558\n",
      "[451/500] | Loss: 0.3024103715055342 | GradNorm^2: 6.848467217928343e-06 | Acc: 0.8623052959501558\n",
      "[452/500] | Loss: 0.3024185951651899 | GradNorm^2: 4.270335987775183e-05 | Acc: 0.8610591900311526\n",
      "[453/500] | Loss: 0.3024147017140462 | GradNorm^2: 4.614463405153499e-05 | Acc: 0.8616822429906542\n",
      "[454/500] | Loss: 0.3023989265736454 | GradNorm^2: 1.4605395625012561e-05 | Acc: 0.8610591900311526\n",
      "[455/500] | Loss: 0.3024180119837048 | GradNorm^2: 7.931685380270739e-05 | Acc: 0.8610591900311526\n",
      "[456/500] | Loss: 0.3023865790687428 | GradNorm^2: 4.349300301659383e-06 | Acc: 0.8604361370716511\n",
      "[457/500] | Loss: 0.30239983098877254 | GradNorm^2: 5.967361191123992e-05 | Acc: 0.8623052959501558\n",
      "[458/500] | Loss: 0.30237595675659495 | GradNorm^2: 1.5187474648772987e-06 | Acc: 0.8629283489096573\n",
      "[459/500] | Loss: 0.30238641914163206 | GradNorm^2: 4.040291374950737e-05 | Acc: 0.8604361370716511\n",
      "[460/500] | Loss: 0.30239706580974185 | GradNorm^2: 8.459022335147217e-05 | Acc: 0.8610591900311526\n",
      "[461/500] | Loss: 0.3023729470647888 | GradNorm^2: 2.566564183546965e-05 | Acc: 0.8610591900311526\n",
      "[462/500] | Loss: 0.302366400556436 | GradNorm^2: 2.5355060182999623e-05 | Acc: 0.8616822429906542\n",
      "[463/500] | Loss: 0.3023568726728845 | GradNorm^2: 7.391926768482478e-06 | Acc: 0.8610591900311526\n",
      "[464/500] | Loss: 0.3023509476207013 | GradNorm^2: 5.421225213381174e-06 | Acc: 0.8629283489096573\n",
      "[465/500] | Loss: 0.30234685615846196 | GradNorm^2: 1.728239310524372e-06 | Acc: 0.8610591900311526\n",
      "[466/500] | Loss: 0.3023448707940491 | GradNorm^2: 5.715037100207795e-06 | Acc: 0.8610591900311526\n",
      "[467/500] | Loss: 0.30237559057261965 | GradNorm^2: 0.00010186598138417503 | Acc: 0.8623052959501558\n",
      "[468/500] | Loss: 0.3023376691795922 | GradNorm^2: 5.229957429183549e-06 | Acc: 0.8623052959501558\n",
      "[469/500] | Loss: 0.3023328645682381 | GradNorm^2: 2.4768585036380266e-06 | Acc: 0.8629283489096573\n",
      "[470/500] | Loss: 0.3023274003715684 | GradNorm^2: 1.99436955278537e-06 | Acc: 0.8629283489096573\n",
      "[471/500] | Loss: 0.30232219160321117 | GradNorm^2: 1.788345244571423e-06 | Acc: 0.8629283489096573\n",
      "[472/500] | Loss: 0.302316069810843 | GradNorm^2: 1.4081557761914377e-06 | Acc: 0.8629283489096573\n",
      "[473/500] | Loss: 0.3023122662955613 | GradNorm^2: 3.1952408167495002e-06 | Acc: 0.8623052959501558\n",
      "[474/500] | Loss: 0.3023086645202902 | GradNorm^2: 6.6938221116414915e-06 | Acc: 0.8616822429906542\n",
      "[475/500] | Loss: 0.302303459876713 | GradNorm^2: 5.492635016151063e-06 | Acc: 0.8616822429906542\n",
      "[476/500] | Loss: 0.3023032089232558 | GradNorm^2: 2.0729913921406816e-05 | Acc: 0.8616822429906542\n",
      "[477/500] | Loss: 0.3023060078978888 | GradNorm^2: 3.561391692068179e-05 | Acc: 0.8604361370716511\n",
      "[478/500] | Loss: 0.3023094824326405 | GradNorm^2: 5.8557302653645826e-05 | Acc: 0.8604361370716511\n",
      "[479/500] | Loss: 0.3022854102060258 | GradNorm^2: 1.307821395835457e-06 | Acc: 0.8616822429906542\n",
      "[480/500] | Loss: 0.3022884144270872 | GradNorm^2: 2.256910184317108e-05 | Acc: 0.8610591900311526\n",
      "[481/500] | Loss: 0.30228156314833443 | GradNorm^2: 1.583793427043238e-05 | Acc: 0.8610591900311526\n",
      "[482/500] | Loss: 0.3022750528593483 | GradNorm^2: 7.189932889167217e-06 | Acc: 0.8610591900311526\n",
      "[483/500] | Loss: 0.30226785992762223 | GradNorm^2: 1.1096029362782385e-06 | Acc: 0.8616822429906542\n",
      "[484/500] | Loss: 0.30226989794726117 | GradNorm^2: 2.3118845516050425e-05 | Acc: 0.8616822429906542\n",
      "[485/500] | Loss: 0.30226688020631276 | GradNorm^2: 2.5548285037534256e-05 | Acc: 0.8616822429906542\n",
      "[486/500] | Loss: 0.3022600414006681 | GradNorm^2: 1.346623850123067e-05 | Acc: 0.8604361370716511\n",
      "[487/500] | Loss: 0.3022513357313835 | GradNorm^2: 1.6621088452296983e-06 | Acc: 0.8616822429906542\n",
      "[488/500] | Loss: 0.30224786279577737 | GradNorm^2: 4.02246317832189e-06 | Acc: 0.8610591900311526\n",
      "[489/500] | Loss: 0.3022486265184681 | GradNorm^2: 1.3855637451471257e-05 | Acc: 0.8604361370716511\n",
      "[490/500] | Loss: 0.3022414184723417 | GradNorm^2: 5.205432672144069e-06 | Acc: 0.8616822429906542\n",
      "[491/500] | Loss: 0.3022555254711789 | GradNorm^2: 6.182037406113203e-05 | Acc: 0.8616822429906542\n",
      "[492/500] | Loss: 0.3022329591800064 | GradNorm^2: 2.979583209320154e-06 | Acc: 0.8616822429906542\n",
      "[493/500] | Loss: 0.30223088862988107 | GradNorm^2: 6.9444429158281304e-06 | Acc: 0.8616822429906542\n",
      "[494/500] | Loss: 0.3022559725762502 | GradNorm^2: 8.796672740284128e-05 | Acc: 0.8616822429906542\n",
      "[495/500] | Loss: 0.30222880982472555 | GradNorm^2: 1.9711843586930587e-05 | Acc: 0.8604361370716511\n",
      "[496/500] | Loss: 0.30222011943721705 | GradNorm^2: 1.3452513807725778e-05 | Acc: 0.8616822429906542\n",
      "[497/500] | Loss: 0.302213000627849 | GradNorm^2: 1.1482782573376175e-06 | Acc: 0.8616822429906542\n",
      "[498/500] | Loss: 0.3022104560892945 | GradNorm^2: 1.4538457014062955e-06 | Acc: 0.8616822429906542\n",
      "[499/500] | Loss: 0.3022070972778586 | GradNorm^2: 3.854709016235836e-06 | Acc: 0.8616822429906542\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "hist_adam2 = run_optimizer(optim.Adagrad, logreg, train_data, train_target, train_dataloader, 500, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x14a7e093eea0>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogy(hist_adam1[\"train/epoch/loss\"], label=\"hist1\")\n",
    "plt.semilogy([x[0] for x in hist_adam2], label=\"hist2\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
