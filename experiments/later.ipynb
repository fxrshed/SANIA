{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SP2 for GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sp2_glm(train_data, train_target, train_dataloader, epochs, seed=0):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # parameters\n",
    "    w = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "\n",
    "    # save loss and grad size to history\n",
    "    hist = []\n",
    "\n",
    "    # define loss function\n",
    "    loss_function = lf.logreg\n",
    "\n",
    "    def logreg_a(w, X, y):\n",
    "        r = torch.exp(-y * (X @ w))\n",
    "        return (r/(1 + r)) * -y\n",
    "\n",
    "    def logreg_h(w, X, y):\n",
    "        r = torch.exp(-y * (X @ w))\n",
    "        return (r/torch.square(1 + r)) \n",
    "\n",
    "\n",
    "    loss = loss_function(w, train_data.to(device), train_target.to(device))\n",
    "    g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "    f_grad = g.clone().detach() \n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        loss = loss_function(w, train_data.to(device), train_target.to(device))\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "        grad_norm_sq = torch.linalg.norm(g) ** 2  \n",
    "        acc = (np.sign(train_data @ w.detach().numpy()) == train_target).sum() / train_target.shape[0]\n",
    "\n",
    "        print(f\"[{epoch}/{epochs}] | Loss: {loss.item()} | GradNorm^2: {grad_norm_sq.item()} | Accuracy: {acc}\")\n",
    "        hist.append([loss.item(), grad_norm_sq.item(), acc])\n",
    "            \n",
    "        for i, (batch_data, batch_target) in enumerate(train_dataloader):  \n",
    "\n",
    "            loss = loss_function(w, batch_data, batch_target)\n",
    "            g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "            f_grad = g.clone().detach()\n",
    "\n",
    "            a = logreg_a(w, batch_data, batch_target)\n",
    "            h = logreg_h(w, batch_data, batch_target)\n",
    "\n",
    "            det = a*a - 2 * h * loss\n",
    "            if det >= 0:\n",
    "                step_size = (1 - (torch.sqrt(det) / torch.abs(a))).item()\n",
    "            else:\n",
    "                step_size = 1.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                w.sub_((a/h)*(batch_data.flatten()/torch.norm(batch_data)**2), alpha=step_size)\n",
    "\n",
    "\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SP2+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sp2(train_data, train_target, train_dataloader, epochs, lr=1.0):\n",
    "\n",
    "    w_tp1 = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "    w_t = w_tp1 * 1.0\n",
    "\n",
    "    # logging \n",
    "    hist = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        loss = loss_function(w_tp1, train_data.to(device), train_target.to(device))\n",
    "        g, = torch.autograd.grad(loss, w_tp1, create_graph=True)\n",
    "        acc = (np.sign(train_data @ w_tp1.detach().numpy()) == train_target).sum() / train_target.shape[0]\n",
    "        print(f\"[{epoch}/{epochs}] | Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()} | Acc: {acc}\")\n",
    "        hist.append([loss.item(), (torch.linalg.norm(g) ** 2).item(), acc])\n",
    "\n",
    "        for i, (batch_data, batch_target) in enumerate(train_dataloader):\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_target = batch_target.to(device)\n",
    "\n",
    "            closure  = lambda w: loss_function(w, batch_data, batch_target)\n",
    "            wdiff = torch.sub(w_t, w_tp1)\n",
    "            hessvgrad = torch.autograd.functional.hvp(closure, w_tp1, wdiff, create_graph=True)[1]\n",
    "            with torch.no_grad():\n",
    "                q = loss + torch.dot(g, wdiff) + 0.5 * torch.dot(wdiff, hessvgrad)\n",
    "                nablaq = torch.add(g, hessvgrad)\n",
    "                nablaqnorm = torch.norm(nablaq)\n",
    "                if nablaqnorm < 1e-22:\n",
    "                    break\n",
    "                w_t = w_tp1 * 1.0\n",
    "                w_tp1.sub_(nablaq, alpha = lr*q/nablaqnorm**2)\n",
    "\n",
    "    return hist\n",
    "\n",
    "def run_sp2plus(train_data, train_target, train_dataloader, epochs, lr=1.0):\n",
    "\n",
    "    w = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "    # save loss and grad size to history\n",
    "    hist = []\n",
    "       \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        loss = loss_function(w, train_data.to(device), train_target.to(device))\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "        grad_norm_sq = torch.linalg.norm(g) ** 2  \n",
    "        acc = (np.sign(train_data @ w.detach().numpy()) == train_target).sum() / train_target.shape[0]\n",
    "\n",
    "        print(f\"[{epoch}/{epochs}] | Loss: {loss.item()} | GradNorm^2: {grad_norm_sq.item()} | Accuracy: {acc}\")\n",
    "        hist.append([loss.item(), grad_norm_sq.item(), acc])\n",
    "            \n",
    "\n",
    "        for i, (batch_data, batch_target) in enumerate(train_dataloader):\n",
    "\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_target = batch_target.to(device)\n",
    "\n",
    "            loss = loss_function(w, batch_data, batch_target)\n",
    "            g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "            f_grad = g.clone().detach()\n",
    "\n",
    "            loss_closure = lambda w: loss_function(w, batch_data, batch_target)\n",
    "            hgp = torch.autograd.functional.hvp(loss_closure, w, g, create_graph=True)[1]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                gnormsq = torch.norm(f_grad)**2\n",
    "                sps_step = loss.item() / gnormsq\n",
    "                w.sub_(sps_step * f_grad, alpha=lr)\n",
    "                gdiffHgp = torch.sub(f_grad, hgp, alpha=sps_step)\n",
    "                if torch.norm(gdiffHgp)**2 > 1e-10:\n",
    "                        w.sub_(0.5 * (sps_step**2) * gdiffHgp * torch.dot(f_grad, gdiffHgp)/ (torch.norm(gdiffHgp)**2))\n",
    "            \n",
    "\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp_from_grad(grads_tuple, list_params, vec_tuple):\n",
    "    # don't damage grads_tuple. Grads_tuple should be calculated with create_graph=True\n",
    "    dot = 0.\n",
    "    for grad, vec in zip(grads_tuple, vec_tuple):\n",
    "        dot += grad.mul(vec).sum()\n",
    "    return torch.autograd.grad(dot, list_params, retain_graph=True)[0]\n",
    "\n",
    "\n",
    "def custom_sp2plus(train_data, train_target, train_dataloader, epochs, lr=1.0):\n",
    "\n",
    "    w = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "    # save loss and grad size to history\n",
    "    hist = []\n",
    "\n",
    "    eps = 1e-8\n",
    "       \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        loss = loss_function(w, train_data.to(device), train_target.to(device))\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "        grad_norm_sq = torch.linalg.norm(g) ** 2  \n",
    "        acc = (np.sign(train_data @ w.detach().numpy()) == train_target).sum() / train_target.shape[0]\n",
    "\n",
    "        print(f\"[{epoch}/{epochs}] | Loss: {loss.item()} | GradNorm^2: {grad_norm_sq.item()} | Accuracy: {acc}\")\n",
    "        hist.append([loss.item(), grad_norm_sq.item(), acc])\n",
    "            \n",
    "\n",
    "        for i, (batch_data, batch_target) in enumerate(train_dataloader):\n",
    "\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_target = batch_target.to(device)\n",
    "\n",
    "            loss = loss_function(w, batch_data, batch_target)\n",
    "            g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "            f_grad = g.clone().detach()\n",
    "\n",
    "            loss_closure = lambda w: loss_function(w, batch_data, batch_target)\n",
    "            # hgp = torch.autograd.functional.hvp(loss_closure, w, g, create_graph=True)[1]\n",
    "            hgp = torch.autograd.grad(g, w, grad_outputs=g, retain_graph=True)[0]\n",
    "\n",
    "            # hgp = hvp_from_grad(list(g), w, list(g))\n",
    "            # print(torch.norm(hgp - hgp2))\n",
    "            \n",
    "            grad_norm_sq = torch.dot(f_grad, f_grad)\n",
    "            polyak = loss / (grad_norm_sq + eps)\n",
    "            v = f_grad - (hgp * polyak)\n",
    "            v_norm_sq = torch.dot(v, v)\n",
    "            step = (polyak * f_grad) + (0.5 * polyak**2 * (torch.dot(hgp, f_grad) / (v_norm_sq + eps )) * v) \n",
    "\n",
    "            with torch.no_grad():\n",
    "                w.sub_(step, alpha=lr)\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SP2Plus(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            params,\n",
    "            lr=1.0,\n",
    "            eps=1e-8):\n",
    "        \n",
    "        defaults = dict(lr=lr, eps=eps)\n",
    "\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        \n",
    "        loss = None \n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "\n",
    "        # loss_closure = lambda w: loss_function(w, batch_data, batch_target)\n",
    "        # hgp = torch.autograd.functional.hvp(loss_closure, w, g, create_graph=True)[1]\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                grad_flat = torch.flatten(p.grad)\n",
    "                p_flat = torch.flatten(p)\n",
    "                eps = group[\"eps\"]\n",
    "                lr =group[\"lr\"]\n",
    "                hgp = torch.autograd.grad(grad_flat, p_flat, grad_outputs=grad_flat, retain_graph=True)[0]\n",
    "                # hgp = torch.autograd.functional.hvp(closure, p, p.grad)[1]\n",
    "                # hgp = hvp_from_grad(list(p.grad), p, list(p.grad))\n",
    "                \n",
    "                grad_norm_sq = torch.dot(grad_flat, grad_flat)\n",
    "                polyak = loss / (grad_norm_sq + eps)\n",
    "                v = grad_flat - (hgp * polyak)\n",
    "                v_norm_sq = torch.dot(v, v)\n",
    "                step = (polyak * grad_flat) + (0.5 * polyak**2 * (torch.dot(hgp, grad_flat) / (v_norm_sq + eps )) * v) \n",
    "                with torch.no_grad():\n",
    "                    p.sub_(step, alpha=lr)\n",
    "\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            params,\n",
    "            eps=1e-8):\n",
    "        \n",
    "        defaults = dict(eps=eps)\n",
    "\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "        self._update_precond_grad = self._update_precond_grad_cg\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                self.state[p][\"v\"] = torch.flatten(torch.zeros_like(p))\n",
    "\n",
    "        self._step_t = 0\n",
    "        \n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \n",
    "        loss = None \n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        self._step_t += 1\n",
    "        self._update_precond_grad()\n",
    "        self.update(loss=loss)\n",
    "            \n",
    "                          \n",
    "        return loss \n",
    "\n",
    "    def update(self, loss):\n",
    "        for group in self.param_groups: \n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                precond_grad = state[\"precond_grad\"]\n",
    "                flat_grad = torch.flatten(p.grad.detach().clone())\n",
    "                grad_norm_sq = torch.dot(flat_grad, precond_grad)\n",
    "                eps = group['eps']\n",
    "                if 2 * loss <= grad_norm_sq:    \n",
    "                    det = 1 - 2 * (loss / (grad_norm_sq + eps))\n",
    "                    if det < 0.0:\n",
    "                        group[\"step_size\"] = 1.0\n",
    "                    else:\n",
    "                        group[\"step_size\"] = 1 - torch.sqrt(det).item()\n",
    "                else:\n",
    "                    group[\"step_size\"] = 1.0\n",
    "\n",
    "                # group[\"step_size\"] = 0.01\n",
    "                with torch.no_grad():\n",
    "                    p.sub_(precond_grad.view_as(p), alpha=group['step_size'])\n",
    "\n",
    "    def _update_precond_grad_identity(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                self.state[p][\"precond_grad\"] = torch.flatten(p.grad)\n",
    "\n",
    "\n",
    "    def _update_precond_grad_cg(self):\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                state = self.state[p]\n",
    "                p_flat = torch.flatten(p.detach().clone())\n",
    "                grad_flat = torch.flatten(p.grad.detach().clone())\n",
    "\n",
    "                s = torch.zeros_like(p_flat) # s = H_inv * grad\n",
    "                r = grad_flat.clone()\n",
    "                b = r.clone()\n",
    "                MAX_ITER = p.shape[0] * 2\n",
    "\n",
    "                for cg_step in range(MAX_ITER):\n",
    "                    hvp = torch.flatten(torch.autograd.grad(p.grad, p, grad_outputs=b.view_as(p), retain_graph=True)[0])\n",
    "                    alpha_k = torch.dot(r, r) / torch.dot(b, hvp)\n",
    "                    s = s + alpha_k * b\n",
    "                    r_prev = r.clone()\n",
    "                    r = r - alpha_k * hvp\n",
    "                    if torch.norm(r) < 1e-4:\n",
    "                        # Ax = torch.autograd.grad(g, w, grad_outputs=s, retain_graph=True)[0]    \n",
    "                        # diff = torch.norm(Ax - f_grad)\n",
    "                        break\n",
    "\n",
    "                    beta_k = torch.dot(r, r) / torch.dot(r_prev, r_prev)\n",
    "                    b = r + beta_k * b\n",
    "\n",
    "                state[\"precond_grad\"] = s\n",
    "\n",
    "\n",
    "    def _update_precond_grad_adagrad(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                state = self.state[p]\n",
    "                flat_grad = torch.flatten(p.grad.detach().clone())\n",
    "                state[\"v\"] = state[\"v\"] + torch.square(flat_grad)\n",
    "                precond = 1 / (torch.sqrt( state[\"v\"]) + 1e-10)\n",
    "                state[\"precond_grad\"] = torch.mul(precond, flat_grad)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rademacher_old(weights):\n",
    "    return torch.round(torch.rand_like(weights)) * 2 - 1\n",
    "\n",
    "def diag_estimate_old(weights, grad, iters):\n",
    "    Ds = []\n",
    "    for j in range(iters):\n",
    "        z = rademacher_old(weights)\n",
    "        with torch.no_grad():\n",
    "            hvp = torch.autograd.grad(grad, weights, grad_outputs=z, retain_graph=True)[0]\n",
    "        Ds.append((hvp*z))\n",
    "\n",
    "    return torch.mean(torch.stack(Ds), 0)\n",
    "\n",
    "def run_psps(data, target, dataloader, EPOCHS, precond_method=\"none\", seed=0, scaling_vec=None):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    alpha=0.1\n",
    "    beta=0.999\n",
    "    eps = 1e-12\n",
    "\n",
    "    w = torch.zeros(data.shape[1], device=device).requires_grad_()\n",
    "    hist = []\n",
    "\n",
    "    loss = loss_function(w, data, target)\n",
    "    g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "\n",
    "    if precond_method == \"none\":\n",
    "        D = torch.eye(w.shape[0])\n",
    "    elif precond_method == \"hutch\":\n",
    "        Dk = diag_estimate_old(w, g, 100)\n",
    "    elif precond_method == \"cg\":\n",
    "        s = torch.zeros_like(w) # s = H_inv * grad\n",
    "        f_grad = torch.zeros_like(w)\n",
    "        r = torch.zeros_like(w)\n",
    "        p = r.detach().clone()\n",
    "        r_prev = torch.dot(r, r)\n",
    "        MAX_ITER = 1000\n",
    "    elif precond_method == \"scaling_vec\":\n",
    "        D = torch.diag((1 / scaling_vec)**2)\n",
    "    elif precond_method == \"adam\":\n",
    "        D = torch.zeros_like(g)\n",
    "        v = torch.zeros_like(g)\n",
    "        step_t = torch.tensor(0.)\n",
    "        betas = (0.9, 0.999)\n",
    "    elif precond_method == \"adagrad\":\n",
    "        D = torch.zeros_like(g)\n",
    "        v = torch.zeros_like(g)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        loss = loss_function(w, data, target)\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "        acc = (np.sign(data @ w.detach().numpy()) == target).sum() / target.shape[0]\n",
    "\n",
    "        print(f\"[{epoch}/{EPOCHS}] | Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()} | Accuracy: {acc}\")\n",
    "        hist.append([loss.item(), (torch.linalg.norm(g) ** 2 ).item(), acc])\n",
    "\n",
    "        for i, (batch_data, batch_target) in enumerate(dataloader):\n",
    "            loss = loss_function(w, batch_data, batch_target)\n",
    "            g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "            f_grad = g.clone().detach()\n",
    "\n",
    "            if precond_method == \"hess_diag\":\n",
    "                D = loss_hessian(w, batch_data, batch_target)\n",
    "                D = torch.diag(1 / torch.diag(D))\n",
    "\n",
    "            elif precond_method == \"adam\":\n",
    "                step_t += 1\n",
    "                v = betas[1] * v + (1 - betas[1]) * g.square()\n",
    "                v_hat = v / (1 - torch.pow(betas[1], step_t))\n",
    "\n",
    "                D = torch.diag(1 / (torch.sqrt(v_hat) + 1e-8))\n",
    "                # D = torch.diag(1 / (v_hat + 1e-8))\n",
    "\n",
    "            elif precond_method == \"adagrad\":\n",
    "                v.add_(torch.square(g))\n",
    "                D = torch.diag( 1 / (torch.sqrt(v) + 1e-10) )\n",
    "\n",
    "            \n",
    "            elif precond_method == \"cg\":\n",
    "                # CG is here\n",
    "                s = torch.zeros_like(w) # s = H_inv * grad\n",
    "                r = f_grad.clone()\n",
    "                p = r.detach().clone()\n",
    "\n",
    "                for cg_step in range(MAX_ITER):\n",
    "                    hvp = torch.autograd.grad(g, w, grad_outputs=p, retain_graph=True)[0]\n",
    "                    alpha_k = torch.dot(r, r) / torch.dot(p, hvp)\n",
    "                    s = s + alpha_k * p\n",
    "                    r_prev = r.clone()\n",
    "                    r = r - alpha_k * hvp\n",
    "                    if torch.norm(r) < 1e-10:\n",
    "                        Ax = torch.autograd.grad(g, w, grad_outputs=s, retain_graph=True)[0]    \n",
    "                        diff = torch.norm(Ax - f_grad)\n",
    "                        # print(f\"Took {cg_step} to reach diff={diff}\")\n",
    "                        break\n",
    "\n",
    "                    beta_k = torch.dot(r, r) / torch.dot(r_prev, r_prev)\n",
    "                    p = r + beta_k * p\n",
    "\n",
    "                gnorm = torch.dot(g, s)\n",
    "                precond = (loss / (gnorm + eps))\n",
    "            \n",
    "                with torch.no_grad():\n",
    "                    w.sub_(precond * s) \n",
    "\n",
    "                continue \n",
    "\n",
    "            elif precond_method == \"hutch\":\n",
    "                vk = diag_estimate_old(w, g, 1)\n",
    "\n",
    "                # Smoothing and Truncation \n",
    "                Dk = beta * Dk + (1 - beta) * vk\n",
    "                Dk_hat = torch.abs(Dk)\n",
    "                Dk_hat[Dk_hat < alpha] = alpha\n",
    "\n",
    "                D = torch.diag(1 / Dk_hat)\n",
    "\n",
    "            gnorm = g.dot(D @ g)\n",
    "            precond = (loss / (gnorm)) * D\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                w.sub_(precond @ g)\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rademacher_old(weights):\n",
    "    return torch.round(torch.rand_like(weights)) * 2 - 1\n",
    "\n",
    "def diag_estimate_old(weights, grad, iters):\n",
    "    Ds = []\n",
    "    for j in range(iters):\n",
    "        z = rademacher_old(weights)\n",
    "        with torch.no_grad():\n",
    "            hvp = torch.autograd.grad(grad, weights, grad_outputs=z, retain_graph=True)[0]\n",
    "        Ds.append((hvp*z))\n",
    "    return torch.mean(torch.stack(Ds), 0)\n",
    "\n",
    "def ttv(tuple_in):\n",
    "    return torch.cat([t.view(-1) for t in tuple_in])\n",
    "\n",
    "def flat_hessian(flat_grads, params):\n",
    "    full_hessian = []\n",
    "    for i in range(flat_grads.size()[0]):\n",
    "        temp_hess = torch.autograd.grad(flat_grads[i], params, retain_graph=True)\n",
    "        full_hessian.append(ttv(temp_hess))\n",
    "    return torch.stack(full_hessian)\n",
    "\n",
    "def run_psps2(dataset, epochs, precond_method, pcg_method=\"none\", seed=0, **kwargs):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    data, target, dataloader = dataset\n",
    "\n",
    "    eps = kwargs.get(\"eps\", 1e-6)\n",
    "\n",
    "    # torch.manual_seed(seed)\n",
    "    \n",
    "    # parameters\n",
    "    w = torch.zeros(data.shape[1], device=device).requires_grad_()\n",
    "\n",
    "    # save loss and grad size to history\n",
    "    hist = []\n",
    "\n",
    "    opt = Adam([w], lr=0.1)\n",
    "    \n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss = loss_function(w, data.to(device), target.to(device))\n",
    "    g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "    f_grad = g.clone().detach() \n",
    "\n",
    "\n",
    "    if precond_method == \"none\":\n",
    "        D = torch.ones_like(w)\n",
    "    elif precond_method == \"hutch\":\n",
    "        alpha=0.1\n",
    "        beta=0.999\n",
    "        init_iters = kwargs.get(\"hutch_init_iters\", 20_000)\n",
    "        Dk = diag_estimate_old(w, g, init_iters)\n",
    "    elif precond_method == \"pcg\":\n",
    "        MAX_ITER = train_data.shape[1] * 2\n",
    "\n",
    "    elif precond_method == \"scaling_vec\":\n",
    "        scaling_vec = kwargs[\"scaling_vec\"]\n",
    "        D = (1 / scaling_vec)**2\n",
    "    elif precond_method == \"adam\" or precond_method == \"adam_m\":\n",
    "        D = torch.zeros_like(g)\n",
    "        v = torch.zeros_like(g)\n",
    "        step_t = torch.tensor(0.)\n",
    "        betas = (0.9, 0.999)\n",
    "    elif precond_method == \"adagrad\" or precond_method == \"adagrad_m\":\n",
    "        D = torch.zeros_like(g)\n",
    "        v = torch.zeros_like(g)\n",
    "\n",
    "    if pcg_method == \"hutch\":\n",
    "        alpha=0.1\n",
    "        beta=0.999\n",
    "        init_iters = 20_000\n",
    "        Dk_pcg = diag_estimate_old(w, g, init_iters)\n",
    "    elif pcg_method == \"adam\" or pcg_method == \"adam_m\":\n",
    "        D_pcg = torch.zeros_like(g)\n",
    "        v_pcg = torch.zeros_like(g)\n",
    "        step_t_pcg = torch.tensor(0.)\n",
    "        betas = (0.9, 0.999)\n",
    "    elif pcg_method == \"adagrad\" or pcg_method == \"adagrad_m\":\n",
    "        D_pcg = torch.zeros_like(g)\n",
    "        v_pcg = torch.zeros_like(g)\n",
    "    elif pcg_method == \"none\":\n",
    "        D_pcg = torch.ones_like(g)\n",
    "    elif pcg_method == \"hess_diag\":\n",
    "        closure = lambda w: loss_function(w, data, target)\n",
    "        hess = torch.autograd.functional.hessian(closure, w)\n",
    "        # hess_diag_inv = 1 / torch.diag(torch.autograd.functional.hessian(closure, w))\n",
    "        # hess_diag_inv = 1 / torch.diag(loss_hessian(w, data, target))\n",
    "        # hess = flat_hessian(g,[w])\n",
    "        D_pcg = 1 / torch.diag(hess)\n",
    "    elif pcg_method == \"hess\":\n",
    "        # closure = lambda w: loss_function(w, data, target)\n",
    "        # hess_inv = torch.inverse(torch.autograd.functional.hessian(closure, w))\n",
    "        hess_inv = torch.inverse(loss_hessian(w, data, target))\n",
    "        D_pcg = hess_inv.clone()\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # EVAL\n",
    "        opt.zero_grad()\n",
    "        loss = loss_function(w, data.to(device), target.to(device))\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "        grad_norm_sq = torch.linalg.norm(g) ** 2  \n",
    "        acc = (np.sign(data @ w.detach().numpy()) == target).sum() / target.shape[0]\n",
    "        print(f\"[{epoch}/{epochs}] | Loss: {loss.item()} | GradNorm^2: {grad_norm_sq.item()} | Accuracy: {acc}\")\n",
    "        hist.append([loss.item(), grad_norm_sq.item(), acc])\n",
    "        # END EVAL\n",
    "           \n",
    "        for i, (batch_data, batch_target) in enumerate(dataloader): \n",
    "            \n",
    "            # opt.zero_grad()\n",
    "            # loss = loss_function(w, data.to(device), target.to(device))\n",
    "            # g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "            # grad_norm_sq = torch.linalg.norm(g) ** 2  \n",
    "            # print(f\"[{i}/{epoch}] | Loss: {loss.item()} | GradNorm^2: {grad_norm_sq.item()}\")\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss = loss_function(w, batch_data, batch_target)\n",
    "            g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "            f_grad = g.detach().clone()\n",
    "\n",
    "            if precond_method == \"hess_diag\":\n",
    "                hess = loss_hessian(w, data, target)\n",
    "                # closure = lambda w: loss_function(w, batch_data, batch_target)\n",
    "                # hess = torch.autograd.functional.hessian(closure, w)\n",
    "                hess_diag_inv = 1 / torch.diag(hess)\n",
    "                s = hess_diag_inv * f_grad\n",
    "\n",
    "            elif precond_method == \"newton\":\n",
    "                closure = lambda w: loss_function(w, batch_data, batch_target)\n",
    "                hess = torch.autograd.functional.hessian(closure, w)\n",
    "                # hess = loss_hessian(w, batch_data, batch_target)\n",
    "                s = torch.linalg.solve(hess, f_grad)\n",
    "                # hess[hess <= 0.01] = 0.01\n",
    "                # hess_inv = torch.linalg.inv(hess)\n",
    "                # s = hess_inv @ f_grad\n",
    "\n",
    "            elif precond_method == \"scaling_vec\":\n",
    "                s = D * f_grad\n",
    "\n",
    "            elif precond_method in (\"adam\", \"adam_m\"):\n",
    "                step_t += 1\n",
    "                v = betas[1] * v + (1 - betas[1]) * g.square()\n",
    "                v_hat = v / (1 - torch.pow(betas[1], step_t))\n",
    "\n",
    "                if precond_method == \"adam\":\n",
    "                    D = 1 / (torch.sqrt(v_hat) + eps)\n",
    "                else:\n",
    "                    D = 1 / (v_hat + eps) \n",
    "                s = D * f_grad\n",
    "\n",
    "            elif precond_method in (\"adagrad\", \"adagrad_m\"):\n",
    "                v.add_(torch.square(g))\n",
    "                if precond_method == \"adagrad\":\n",
    "                    D = 1 / (torch.sqrt(v) + eps)\n",
    "                else:\n",
    "                    D = 1 / (v + eps)\n",
    "                s = D * f_grad\n",
    "\n",
    "            elif precond_method == \"scipy_cg\":\n",
    "                closure = lambda w: loss_function(w, batch_data, batch_target)\n",
    "                hess = torch.autograd.functional.hessian(closure, w)\n",
    "                A = scipy.sparse.csc_matrix(hess.detach().numpy())\n",
    "                s, exit_code = scipy.sparse.linalg.cg(A, f_grad.numpy(), tol=1e-10)\n",
    "                s = torch.tensor(s)\n",
    "\n",
    "            elif precond_method == \"none\":\n",
    "                s = D * f_grad\n",
    "\n",
    "            elif precond_method == \"hutch\":\n",
    "                vk = diag_estimate_old(w, g, 1)\n",
    "\n",
    "                # Smoothing and Truncation \n",
    "                Dk = beta * Dk + (1 - beta) * vk\n",
    "                Dk_hat = torch.abs(Dk)\n",
    "                Dk_hat[Dk_hat < alpha] = alpha\n",
    "\n",
    "                D = 1 / Dk_hat\n",
    "                s = D * f_grad\n",
    "\n",
    "            elif precond_method == \"pcg\":\n",
    "\n",
    "                if pcg_method == \"hutch\":\n",
    "                    vk_pcg = diag_estimate_old(w, g, 1)\n",
    "                    # Smoothing and Truncation \n",
    "                    Dk_pcg = beta * Dk_pcg + (1 - beta) * vk_pcg\n",
    "                    Dk_hat = torch.abs(Dk_pcg)\n",
    "                    Dk_hat[Dk_hat < alpha] = alpha\n",
    "                    D_pcg = 1 / Dk_hat\n",
    "\n",
    "                elif pcg_method == \"adam\":\n",
    "                    step_t_pcg += 1\n",
    "                    v_pcg = betas[1] * v_pcg + (1 - betas[1]) * f_grad.square()\n",
    "                    v_hat = v_pcg / (1 - torch.pow(betas[1], step_t_pcg))\n",
    "                    # if pcg_method == \"adam\":\n",
    "                    # D_pcg = 1 / (torch.sqrt(v_hat) + eps)\n",
    "                    # else:\n",
    "                    D_pcg = 1 / (v_hat + eps)                \n",
    "\n",
    "                elif pcg_method == \"adagrad\":\n",
    "                    v_pcg.add_(f_grad.square())\n",
    "                    # if pcg_method == \"adagrad\":\n",
    "                    # D_pcg = 1 / (torch.sqrt(v_pcg) + eps)\n",
    "                    # else:   \n",
    "                    D_pcg = 1 / (v_pcg + eps)\n",
    "\n",
    "                M_inv = D_pcg.clone()\n",
    "                # Preconditioned CG is here\n",
    "                s = torch.zeros_like(w) # s = H_inv * grad\n",
    "                r = f_grad.clone()\n",
    "                z = M_inv * r\n",
    "                p = z.clone()\n",
    "\n",
    "                for cg_step in range(MAX_ITER):\n",
    "                    hvp = torch.autograd.grad(g, w, grad_outputs=p, retain_graph=True)[0]\n",
    "                    \n",
    "                    if torch.dot(p, hvp) <= 0:\n",
    "                        gamma = 0.7\n",
    "                        s = gamma * s + (1 - gamma) * p * torch.sign(torch.dot(p, f_grad))\n",
    "                        hvs = torch.autograd.grad(g, w, grad_outputs=s, retain_graph=True)[0]\n",
    "                        step_size=torch.min(torch.tensor([torch.abs(loss/torch.dot(s,hvs)),5]))\n",
    "                        break\n",
    "                    \n",
    "                    alpha_k = torch.dot(r, z) / torch.dot(p, hvp)\n",
    "                    s = s + alpha_k * p\n",
    "                    r_prev = r.clone()\n",
    "                    r = r - alpha_k * hvp\n",
    "\n",
    "                    if torch.dot(r, M_inv * r) < 1e-5:\n",
    "                        break\n",
    "\n",
    "                    z_prev = z.clone()\n",
    "                    z = M_inv * r\n",
    "                    beta_k = torch.dot(r, z) / torch.dot(r_prev, z_prev)\n",
    "                    p = z + beta_k * p    \n",
    "\n",
    "            grad_norm_sq_scaled = torch.dot(f_grad, s)\n",
    "            if 2 * loss <= grad_norm_sq_scaled:\n",
    "                c = loss / ( grad_norm_sq_scaled )\n",
    "                det = 1 - 2 * c\n",
    "                if det < 0.0:\n",
    "                    step_size = 1.0 \n",
    "                else:\n",
    "                    step_size = 1 - torch.sqrt(det)\n",
    "            else:\n",
    "                step_size = 1.0\n",
    "\n",
    "            # step_size = min(0.05, float(step_size))\n",
    "            with torch.no_grad():\n",
    "                w.sub_(s * step_size)\n",
    "\n",
    "    return hist"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
