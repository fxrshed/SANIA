{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python \n",
    "!whoami \n",
    "!hostname\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "from momo import Momo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import experiments.loss_functions as lf\n",
    "import experiments.datasets as datasets \n",
    "import experiments.models as models\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_loader, test_loader = datasets.get_MNIST(batch_size=batch_size, percentage=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.LeNet5(num_classes=10)\n",
    "print(model)\n",
    "for images, labels in test_loader:\n",
    "    outputs = model(images)\n",
    "    print(outputs[0])\n",
    "    print(labels[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.NN(num_classes=10)\n",
    "print(model)\n",
    "w0 = [w+0.0 for w in model.parameters()]\n",
    "d = np.sum([w.numel() for w in model.parameters()])\n",
    "print (\"Total parameters\",d)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = models.LeNet5(num_classes=10)\n",
    "w0 = [w+0.0 for w in lenet.parameters()]\n",
    "d = np.sum([w.numel() for w in lenet.parameters()])\n",
    "print (\"Total parameters\",d)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSPS2(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            params, \n",
    "            preconditioner=\"none\",\n",
    "            lmd=0.01, \n",
    "            mu=0.1,\n",
    "            eps=1e-8):\n",
    "        \n",
    "        defaults = dict(lmd=lmd, mu=mu, eps=eps)\n",
    "\n",
    "\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "\n",
    "        if preconditioner == \"hessian\":\n",
    "            self._update_precond_grad = self._update_precond_grad_cg\n",
    "        elif preconditioner == \"adam\":\n",
    "            self._update_precond_grad = self._update_precond_grad_adam\n",
    "            for group in self.param_groups:\n",
    "                for p in group[\"params\"]:\n",
    "                    self.state[p][\"adam_v\"] = torch.zeros_like(p)\n",
    "        elif preconditioner == \"adagrad\":\n",
    "            self._update_precond_grad = self._update_precond_grad_adagrad\n",
    "            for group in self.param_groups:\n",
    "                for p in group[\"params\"]:\n",
    "                    self.state[p][\"adagrad_v\"] = torch.zeros_like(p)\n",
    "\n",
    "\n",
    "        elif preconditioner == \"none\":\n",
    "            self._update_precond_grad = self._update_precond_grad_identity\n",
    "\n",
    "\n",
    "        # TO-DO: Think of something better\n",
    "        self.replay_buffer = []\n",
    "\n",
    "        self.step_t = 0\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \n",
    "        loss = None \n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        self.step_t += 1\n",
    "        self._update_precond_grad()\n",
    "        self.update(loss=loss)\n",
    "\n",
    "\n",
    "        # self.replay_buffer.append({\n",
    "        #         \"loss\": loss,\n",
    "        #         \"grad_norm_sq\": gnorm_square,\n",
    "        #         \"slack\": group['s'],\n",
    "        #         \"step_size\": group[\"step_size\"],\n",
    "        #     }) \n",
    "            \n",
    "                          \n",
    "        return loss \n",
    "\n",
    "    def update(self, loss):\n",
    "        for group in self.param_groups: \n",
    "            for p in group['params']:\n",
    "                precond_grad = self.state[p][\"precond_grad\"]\n",
    "                print(p.grad.shape, precond_grad.shape)\n",
    "                grad_norm_sq = torch.dot(p.grad, precond_grad)\n",
    "                eps = group['eps']\n",
    "                if 2 * loss <= grad_norm_sq:    \n",
    "                    det = 1 - ( (2 * loss) / ( grad_norm_sq ))\n",
    "                    group[\"step_size\"] = 1 - torch.sqrt(det).item()\n",
    "                else:\n",
    "                    group[\"step_size\"] = 1.0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    p.sub_(precond_grad, alpha=group['step_size'])\n",
    "\n",
    "    def _update_precond_grad_identity(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                self.state[p][\"precond_grad\"] = p.grad\n",
    "\n",
    "\n",
    "    def _update_precond_grad_adam(self):\n",
    "        betas = (0.9, 0.999)\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                with torch.no_grad():\n",
    "                    self.state[p][\"adam_v\"] = betas[1] * self.state[p][\"adam_v\"] + (1 - betas[1]) * torch.square(p.grad)\n",
    "                    v_hat = self.state[p][\"adam_v\"] / (1 - betas[1]**self.step_t)\n",
    "                    precond = 1 / (torch.sqrt(v_hat) + 1e-8)\n",
    "                    self.state[p][\"precond_grad\"] = precond * p.grad\n",
    "\n",
    "    def _update_precond_grad_adagrad(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                with torch.no_grad():\n",
    "                    self.state[p][\"adagrad_v\"] = self.state[p][\"adagrad_v\"] + torch.square(p.grad)\n",
    "                    precond = 1 / (torch.sqrt( self.state[p][\"adagrad_v\"]) + 1e-10)\n",
    "                    self.state[p][\"precond_grad\"] = precond * p.grad\n",
    "        \n",
    "\n",
    "    def _update_precond_grad_cg(self):\n",
    "        for group in self.param_groups: \n",
    "            for p in group[\"params\"]:\n",
    "                # print(p.shape)\n",
    "                s = torch.zeros_like(p) # s = H_inv * grad\n",
    "                r = p.grad.detach().clone()\n",
    "                d = r.detach().clone()\n",
    "                r_prev = torch.dot(r, r)\n",
    "                MAX_ITER = torch.flatten(p).shape[0] * 2\n",
    "\n",
    "                for cg_step in range(MAX_ITER):\n",
    "                    hvp = torch.autograd.grad(p.grad, p, grad_outputs=d, retain_graph=True)[0]\n",
    "                    alpha_k = torch.dot(r, r) / torch.dot(d, hvp)\n",
    "                    s = s + alpha_k * d\n",
    "                    r_prev = r.detach().clone()\n",
    "                    r = r - alpha_k * hvp\n",
    "                    if torch.norm(r) < 1e-2:\n",
    "                        break\n",
    "                        \n",
    "                    beta_k = torch.dot(r, r) / torch.dot(r_prev, r_prev)\n",
    "                    d = r + beta_k * d\n",
    "                self.state[p][\"precond_grad\"] = s\n",
    "\n",
    "\n",
    "    def _update_precond(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if not self.preconditioner.initialized:\n",
    "                    self.state[p]['precond_mx'] = self.preconditioner.init(p)\n",
    "                self.state[p]['precond_mx'] = self.preconditioner.step(p)\n",
    "\n",
    "    def _init_empty_precond(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                self.state[p]['precond_mx'] = torch.ones_like(p) \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_grad_norm_sq(self):\n",
    "        for group in self.param_groups: \n",
    "            grad_norm_sq = torch.tensor(0.)\n",
    "            for p in group['params']:\n",
    "                precond_grad = self.state[p][\"precond_grad\"]\n",
    "                grad_norm_sq.add_(precond_grad.mul(p.grad).sum())\n",
    "\n",
    "        return grad_norm_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp_from_grad(grads_tuple, list_params, vec_tuple):\n",
    "    # don't damage grads_tuple. Grads_tuple should be calculated with create_graph=True\n",
    "    dot = 0.\n",
    "    for grad, vec in zip(grads_tuple, vec_tuple):\n",
    "        dot += grad.mul(vec).sum()\n",
    "    return torch.autograd.grad(dot, list_params, retain_graph=True)[0]\n",
    "\n",
    "\n",
    "class SP2Plus(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            params,\n",
    "            lr=1.0,\n",
    "            eps=1e-8):\n",
    "        \n",
    "        defaults = dict(lr=lr, eps=eps)\n",
    "\n",
    "        self.step_t = 0\n",
    "\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        \n",
    "        loss = None \n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        self.step_t += 1\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                eps = group[\"eps\"]\n",
    "                lr =group[\"lr\"]\n",
    "\n",
    "                grad = p.grad.detach().clone()\n",
    "                grad_flat = torch.flatten(p.grad.detach().clone())\n",
    "                hgp = torch.autograd.grad(p.grad, p, grad_outputs=p.grad, retain_graph=True)[0]\n",
    "                # hgp = hvp_from_grad(list(p.grad), p, list(p.grad))\n",
    "                grad_norm_sq = torch.dot(grad_flat, grad_flat)\n",
    "                polyak = loss / (grad_norm_sq + eps)\n",
    "                v = grad - (hgp * polyak)\n",
    "                v_norm_sq = torch.dot(torch.flatten(v), torch.flatten(v))\n",
    "                step = (polyak * grad) + (0.5 * polyak**2 * (torch.dot(torch.flatten(hgp), grad_flat) / (v_norm_sq + eps )) * v) \n",
    "                with torch.no_grad():\n",
    "                    p = p - (lr * step)\n",
    "                    # p.sub_(step, alpha=lr)\n",
    "                    \n",
    "                    \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rademacher_old(weights):\n",
    "    return torch.round(torch.rand_like(weights)) * 2 - 1\n",
    "\n",
    "def diag_estimate_old(weights, grad, iters):\n",
    "    Ds = []\n",
    "    for j in range(iters):\n",
    "        z = rademacher_old(weights)\n",
    "        with torch.no_grad():\n",
    "            hvp = torch.autograd.grad(grad, weights, grad_outputs=z, retain_graph=True)[0]\n",
    "        Ds.append((hvp*z))\n",
    "    return torch.mean(torch.stack(Ds), 0)\n",
    "\n",
    "class Custom(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            params,\n",
    "            eps=1e-8):\n",
    "        \n",
    "        defaults = dict(eps=eps)\n",
    "\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "        self._update_precond_grad = self._update_precond_grad_cg\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                self.state[p][\"v\"] = torch.flatten(torch.zeros_like(p))\n",
    "                self.state[p][\"v_init\"] = False\n",
    "\n",
    "\n",
    "        self._step_t = 0\n",
    "        \n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \n",
    "        loss = None \n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        self._step_t += 1\n",
    "        self._update_precond_grad()\n",
    "        self.update(loss=loss)\n",
    "            \n",
    "                          \n",
    "        return loss \n",
    "\n",
    "    def update(self, loss):\n",
    "        for group in self.param_groups: \n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                precond_grad = state[\"precond_grad\"]\n",
    "                flat_grad = torch.flatten(p.grad.detach().clone())\n",
    "                grad_norm_sq = torch.dot(flat_grad, precond_grad)\n",
    "                eps = group['eps']\n",
    "                if 2 * loss <= grad_norm_sq:    \n",
    "                    det = 1 - ( (2 * loss) / ( grad_norm_sq )) # removed eps    \n",
    "                    group[\"step_size\"] = 1 - torch.sqrt(det).item()\n",
    "                else:\n",
    "                    group[\"step_size\"] = 1.0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p.sub_(precond_grad.view_as(p), alpha=group['step_size'])\n",
    "\n",
    "    def _update_precond_grad_identity(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                self.state[p][\"precond_grad\"] = torch.flatten(p.grad)\n",
    "\n",
    "\n",
    "    def _update_precond_grad_cg(self):\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                state = self.state[p]\n",
    "                p_flat = torch.flatten(p.detach().clone())\n",
    "                grad_flat = torch.flatten(p.grad.detach().clone())\n",
    "\n",
    "                s = torch.zeros_like(p_flat) # s = H_inv * grad\n",
    "                r = grad_flat.clone()\n",
    "                b = r.clone()\n",
    "                MAX_ITER = p.shape[0] * 2\n",
    "\n",
    "                for cg_step in range(MAX_ITER):\n",
    "                    hvp = torch.flatten(torch.autograd.grad(p.grad, p, grad_outputs=b.view_as(p), retain_graph=True)[0])\n",
    "                    # hvp = torch.flatten(self.hvp_from_grad(list(p.grad), p, list(b.view_as(p))))\n",
    "\n",
    "                    alpha_k = torch.dot(r, r) / torch.dot(b, hvp)\n",
    "                    if torch.dot(b, hvp) <= 0.0:\n",
    "                        if torch.dot(grad_flat, b) > 0.0:\n",
    "                            s = -b\n",
    "                        else:\n",
    "                            s = b\n",
    "                        break\n",
    "                    s = s + alpha_k * b\n",
    "                    r_prev = r.clone()\n",
    "                    r = r - alpha_k * hvp\n",
    "                    if torch.norm(r) < 1e-2:\n",
    "                        # Ax = torch.autograd.grad(g, w, grad_outputs=s, retain_graph=True)[0]    \n",
    "                        # diff = torch.norm(Ax - f_grad)\n",
    "                        break\n",
    "\n",
    "                    beta_k = torch.dot(r, r) / torch.dot(r_prev, r_prev)\n",
    "                    b = r + beta_k * b\n",
    "\n",
    "                state[\"precond_grad\"] = s\n",
    "\n",
    "\n",
    "\n",
    "    def _update_precond_grad_hutch(self):\n",
    "\n",
    "        alpha = 0.1\n",
    "        beta = 0.999\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                state = self.state[p]\n",
    "\n",
    "                if not state[\"v_init\"]:\n",
    "                    state[\"v\"] = diag_estimate_old(p, p.grad, 1000)\n",
    "                    state[\"v_init\"] = True\n",
    "\n",
    "                grad_flat = torch.flatten(p.grad.detach().clone())\n",
    "                vk = diag_estimate_old(p, p.grad, 1)\n",
    "                state[\"v\"] = beta * state[\"v\"] + (1 - beta) * vk\n",
    "                v_hat = torch.abs(state[\"v\"])\n",
    "                v_hat[v_hat < alpha] = alpha\n",
    "                precond = 1 / v_hat\n",
    "                state[\"precond_grad\"] = torch.mul(torch.flatten(precond), grad_flat)\n",
    "\n",
    "\n",
    "\n",
    "    def _update_precond_grad_adagrad(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                state = self.state[p]\n",
    "                flat_grad = torch.flatten(p.grad.detach().clone())\n",
    "                state[\"v\"] = state[\"v\"] + torch.square(flat_grad)\n",
    "                precond = 1 / (torch.sqrt( state[\"v\"]) + 1e-10)\n",
    "                state[\"precond_grad\"] = torch.mul(precond, flat_grad)\n",
    "\n",
    "    def _update_precond_grad_adam(self):\n",
    "        betas = (0.9, 0.999)\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                with torch.no_grad():\n",
    "                    grad_flat = torch.flatten(p.grad.detach().clone())\n",
    "                    self.state[p][\"v\"] = betas[1] * self.state[p][\"v\"] + (1 - betas[1]) * torch.square(grad_flat)\n",
    "                    v_hat = self.state[p][\"v\"] / (1 - betas[1]**self._step_t)\n",
    "                    precond = 1 / (torch.sqrt(v_hat) + 1e-8)\n",
    "                    self.state[p][\"precond_grad\"] = torch.mul(precond, grad_flat)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_model(model, loss_fn, data_loader):\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    loss = 0\n",
    "    \n",
    "    for images, labels in data_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss += loss_fn(outputs, labels).item() / len(data_loader)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()  \n",
    "    \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def train_nn(model, criterion, train_loader, test_loader, epochs, optimizer_class, **optimizer_kwargs):\n",
    "    \n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    optimizer = optimizer_class(model.parameters(), **optimizer_kwargs)\n",
    "\n",
    "    hist = []\n",
    "\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            train_loss, train_acc = eval_model(model, criterion, train_loader) \n",
    "            print(f\"[{epoch}] | Train Loss: {train_loss} | Train Acc: {train_acc}\")\n",
    "            test_loss, test_acc = eval_model(model, criterion, test_loader)\n",
    "            print(f\"[{epoch}] | Test Loss: {test_loss} | Test Acc: {test_acc}\")\n",
    "            hist.append([train_loss, train_acc, test_loss, test_acc])\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            def closure():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward(create_graph=True)\n",
    "                return loss\n",
    "            \n",
    "            if isinstance(optimizer, PSPS2 | Momo | Custom | SP2Plus): \n",
    "                optimizer.step(closure) \n",
    "            else:\n",
    "                loss = closure()\n",
    "                optimizer.step()\n",
    "        \n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.NN(num_classes=10).to(device)\n",
    "\n",
    "hist_nn = train_nn(\n",
    "    model, \n",
    "    torch.nn.CrossEntropyLoss(),\n",
    "    # F.nll_loss,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    100,\n",
    "    Custom,\n",
    "    # lr=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "class PSPS(Optimizer):\n",
    "\n",
    "    def __init__(self, params, eps=1e-8):\n",
    "        defaults = dict(s=0.0, step_size=0.0, eps=eps)\n",
    "        \n",
    "        self.update = self.update_sps\n",
    "\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                self.state[p]['D'] = torch.ones_like(p) \n",
    "\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \n",
    "        loss = None \n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        gnorm_square = self.calc_grad_norm()\n",
    "        self.update(gnorm_square, loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update_sps(self, grad_norm_sq, loss):\n",
    "        for group in self.param_groups: \n",
    "            for p in group['params']:\n",
    "                grad = p.grad.detach().clone()\n",
    "                state = self.state[p]\n",
    "                scaled_grad = state[\"D\"].mul(grad)\n",
    "                grad_norm_sq = torch.dot(grad, scaled_grad)\n",
    "                group[\"step_size\"] = loss / ( grad_norm_sq + group[\"eps\"] )\n",
    "                # scaled_grad = state[\"scaled_grad\"]\n",
    "                with torch.no_grad():\n",
    "                    # p.sub_(self.state[p]['scaled_grad'].mul(group[\"step_size\"]))\n",
    "                    p.sub_(scaled_grad.mul(group[\"step_size\"])) \n",
    "    \n",
    "\n",
    "    def calc_grad_norm(self):\n",
    "        for group in self.param_groups: \n",
    "            grad_norm_sq = 0.\n",
    "            for p in group[\"params\"]:\n",
    "                grad = p.grad.detach().clone()\n",
    "                state = self.state[p]\n",
    "                state[\"scaled_grad\"] = state[\"D\"].mul(grad)\n",
    "                grad_norm_sq += state[\"scaled_grad\"].mul(grad).sum()\n",
    "        return grad_norm_sq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "batch_size = 64\n",
    "# dataset_name = \"covtype.libsvm.binary\"\n",
    "dataset_name = \"mushrooms\"\n",
    "percentage = 1.0\n",
    "\n",
    "EPOCHS = 20\n",
    "# train_dataloader, train_data, train_target = get_dataset(dataset_name, batch_size, scale_data)\n",
    "datasets_path = os.getenv(\"LIBSVM_DIR\")\n",
    "trainX, trainY = load_svmlight_file(f\"{datasets_path}/{dataset_name}\")\n",
    "sample = np.random.choice(trainX.shape[0], round(trainX.shape[0] * percentage), replace=False)\n",
    "\n",
    "assert sample.shape == np.unique(sample).shape\n",
    "\n",
    "trainX = trainX[sample]\n",
    "trainY = trainY[sample]\n",
    "\n",
    "train_data = torch.tensor(trainX.toarray())\n",
    "train_target = torch.tensor(trainY)\n",
    "\n",
    "\n",
    "# dataloader\n",
    "train_load = data_utils.TensorDataset(train_data, train_target)\n",
    "train_dataloader = DataLoader(train_load, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "loss_function = lf.logreg\n",
    "loss_grad = lf.grad_logreg\n",
    "loss_hessian = lf.hess_logreg\n",
    "\n",
    "if loss_function == lf.logreg:\n",
    "    train_target[train_target == train_target.unique()[0]] = torch.tensor(-1.0, dtype=torch.get_default_dtype())\n",
    "    train_target[train_target == train_target.unique()[1]] = torch.tensor(1.0, dtype=torch.get_default_dtype())\n",
    "    assert torch.equal(train_target.unique(), torch.tensor([-1.0, 1.0]))\n",
    "\n",
    "elif loss_function == lf.nllsq:\n",
    "    train_target[train_target == train_target.unique()[0]] = 0.0\n",
    "    train_target[train_target == train_target.unique()[1]] = 1.0\n",
    "    assert torch.equal(train_target.unique(), torch.tensor([0.0, 1.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "eps = 1e-8\n",
    "\n",
    "w = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "hist_sps = []\n",
    "\n",
    "D = torch.ones_like(w)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    loss = loss_function(w, train_data, train_target)\n",
    "    g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "    acc = (np.sign(train_data @ w.detach().numpy()) == train_target).sum() / train_target.shape[0]\n",
    "    print(f\"[{epoch}/{EPOCHS}] | Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()} | Accuracy: {acc}\")\n",
    "    hist_sps.append([loss.item(), (torch.linalg.norm(g) ** 2 ).item(), acc])\n",
    "\n",
    "    for i, (batch_data, batch_target) in enumerate(train_dataloader):\n",
    "        loss = loss_function(w, batch_data, batch_target)\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "        f_grad = g.clone().detach()\n",
    "\n",
    "        gnorm = torch.dot(f_grad, D.mul(f_grad))\n",
    "        precond = (loss / (gnorm + eps)) * D\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            w.sub_(precond * f_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# parameters\n",
    "w = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "opt = PSPS([w])\n",
    "\n",
    "# logging \n",
    "hist_psps = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    loss = loss_function(w, train_data.to(device), train_target.to(device))\n",
    "    g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "    acc = (np.sign(train_data @ w.detach().numpy()) == train_target).sum() / train_target.shape[0]\n",
    "    print(f\"[{epoch}/{EPOCHS}] | Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()} | Acc: {acc}\")\n",
    "    hist_psps.append([loss.item(), (torch.linalg.norm(g) ** 2).item(), acc])\n",
    "\n",
    "    for i, (batch_data, batch_target) in enumerate(train_dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        def closure():\n",
    "            loss = loss_function(w, batch_data, batch_target)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        \n",
    "        opt.step(closure=closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5), dpi=100)\n",
    "\n",
    "ax[0].semilogy([x[0] for x in hist_sps], label=\"sps\")\n",
    "ax[0].semilogy([x[0] for x in hist_psps], linestyle=\"--\", label=\"psps torch opt\")\n",
    "\n",
    "ax[0].set_ylabel(\"log(loss)\")\n",
    "\n",
    "ax[1].semilogy([x[1] for x in hist_sps])\n",
    "ax[1].semilogy([x[1] for x in hist_psps], linestyle=\"--\")\n",
    "\n",
    "ax[1].set_ylabel(\"grad_norm_sq\")\n",
    "\n",
    "ax[2].semilogy([x[2] for x in hist_sps])\n",
    "ax[2].semilogy([x[2] for x in hist_psps], linestyle=\"--\")\n",
    "\n",
    "ax[2].set_ylabel(\"accuracy\")\n",
    "\n",
    "\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
