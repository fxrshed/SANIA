{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/farshed.abdukhakimov/.conda/envs/psps/bin/python\n",
      "farshed.abdukhakimov\n",
      "cn-08\n",
      "/home/farshed.abdukhakimov/projects/psps2\n"
     ]
    }
   ],
   "source": [
    "!which python \n",
    "!whoami \n",
    "!hostname\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "from torch.optim import SGD, Adam, Adagrad, Adadelta\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import experiments.loss_functions as lf\n",
    "from experiments.utils import get_dataset\n",
    "\n",
    "import optuna\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_mx(X, y):\n",
    "    print(f\"{X.shape=} \\n\"\n",
    "          f\"{X.min()=} \\n\"\n",
    "          f\"{X.max()=} \\n\"\n",
    "          f\"{y.unique()=} \\n\"\n",
    "          f\"{torch.linalg.cond(X)=} \\n\"\n",
    "          f\"Sparsity={1.0 - torch.count_nonzero(X) / X.numel()} \\n\\n\"\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape=torch.Size([62, 2000]) \n",
      "X.min()=tensor(-5.2417) \n",
      "X.max()=tensor(4.5011) \n",
      "y.unique()=tensor([-1.,  1.]) \n",
      "torch.linalg.cond(X)=tensor(1935302.8878) \n",
      "Sparsity=0.0 \n",
      "\n",
      "\n",
      "X.shape=torch.Size([62, 2000]) \n",
      "X.min()=tensor(-1347.5415) \n",
      "X.max()=tensor(1559.9773) \n",
      "y.unique()=tensor([-1.,  1.]) \n",
      "torch.linalg.cond(X)=tensor(2493442.3704) \n",
      "Sparsity=0.0 \n",
      "\n",
      "\n",
      "logreg\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "dataset_name = \"colon-cancer\"\n",
    "batch_size = 16\n",
    "\n",
    "datasets_path = os.getenv(\"LIBSVM_DIR\")\n",
    "trainX, trainY = load_svmlight_file(f\"{datasets_path}/{dataset_name}\")\n",
    "\n",
    "train_data = torch.tensor(trainX.toarray())\n",
    "train_target = torch.tensor(trainY)\n",
    "\n",
    "train_load = TensorDataset(train_data, train_target)\n",
    "train_dataloader = DataLoader(train_load, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "scale = 6\n",
    "r1 = -scale\n",
    "r2 = scale\n",
    "scaling_vec = (r1 - r2) * torch.rand(train_data.shape[1]) + r2\n",
    "scaling_vec = torch.pow(torch.e, scaling_vec)\n",
    "train_data_scaled = scaling_vec * train_data\n",
    "\n",
    "train_load_scaled = TensorDataset(train_data_scaled, train_target)\n",
    "train_dataloader_scaled = DataLoader(train_load_scaled, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train = train_data, train_target, train_dataloader\n",
    "train_scaled = train_data_scaled, train_target, train_dataloader_scaled\n",
    "\n",
    "loss_function = lf.logreg\n",
    "loss_grad = lf.grad_logreg\n",
    "loss_hessian = lf.hess_logreg\n",
    "\n",
    "\n",
    "if loss_function == lf.logreg:\n",
    "    train_target[train_target == train_target.unique()[0]] = torch.tensor(-1.0, dtype=torch.get_default_dtype())\n",
    "    train_target[train_target == train_target.unique()[1]] = torch.tensor(1.0, dtype=torch.get_default_dtype())\n",
    "    assert torch.equal(train_target.unique(), torch.tensor([-1.0, 1.0]))\n",
    "\n",
    "elif loss_function == lf.nllsq:\n",
    "    train_target[train_target == train_target.unique()[0]] = 0.0\n",
    "    train_target[train_target == train_target.unique()[1]] = 1.0\n",
    "    assert torch.equal(train_target.unique(), torch.tensor([0.0, 1.0]))\n",
    "\n",
    "\n",
    "desc_mx(train_data, train_target)\n",
    "desc_mx(train_data_scaled, train_target)\n",
    "print(loss_function.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape=torch.Size([1000, 1000]) \n",
      "X.min()=tensor(-3.2768) \n",
      "X.max()=tensor(3.4887) \n",
      "y.unique()=tensor([-1.,  1.]) \n",
      "torch.linalg.cond(X)=tensor(2531.8414) \n",
      "Sparsity=0.0 \n",
      "\n",
      "\n",
      "X.shape=torch.Size([1000, 1000]) \n",
      "X.min()=tensor(-1250.2118) \n",
      "X.max()=tensor(1108.5549) \n",
      "y.unique()=tensor([-1.,  1.]) \n",
      "torch.linalg.cond(X)=tensor(50207964.6006) \n",
      "Sparsity=0.0 \n",
      "\n",
      "\n",
      "logreg\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "n = 1000\n",
    "d = 1000\n",
    "\n",
    "train_data = np.random.randn(n, d)\n",
    "train_data = (train_data + train_data.T) / 2\n",
    "w_star = np.random.randn(d)\n",
    "\n",
    "batch_size = 200\n",
    "EPOCHS = 100\n",
    "\n",
    "dataset_name = f\"synthetic-classification-{n}x{d}\"\n",
    "\n",
    "train_target = train_data @ w_star\n",
    "train_target[train_target < 0.0] = -1.0\n",
    "train_target[train_target > 0.0] = 1.0\n",
    "\n",
    "train_data = torch.Tensor(train_data)\n",
    "train_target = torch.Tensor(train_target)\n",
    "\n",
    "train_load = TensorDataset(train_data, train_target)\n",
    "train_dataloader = DataLoader(train_load, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "scale = 6\n",
    "r1 = -scale\n",
    "r2 = scale\n",
    "scaling_vec = (r1 - r2) * torch.rand(train_data.shape[1]) + r2\n",
    "scaling_vec = torch.pow(torch.e, scaling_vec)\n",
    "train_data_scaled = scaling_vec * train_data\n",
    "\n",
    "train_load_scaled = TensorDataset(train_data_scaled, train_target)\n",
    "train_dataloader_scaled = DataLoader(train_load_scaled, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train = train_data, train_target, train_dataloader\n",
    "train_scaled = train_data_scaled, train_target, train_dataloader_scaled\n",
    "\n",
    "loss_function = lf.logreg\n",
    "# loss_grad = lf.grad_logreg\n",
    "# loss_hessian = lf.hess_logreg\n",
    "\n",
    "\n",
    "if loss_function in [lf.logreg]:\n",
    "    train_target[train_target == train_target.unique()[0]] = torch.tensor(-1.0, dtype=torch.get_default_dtype())\n",
    "    train_target[train_target == train_target.unique()[1]] = torch.tensor(1.0, dtype=torch.get_default_dtype())\n",
    "    assert torch.equal(train_target.unique(), torch.tensor([-1.0, 1.0]))\n",
    "\n",
    "elif loss_function == lf.nllsq:\n",
    "    train_target[train_target == train_target.unique()[0]] = 0.0\n",
    "    train_target[train_target == train_target.unique()[1]] = 1.0\n",
    "    assert torch.equal(train_target.unique(), torch.tensor([0.0, 1.0]))\n",
    "\n",
    "desc_mx(train_data, train_target)\n",
    "desc_mx(train_data_scaled, train_target)\n",
    "print(loss_function.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-28 13:56:39,315] A new study created in RDB with name: synthetic-classification-1000x1000-0-500-200-adam-logreg\n",
      "[I 2023-11-28 13:56:44,130] Trial 0 finished with value: 1.91866106159262e-08 and parameters: {'lr': 1.2617714714091588}. Best is trial 0 with value: 1.91866106159262e-08.\n",
      "[I 2023-11-28 13:56:47,774] Trial 1 finished with value: 1.6929425879877127e-09 and parameters: {'lr': 2.2111502100130154}. Best is trial 1 with value: 1.6929425879877127e-09.\n",
      "[W 2023-11-28 13:56:51,272] Trial 2 failed with parameters: {'lr': 8.100353772778924} because of the following error: The value nan is not acceptable.\n",
      "[W 2023-11-28 13:56:51,273] Trial 2 failed with value tensor(nan, grad_fn=<MeanBackward0>).\n",
      "[W 2023-11-28 13:56:54,818] Trial 3 failed with parameters: {'lr': 7.9770442083073325} because of the following error: The value nan is not acceptable.\n",
      "[W 2023-11-28 13:56:54,819] Trial 3 failed with value tensor(nan, grad_fn=<MeanBackward0>).\n",
      "[W 2023-11-28 13:56:58,328] Trial 4 failed with parameters: {'lr': 9.795115573164807} because of the following error: The value nan is not acceptable.\n",
      "[W 2023-11-28 13:56:58,329] Trial 4 failed with value tensor(nan, grad_fn=<MeanBackward0>).\n",
      "[W 2023-11-28 13:57:03,074] Trial 5 failed with parameters: {'lr': 9.105760752575616} because of the following error: The value nan is not acceptable.\n",
      "[W 2023-11-28 13:57:03,076] Trial 5 failed with value tensor(nan, grad_fn=<MeanBackward0>).\n",
      "[W 2023-11-28 13:57:06,565] Trial 6 failed with parameters: {'lr': 8.57850839480106} because of the following error: The value nan is not acceptable.\n",
      "[W 2023-11-28 13:57:06,566] Trial 6 failed with value tensor(nan, grad_fn=<MeanBackward0>).\n",
      "[I 2023-11-28 13:57:10,125] Trial 7 finished with value: 0.0 and parameters: {'lr': 6.898905541671059}. Best is trial 7 with value: 0.0.\n",
      "[I 2023-11-28 13:57:13,683] Trial 8 finished with value: 2.37884556214702e-12 and parameters: {'lr': 6.142192187066932}. Best is trial 7 with value: 0.0.\n",
      "[I 2023-11-28 13:57:17,270] Trial 9 finished with value: 1.0528517153417706e-09 and parameters: {'lr': 2.6295000771716803}. Best is trial 7 with value: 0.0.\n"
     ]
    }
   ],
   "source": [
    "opt = Adam\n",
    "epochs = 500\n",
    "\n",
    "for dataset, sc in zip([train, train_scaled], [0, scale]):\n",
    "\n",
    "    def objective(trial):\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        data, target, dataloader = dataset\n",
    "\n",
    "        # parameters\n",
    "        w = torch.zeros(data.shape[1], device=device).requires_grad_()\n",
    "        lr = trial.suggest_float(\"lr\", 1e-20, 10)\n",
    "        opt = Adam([w], lr=lr)\n",
    "        \n",
    "        def compute_loss(w, data, target):\n",
    "            loss = loss_function(w, data, target)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        \n",
    "        for epoch in range(500):\n",
    "            for i, (batch_data, batch_target) in enumerate(dataloader):\n",
    "                batch_data = batch_data.to(device)\n",
    "                batch_target = batch_target.to(device)\n",
    "                opt.zero_grad()\n",
    "                loss = compute_loss(w, batch_data, batch_target)\n",
    "                opt.step()\n",
    "\n",
    "        loss = loss_function(w, data.to(device), target.to(device))\n",
    "        return loss\n",
    "\n",
    "    study_name = f\"{dataset_name}-{scale}-{epochs}-{batch_size}-{opt.__name__.lower()}-{loss_function.__name__}\"\n",
    "    storage_name = f\"sqlite:///experiments/optuna/{study_name}.db\"\n",
    "    study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'lr': 0.00999990153926641}, 8.291517751717862e-11)"
      ]
     },
     "execution_count": 1142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"leu\"\n",
    "scale = 6\n",
    "epochs = 500\n",
    "batch_size = 16\n",
    "opt = \"adagrad\"\n",
    "loss = \"logreg\"\n",
    "\n",
    "study_name = f\"{dataset_name}-{scale}-{epochs}-{batch_size}-{opt}-{loss}\"\n",
    "storage_name = f\"sqlite:///experiments/optuna/{study_name}.db\"\n",
    "study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psps",
   "language": "python",
   "name": "psps"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
