{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python \n",
    "!whoami \n",
    "!hostname\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "from torch.optim import SGD, Adam, Adagrad, Adadelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import experiments.loss_functions as lf\n",
    "from experiments.utils import get_dataset\n",
    "\n",
    "import scipy \n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "batch_size = 256\n",
    "# dataset_name = \"covtype.libsvm.binary\" \n",
    "dataset_name = \"mushrooms\"\n",
    "percentage = 1.0\n",
    "\n",
    "EPOCHS = 1000\n",
    "# train_dataloader, train_data, train_target = get_dataset(dataset_name, batch_size, scale_data)\n",
    "datasets_path = os.getenv(\"LIBSVM_DIR\")\n",
    "trainX, trainY = load_svmlight_file(f\"{datasets_path}/{dataset_name}\")\n",
    "sample = np.random.choice(trainX.shape[0], round(trainX.shape[0] * percentage), replace=False)\n",
    "\n",
    "assert sample.shape == np.unique(sample).shape\n",
    "\n",
    "trainX = trainX[sample]\n",
    "trainY = trainY[sample]\n",
    "\n",
    "train_data = torch.tensor(trainX.toarray())\n",
    "train_target = torch.tensor(trainY)\n",
    "\n",
    "train_load = TensorDataset(train_data, train_target)\n",
    "train_dataloader = DataLoader(train_load, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "scale = 10\n",
    "r1 = -scale\n",
    "r2 = scale\n",
    "scaling_vec = (r1 - r2) * torch.rand(train_data.shape[1]) + r2\n",
    "scaling_vec = torch.pow(torch.e, scaling_vec)\n",
    "train_data_scaled = scaling_vec * train_data\n",
    "\n",
    "train_load_scaled = data_utils.TensorDataset(train_data_scaled, train_target)\n",
    "train_dataloader_scaled = DataLoader(train_load_scaled, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train = train_data, train_target, train_dataloader\n",
    "train_scaled = train_data_scaled, train_target, train_dataloader_scaled\n",
    "\n",
    "# loss_function = lf.logreg\n",
    "# loss_grad = lf.grad_logreg\n",
    "# loss_hessian = lf.hess_logreg\n",
    "\n",
    "loss_function = lf.nllsq\n",
    "# loss_grad = lf.grad_nllsq\n",
    "# loss_hessian = lf.hess_nllsq\n",
    "\n",
    "if loss_function == lf.logreg:\n",
    "    train_target[train_target == train_target.unique()[0]] = torch.tensor(-1.0, dtype=torch.get_default_dtype())\n",
    "    train_target[train_target == train_target.unique()[1]] = torch.tensor(1.0, dtype=torch.get_default_dtype())\n",
    "    assert torch.equal(train_target.unique(), torch.tensor([-1.0, 1.0]))\n",
    "\n",
    "elif loss_function == lf.nllsq:\n",
    "    train_target[train_target == train_target.unique()[0]] = 0.0\n",
    "    train_target[train_target == train_target.unique()[1]] = 1.0\n",
    "    assert torch.equal(train_target.unique(), torch.tensor([0.0, 1.0]))\n",
    "\n",
    "train_data.shape, (train_data.min(), train_data.max()), train_target.unique(), torch.linalg.cond(train_data), torch.linalg.cond(train_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n = 1000\n",
    "d = 100\n",
    "dataset_name = f\"synthetic-regression-{n}x{d}\"\n",
    "modified = False\n",
    "A = np.random.randn(n,d)\n",
    "\n",
    "if modified:\n",
    "    U, S, VH = np.linalg.svd(A)\n",
    "    S *= 0.0\n",
    "    S = np.asarray([1/((x+1)**2) for x in range(S.shape[0])])\n",
    "    A = np.dot(U[:, :S.shape[0]] * S, VH)\n",
    "    dataset_name += \"-modified\"\n",
    "\n",
    "xopt = np.random.randn(d)\n",
    "b = A @ xopt \n",
    "train_data = torch.Tensor(A)\n",
    "train_target = torch.Tensor(b)\n",
    "xopt = torch.Tensor(xopt)\n",
    "\n",
    "batch_size = 1000\n",
    "EPOCHS = 100\n",
    "\n",
    "train_load = data_utils.TensorDataset(train_data, train_target)\n",
    "train_dataloader = DataLoader(train_load, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "scale = 1\n",
    "r1 = -scale\n",
    "r2 = scale\n",
    "scaling_vec = (r1 - r2) * torch.rand(train_data.shape[1]) + r2\n",
    "scaling_vec = torch.pow(torch.e, scaling_vec)\n",
    "train_data_scaled = scaling_vec * train_data\n",
    "\n",
    "train_load_scaled = torch.utils.data.TensorDataset(train_data_scaled, train_target)\n",
    "train_dataloader_scaled = torch.utils.data.DataLoader(train_load_scaled, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train = [train_data, train_target, train_dataloader]\n",
    "train_scaled = [train_data_scaled, train_target, train_dataloader_scaled]\n",
    "\n",
    "loss_function = lf.mse\n",
    "loss_grad = lf.grad_mse\n",
    "loss_hessian = lf.hess_mse\n",
    "\n",
    "train_data.shape, torch.linalg.cond(train_data), torch.linalg.cond(train_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "n = 1000\n",
    "d = 100\n",
    "\n",
    "train_data = np.random.randn(n, d)\n",
    "w_star = np.random.randn(d)\n",
    "\n",
    "batch_size = 1000\n",
    "EPOCHS = 100\n",
    "\n",
    "# U, S, VH = np.linalg.svd(train_data)\n",
    "# S *= 0.0\n",
    "# S = np.asarray([1/((x+1)**2) for x in range(S.shape[0])])\n",
    "# train_data = np.dot(U[:, :S.shape[0]] * S, VH)\n",
    "\n",
    "dataset_name = f\"synthetic-classification-{n}x{d}\"\n",
    "\n",
    "train_target = train_data @ w_star\n",
    "train_target[train_target < 0.0] = 0.0 # -1.0\n",
    "train_target[train_target > 0.0] = 1.0\n",
    "\n",
    "train_data = torch.Tensor(train_data)\n",
    "train_target = torch.Tensor(train_target)\n",
    "\n",
    "train_load = TensorDataset(train_data, train_target)\n",
    "train_dataloader = DataLoader(train_load, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "scale = 5\n",
    "r1 = -scale\n",
    "r2 = scale\n",
    "scaling_vec = (r1 - r2) * torch.rand(train_data.shape[1]) + r2\n",
    "scaling_vec = torch.pow(torch.e, scaling_vec)\n",
    "train_data_scaled = scaling_vec * train_data\n",
    "\n",
    "train_load_scaled = TensorDataset(train_data_scaled, train_target)\n",
    "train_dataloader_scaled = DataLoader(train_load_scaled, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train = train_data, train_target, train_dataloader\n",
    "train_scaled = train_data_scaled, train_target, train_dataloader_scaled\n",
    "\n",
    "\n",
    "loss_function = lf.nllsq\n",
    "# loss_grad = lf.grad_logreg\n",
    "# loss_hessian = lf.hess_logreg\n",
    "\n",
    "\n",
    "if loss_function == lf.logreg:\n",
    "    train_target[train_target == train_target.unique()[0]] = torch.tensor(-1.0, dtype=torch.get_default_dtype())\n",
    "    train_target[train_target == train_target.unique()[1]] = torch.tensor(1.0, dtype=torch.get_default_dtype())\n",
    "    assert torch.equal(train_target.unique(), torch.tensor([-1.0, 1.0]))\n",
    "\n",
    "elif loss_function == lf.nllsq:\n",
    "    train_target[train_target == train_target.unique()[0]] = 0.0\n",
    "    train_target[train_target == train_target.unique()[1]] = 1.0\n",
    "    assert torch.equal(train_target.unique(), torch.tensor([0.0, 1.0]))\n",
    "\n",
    "\n",
    "train_data.shape, (train_data.min(), train_data.max()), train_target.unique(), torch.linalg.cond(train_data), torch.linalg.cond(train_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimizer(optimizer, dataset, EPOCHS, seed=0, **kwargs_optimizer):\n",
    "\n",
    "    data, target, dataloader = dataset\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # parameters\n",
    "    w = torch.zeros(data.shape[1], device=device).requires_grad_()\n",
    "    opt = optimizer([w], **kwargs_optimizer)\n",
    "\n",
    "    # logging \n",
    "    hist = []\n",
    "    \n",
    "    def compute_loss(w, data, target):\n",
    "        loss = loss_function(w, data, target)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    def compute_loss_graph(w, data, target):\n",
    "        loss = loss_function(w, data, target)\n",
    "        loss.backward(create_graph=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        loss = loss_function(w, data.to(device), target.to(device))\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "        acc = (np.sign(data @ w.detach().numpy()) == target).sum() / target.shape[0]\n",
    "        print(f\"[{epoch}/{EPOCHS}] | Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()} | Acc: {acc}\")\n",
    "        hist.append([loss.item(), (torch.linalg.norm(g) ** 2).item(), acc])\n",
    "\n",
    "        for i, (batch_data, batch_target) in enumerate(dataloader):\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_target = batch_target.to(device)\n",
    "            opt.zero_grad()\n",
    "            # if isinstance(opt, Momo):\n",
    "            #     closure = lambda: compute_loss(w, batch_data, batch_target)\n",
    "            #     opt.step(closure=closure)\n",
    "            # elif isinstance(opt, Custom):\n",
    "            #     closure = lambda: compute_loss_graph(w, batch_data, batch_target)\n",
    "            #     opt.step(closure=closure)\n",
    "            # else:\n",
    "            loss = compute_loss(w, batch_data, batch_target)\n",
    "            opt.step()\n",
    "\n",
    "    return hist\n",
    "\n",
    "\n",
    "\n",
    "def save_results(result, dataset_name, percentage, scale, batch_size, epochs, loss_function_name, optimizer_name, lr, \n",
    "                 precond_method, pcg_method, hutch_init_iters, seed):\n",
    "    \n",
    "    results_path = os.getenv(\"RESULTS_DIR\")\n",
    "    directory = f\"{results_path}/{dataset_name}/percentage_{percentage}/scale_{scale}/bs_{batch_size}\" \\\n",
    "    f\"/epochs_{epochs}/{loss_function_name}/{optimizer_name}/lr_{lr}/precond_{precond_method}/pcg_method_{pcg_method}/hutch_init_iters_{hutch_init_iters}/seed_{seed}\"\n",
    "\n",
    "    print(directory)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    torch.save([x[0] for x in result], f\"{directory}/loss\")\n",
    "    torch.save([x[1] for x in result], f\"{directory}/grad_norm_sq\")\n",
    "    torch.save([x[2] for x in result], f\"{directory}/acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSPS2 Rank 1 Scaling\n",
    "$\n",
    "w^* = \\arg\\min_{w\\in\\mathbb{R} ^d}\\frac{1}{2} \\|w - w_t\\|_{B_t} \\nonumber \\\\\n",
    "      \\text{s.t.} \\quad f_i(w_t) +  \\langle  \\nabla  f_i(w_t), w-w_t\\rangle +\\frac{1}{2}\\langle B_t(w-w^t), w - w^t \\rangle \\leq 0 \\\\ \n",
    "      B_t = \\frac{yy^T}{s^Ty} \\\\ \n",
    "      B_t^{+} = \\frac{ss^T}{s^Ty} \\\\ \n",
    "      \\text{where} \\quad s = \\Big ( \\nabla^2  f_i(w_t) \\Big ) ^{-1} \\nabla f_i(w_t) \\\\ \n",
    "      y = \\nabla^2 f_i(w_t) s = \\nabla f_i(w_t) \\\\ \n",
    "      \\text{Update Rule: } \\\\\n",
    "      w_{t+1} = w_t - \\frac{\\alpha}{1 + \\alpha} B_t^{+} \\nabla f_i(w_t)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rademacher_old(weights):\n",
    "    return torch.round(torch.rand_like(weights)) * 2 - 1\n",
    "\n",
    "def diag_estimate_old(weights, grad, iters):\n",
    "    Ds = []\n",
    "    for j in range(iters):\n",
    "        z = rademacher_old(weights)\n",
    "        with torch.no_grad():\n",
    "            hvp = torch.autograd.grad(grad, weights, grad_outputs=z, retain_graph=True)[0]\n",
    "        Ds.append((hvp*z))\n",
    "    return torch.mean(torch.stack(Ds), 0)\n",
    "\n",
    "def run_psps2(dataset, epochs, precond_method, pcg_method=\"none\", seed=0, **kwargs):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    data, target, dataloader = dataset\n",
    "\n",
    "    eps = kwargs.get(\"eps\", 1e-6)\n",
    "\n",
    "    # torch.manual_seed(seed)\n",
    "    \n",
    "    # parameters\n",
    "    w = torch.zeros(data.shape[1], device=device).requires_grad_()\n",
    "\n",
    "    # save loss and grad size to history\n",
    "    hist = []\n",
    "\n",
    "    opt = Adam([w], lr=0.1)\n",
    "    \n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss = loss_function(w, data.to(device), target.to(device))\n",
    "    g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "    f_grad = g.clone().detach() \n",
    "\n",
    "\n",
    "    if precond_method == \"none\":\n",
    "        D = torch.ones_like(w)\n",
    "    elif precond_method == \"hutch\":\n",
    "        alpha=0.1\n",
    "        beta=0.999\n",
    "        init_iters = kwargs[\"hutch_init_iters\"]\n",
    "        Dk = diag_estimate_old(w, g, init_iters)\n",
    "    elif precond_method == \"pcg\":\n",
    "        MAX_ITER = train_data.shape[1] * 2\n",
    "\n",
    "    elif precond_method == \"scaling_vec\":\n",
    "        scaling_vec = kwargs[\"scaling_vec\"]\n",
    "        D = (1 / scaling_vec)**2\n",
    "    elif precond_method == \"adam\" or precond_method == \"adam_m\":\n",
    "        D = torch.zeros_like(g)\n",
    "        v = torch.zeros_like(g)\n",
    "        step_t = torch.tensor(0.)\n",
    "        betas = (0.9, 0.999)\n",
    "    elif precond_method == \"adagrad\" or precond_method == \"adagrad_m\":\n",
    "        D = torch.zeros_like(g)\n",
    "        v = torch.zeros_like(g)\n",
    "\n",
    "    if pcg_method == \"hutch\":\n",
    "        alpha=0.1\n",
    "        beta=0.999\n",
    "        init_iters = kwargs[\"hutch_init_iters\"]\n",
    "        Dk_pcg = diag_estimate_old(w, g, init_iters)\n",
    "    elif pcg_method == \"adam\" or pcg_method == \"adam_m\":\n",
    "        D_pcg = torch.zeros_like(g)\n",
    "        v_pcg = torch.zeros_like(g)\n",
    "        step_t_pcg = torch.tensor(0.)\n",
    "        betas = (0.9, 0.999)\n",
    "    elif pcg_method == \"adagrad\" or pcg_method == \"adagrad_m\":\n",
    "        D_pcg = torch.zeros_like(g)\n",
    "        v_pcg = torch.zeros_like(g)\n",
    "    elif pcg_method == \"none\":\n",
    "        D_pcg = torch.ones_like(g)\n",
    "\n",
    "    cg_steps = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss = loss_function(w, data.to(device), target.to(device))\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "        grad_norm_sq = torch.linalg.norm(g) ** 2  \n",
    "        acc = (np.sign(data @ w.detach().numpy()) == target).sum() / target.shape[0]\n",
    "\n",
    "        print(f\"[{epoch}/{epochs}] | Loss: {loss.item()} | GradNorm^2: {grad_norm_sq.item()} | Accuracy: {acc}\")\n",
    "        hist.append([loss.item(), grad_norm_sq.item(), acc])\n",
    "           \n",
    "        for i, (batch_data, batch_target) in enumerate(dataloader): \n",
    "            \n",
    "            # opt.zero_grad()\n",
    "            # loss = loss_function(w, data.to(device), target.to(device))\n",
    "            # g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "            # grad_norm_sq = torch.linalg.norm(g) ** 2  \n",
    "            # print(f\"[{epoch}/{epochs}] | Loss: {loss.item()} | GradNorm^2: {grad_norm_sq.item()}\")\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss = loss_function(w, batch_data, batch_target)\n",
    "            g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "            f_grad = g.detach().clone()\n",
    "\n",
    "            if precond_method == \"hess_diag\":\n",
    "                hess = loss_hessian(w, batch_data, batch_target)\n",
    "                # closure = lambda w: loss_function(w, batch_data, batch_target)\n",
    "                # hess = torch.autograd.functional.hessian(closure, w)\n",
    "                hess_diag_inv = 1 / torch.diag(hess)\n",
    "                s = hess_diag_inv * f_grad\n",
    "\n",
    "            elif precond_method == \"true_hessian\":\n",
    "                closure = lambda w: loss_function(w, batch_data, batch_target)\n",
    "                hess = torch.autograd.functional.hessian(closure, w)\n",
    "                # hess = loss_hessian(w, batch_data, batch_target)\n",
    "                s = torch.linalg.solve(hess, f_grad)\n",
    "                # hess[hess <= 0.01] = 0.01\n",
    "                # hess_inv = torch.linalg.inv(hess)\n",
    "                # s = hess_inv @ f_grad\n",
    "\n",
    "            elif precond_method == \"scaling_vec\":\n",
    "                s = D * f_grad\n",
    "\n",
    "            elif precond_method in (\"adam\", \"adam_m\"):\n",
    "                step_t += 1\n",
    "                v = betas[1] * v + (1 - betas[1]) * g.square()\n",
    "                v_hat = v / (1 - torch.pow(betas[1], step_t))\n",
    "\n",
    "                if precond_method == \"adam\":\n",
    "                    D = 1 / (torch.sqrt(v_hat) + eps)\n",
    "                else:\n",
    "                    D = 1 / (v_hat + eps) \n",
    "                s = D * f_grad\n",
    "\n",
    "            elif precond_method in (\"adagrad\", \"adagrad_m\"):\n",
    "                v.add_(torch.square(g))\n",
    "                if precond_method == \"adagrad\":\n",
    "                    D = 1 / (torch.sqrt(v) + eps)\n",
    "                else:\n",
    "                    D = 1 / (v + eps)\n",
    "                s = D * f_grad\n",
    "\n",
    "            elif precond_method == \"scipy_cg\":\n",
    "                A = scipy.sparse.csc_matrix(loss_hessian(w, batch_data, batch_target).detach().numpy())\n",
    "                s, exit_code = scipy.sparse.linalg.cg(A, f_grad.numpy(), tol=1e-10)\n",
    "                s = torch.tensor(s)\n",
    "\n",
    "            elif precond_method == \"none\":\n",
    "                s = D * f_grad\n",
    "\n",
    "            elif precond_method == \"hutch\":\n",
    "                vk = diag_estimate_old(w, g, 1)\n",
    "\n",
    "                # Smoothing and Truncation \n",
    "                Dk = beta * Dk + (1 - beta) * vk\n",
    "                Dk_hat = torch.abs(Dk)\n",
    "                Dk_hat[Dk_hat < alpha] = alpha\n",
    "\n",
    "                D = 1 / Dk_hat\n",
    "                s = D * f_grad\n",
    "\n",
    "            elif precond_method == \"pcg\":\n",
    "\n",
    "                if pcg_method == \"hutch\":\n",
    "                    vk_pcg = diag_estimate_old(w, g, 1)\n",
    "                    # Smoothing and Truncation \n",
    "                    Dk_pcg = beta * Dk_pcg + (1 - beta) * vk_pcg\n",
    "                    Dk_hat = torch.abs(Dk_pcg)\n",
    "                    Dk_hat[Dk_hat < alpha] = alpha\n",
    "                    D_pcg = 1 / Dk_hat\n",
    "\n",
    "                elif pcg_method == \"adam\":\n",
    "                    step_t_pcg += 1\n",
    "                    v_pcg = betas[1] * v_pcg + (1 - betas[1]) * f_grad.square()\n",
    "                    v_hat = v_pcg / (1 - torch.pow(betas[1], step_t_pcg))\n",
    "                    # if pcg_method == \"adam\":\n",
    "                    # D_pcg = 1 / (torch.sqrt(v_hat) + 1e-12)\n",
    "                    # else:\n",
    "                    D_pcg = 1 / (v_hat + 1e-6)\n",
    "\n",
    "                elif pcg_method == \"adagrad\":\n",
    "                    v_pcg.add_(f_grad.square())\n",
    "                    # if pcg_method == \"adagrad\":\n",
    "                    #     D_pcg = 1 / (torch.sqrt(v_pcg) + 1e-8)\n",
    "                    # else:   \n",
    "                    D_pcg = 1 / (v_pcg + 1e-6)\n",
    "\n",
    "\n",
    "                hess_diag_inv = D_pcg.clone()\n",
    "                # Preconditioned CG is here\n",
    "                s = torch.zeros_like(w) # s = H_inv * grad\n",
    "                r = f_grad.clone()\n",
    "                z = hess_diag_inv * r\n",
    "                p = z.detach().clone()\n",
    "\n",
    "                for cg_step in range(MAX_ITER):\n",
    "                    hvp = torch.autograd.grad(g, w, grad_outputs=p, retain_graph=True)[0]\n",
    "                    alpha_k = torch.dot(r, z) / torch.dot(p, hvp)\n",
    "                    # if torch.dot(p, hvp) <= 0:\n",
    "                    #     gamma = 0.5\n",
    "                    #     s = gamma * s * torch.sign(torch.dot(s, f_grad)) + (1 - gamma) * p * torch.sign(torch.dot(p ,f_grad))\n",
    "                    #     # s = p.clone()\n",
    "                    #     break\n",
    "\n",
    "                    s = s + alpha_k * p\n",
    "                    r_prev = r.clone()\n",
    "                    r = r - alpha_k * hvp\n",
    "                    r_hat = torch.autograd.grad(g, w, grad_outputs=r, retain_graph=True)[0] \n",
    "                    z_prev = z.clone()\n",
    "                    z = hess_diag_inv * r\n",
    "                    if torch.dot(r, r_hat) < 1e-14:\n",
    "                        break\n",
    "                    \n",
    "\n",
    "                    beta_k = torch.dot(r, z) / torch.dot(r_prev, z_prev)\n",
    "                    p = z + beta_k * p    \n",
    "\n",
    "            grad_norm_sq_scaled = torch.dot(f_grad, s)\n",
    "\n",
    "            if 2 * loss <= ( grad_norm_sq_scaled ):\n",
    "                c = loss / ( grad_norm_sq_scaled )\n",
    "                det = 1 - 2 * c\n",
    "                step_size = 1 - torch.sqrt(det)\n",
    "            else:\n",
    "                step_size = 1.0\n",
    "\n",
    "            # print(f\"{loss=}\")\n",
    "            # print(f\"{step_size=}\")\n",
    "            # print(f\"{torch.norm(s)=}\")\n",
    "            with torch.no_grad():\n",
    "                w.sub_(step_size * s)\n",
    "            # opt.zero_grad()        \n",
    "            # loss = loss_function(w, batch_data, batch_target)\n",
    "            # print(loss)\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "for opt in [Adam, Adagrad, Adadelta]:\n",
    "    for seed in [0, 1, 2, 3, 4]:\n",
    "        for t, s in zip([train, train_scaled], [0, scale]):\n",
    "            hist = run_optimizer(opt, t, EPOCHS, seed=seed, lr=lr)\n",
    "            save_results(hist, dataset_name, 1.0, s, batch_size, EPOCHS, loss_function.__name__, opt.__name__.lower(), lr, \"none\", \"none\", 1000, seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for precond in [\"adagrad_m\"]:\n",
    "    for seed in [0, 1, 2, 3, 4]:\n",
    "        for t, s, eps in zip([train, train_scaled], [0, scale], [1e-6, 1e-9]):\n",
    "            hist = run_psps2(t, EPOCHS, precond, \"none\", seed, eps=eps)\n",
    "            save_results(hist, dataset_name, 1.0, s, batch_size, EPOCHS, loss_function.__name__, \"psps2\", 1.0, precond, \"none\", 1000, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "for precond in [\"adam_m\"]:\n",
    "    for seed in [0, 1, 2, 3, 4]:\n",
    "        # for t, s, eps in zip([train, train_scaled], [0, scale], [1e-6, 1e-9]):\n",
    "        t = train_scaled\n",
    "        s = scale\n",
    "        hist = run_psps2(t, EPOCHS, precond, \"none\", seed, eps=1e-8)\n",
    "        save_results(hist, dataset_name, 1.0, s, batch_size, EPOCHS, loss_function.__name__, \"psps2\", 1.0, precond, \"none\", 1000, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_adam = run_optimizer(Adam, train, EPOCHS, lr=0.001)\n",
    "hist_adam_scaled = run_optimizer(Adam, train_scaled, EPOCHS, lr=0.001)\n",
    "\n",
    "hist_adagrad = run_optimizer(Adagrad, train, EPOCHS, lr=0.001)\n",
    "hist_adagrad_scaled = run_optimizer(Adagrad, train_scaled, EPOCHS, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_psps2_adam = run_psps2(train, EPOCHS, \"adam_m\", \"none\", seed=3, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_psps2_adam_scaled = run_psps2(train_scaled, EPOCHS, \"adam_m\", \"none\", seed=3, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.semilogy([x[0] for x in hist_pcg_adam], label=\"PSPS2 PCG Adam\")\n",
    "# plt.semilogy([x[0] for x in hist_pcg_adam_scaled], linestyle=\"--\", label=\"PSPS2 PCG Adam\")\n",
    "\n",
    "# plt.semilogy([x[0] for x in hist_pcg_adagrad], label=\"PSPS2 PCG Adagrad\")\n",
    "# plt.semilogy([x[0] for x in hist_pcg_adagrad_scaled], linestyle=\"--\", label=\"PSPS2 PCG Adagrad\")\n",
    "\n",
    "# plt.semilogy([x[0] for x in hist_psps2_newton], label=\"PSPS2 Newton\")\n",
    "# plt.semilogy([x[0] for x in hist_psps2_newton_scaled], linestyle=\"--\", label=\"PSPS2 Newton\")\n",
    "\n",
    "plt.semilogy([x[0] for x in hist_psps2_adam], label=\"PSPS2 Adam\")\n",
    "plt.semilogy([x[0] for x in hist_psps2_adam_scaled], linestyle=\"--\", label=\"PSPS2 Adam\")\n",
    "\n",
    "# plt.semilogy([x[0] for x in hist_psps2_adagrad], label=\"PSPS2 Adagrad\")\n",
    "# plt.semilogy([x[0] for x in hist_psps2_adagrad_scaled], linestyle=\"--\", label=\"PSPS2 Adagrad\")\n",
    "\n",
    "plt.semilogy([x[0] for x in hist_adam], label=\"Adam\")\n",
    "plt.semilogy([x[0] for x in hist_adam_scaled], linestyle=\"--\", label=\"Adam\")\n",
    "\n",
    "plt.semilogy([x[0] for x in hist_adagrad], label=\"Adagrad\")\n",
    "plt.semilogy([x[0] for x in hist_adagrad_scaled], linestyle=\"--\", label=\"Adagrad\")\n",
    "\n",
    "\n",
    "# plt.ylim(bottom=1e-2)\n",
    "\n",
    "plt.legend()\n",
    "# plt.savefig(f\"experiments/plots/testtest.jpeg\", format=\"jpeg\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Convex PSPS2 CG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rademacher_old(weights):\n",
    "    return torch.round(torch.rand_like(weights)) * 2 - 1\n",
    "\n",
    "def diag_estimate_old(weights, grad, iters):\n",
    "    Ds = []\n",
    "    for j in range(iters):\n",
    "        z = rademacher_old(weights)\n",
    "        with torch.no_grad():\n",
    "            hvp = torch.autograd.grad(grad, weights, grad_outputs=z, retain_graph=True)[0]\n",
    "        Ds.append((hvp*z))\n",
    "\n",
    "    return torch.mean(torch.stack(Ds), 0)\n",
    "\n",
    "def run_psps2_nc(train_data, train_target, train_dataloader, epochs, precond_method=\"cg\", seed=0, **kwargs):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # parameters\n",
    "    w = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "\n",
    "    # save loss and grad size to history\n",
    "    hist = []\n",
    "\n",
    "\n",
    "    loss = loss_function(w, train_data.to(device), train_target.to(device))\n",
    "    g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "    f_grad = g.clone().detach()\n",
    "\n",
    "\n",
    "    if precond_method == \"cg\" or precond_method == \"pcg\":\n",
    "        s = torch.zeros_like(w) # s = H_inv * grad\n",
    "        r = f_grad - torch.autograd.grad(g, w, grad_outputs=s, retain_graph=True)[0]\n",
    "        p = r.detach().clone()\n",
    "        r_prev = torch.dot(r, r)\n",
    "        MAX_ITER = train_data.shape[1] * 2\n",
    "        # MAX_ITER = 1000\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        loss = loss_function(w, train_data.to(device), train_target.to(device))\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "        grad_norm_sq = torch.linalg.norm(g) ** 2\n",
    "        acc = (np.sign(train_data @ w.detach().numpy()) == train_target).sum() / train_target.shape[0]\n",
    "\n",
    "        print(f\"[{epoch}/{epochs}] | Loss: {loss.item()} | GradNorm^2: {grad_norm_sq.item()} | Accuracy: {acc}\")\n",
    "        hist.append([loss.item(), grad_norm_sq.item(), acc])\n",
    "\n",
    "        for i, (batch_data, batch_target) in enumerate(train_dataloader):\n",
    "\n",
    "            loss = loss_function(w, batch_data, batch_target)\n",
    "            g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "            f_grad = g.detach().clone()\n",
    "\n",
    "            # ssstep = 1.0\n",
    "\n",
    "            if i % 64 == 0:\n",
    "                loss = loss_function(w, train_data.to(device), train_target.to(device))\n",
    "                g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "                grad_norm_sq = torch.linalg.norm(g) ** 2\n",
    "                print(f\"[{epoch}][{i}] | Loss: {loss.item()} | GradNorm^2: {grad_norm_sq.item()} | Accuracy: {acc}\")\n",
    "                # hist.append([loss.item(), grad_norm_sq.item(), acc])\n",
    "\n",
    "            if precond_method == \"cg\":\n",
    "\n",
    "\n",
    "                # estimate the TR radius from Polyak-step-size\n",
    "                trDelta = (loss / ( f_grad.dot(f_grad) )).item()\n",
    "                # print(\"trDelta\",trDelta)\n",
    "\n",
    "\n",
    "                gamma=0.9\n",
    "\n",
    "                # CG is here\n",
    "                s = torch.zeros_like(w) # s = H_inv * grad\n",
    "                z = torch.zeros_like(w)\n",
    "                r = f_grad.clone()\n",
    "                p = r.clone()\n",
    "                # tt=f_grad.clone()\n",
    "                for cg_step in range(MAX_ITER):\n",
    "                    hvp = torch.autograd.grad(g, w, grad_outputs=p, retain_graph=True)[0]\n",
    "\n",
    "                    # print(torch.dot(p,hvp))\n",
    "                    if torch.dot(p,hvp)<=0: \n",
    "                        # print(\"NEGATIVE CURVATURE\")\n",
    "                        # if torch.dot(p,f_grad)<=0:\n",
    "                        s=gamma*z*torch.sign(torch.dot(z,f_grad))+(1-gamma)*p*torch.sign(torch.dot(p,f_grad))\n",
    "                        # else:\n",
    "                            # s=-p\n",
    "                        # hvs = torch.autograd.grad(g, w, grad_outputs=s, retain_graph=True)[0]\n",
    "                        # ssstep=torch.dot(s,f_grad)/torch.dot(s,hvs)\n",
    "                        step_size=torch.min(torch.tensor([loss/torch.dot(s,s),50]))\n",
    "                        # print(\"*\")\n",
    "                        break\n",
    "\n",
    "\n",
    "\n",
    "                    # print(\">\",cg_step)\n",
    "                    # print(\">\",torch.dot(p,hvp))\n",
    "                    alpha_k = torch.dot(r, r) / torch.dot(p, hvp)\n",
    "                    z = z + alpha_k * p\n",
    "                    r_prev = r.clone()\n",
    "                    r = r - alpha_k * hvp\n",
    "                    if torch.norm(r) < 1e-4:\n",
    "                        s=z\n",
    "                        # hvs = torch.autograd.grad(g, w, grad_outputs=s, retain_graph=True)[0]\n",
    "    \n",
    "                        # Ax = torch.autograd.grad(g, w, grad_outputs=s, retain_graph=True)[0]\n",
    "                        # diff = torch.norm(Ax - f_grad)\n",
    "                        # print(f\"CG Took {cg_step} to reach diff={diff}\")\n",
    "                        # cg_steps.append(cg_step)\n",
    "                        grad_norm_sq_scaled = torch.dot(f_grad, s)\n",
    "                        if 2 * loss <= grad_norm_sq_scaled:\n",
    "                            c = loss / ( grad_norm_sq_scaled )\n",
    "                            det = 1 - 2 * c\n",
    "                            if det < 0.0:\n",
    "                                step_size = 1.0\n",
    "                            else:\n",
    "                                # print(\"**\")\n",
    "                                step_size = 1 - torch.sqrt(det)\n",
    "                        else:\n",
    "                            # print(f\"[{epoch}, {i}] No solution\")\n",
    "                            # print(\"***\")\n",
    "                            step_size = 1.0\n",
    "                        \n",
    "                        \n",
    "                        break\n",
    "\n",
    "                    beta_k = torch.dot(r, r) / torch.dot(r_prev, r_prev)\n",
    "                    p = r + beta_k * p\n",
    "\n",
    "            step_size = step_size\n",
    "            # print(\"step_size\",step_size)\n",
    "            # FB =  loss_function(w, train_data.to(device), train_target.to(device))\n",
    "            # FBB =  loss_function(w, batch_data, batch_target)\n",
    "            with torch.no_grad():\n",
    "                w.sub_(step_size *s)\n",
    "            # FA =  loss_function(w, train_data.to(device), train_target.to(device))\n",
    "            # FAA =  loss_function(w, batch_data, batch_target)\n",
    "            \n",
    "            # print(FA - FB, \" <<< 0\", FAA - FBB)\n",
    "            # if i > 50:\n",
    "            #   return ''\n",
    "\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_cg = run_psps2_nc(train_data, train_target, train_dataloader, EPOCHS, precond_method=\"cg\", seed=2)\n",
    "hist_cg_scaled = run_psps2_nc(train_data_scaled, train_target, train_dataloader_scaled, EPOCHS, precond_method=\"cg\", seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_adam = run_optimizer(Adam, train_data, train_target, train_dataloader, EPOCHS, lr=0.1)\n",
    "hist_adam_scaled = run_optimizer(Adam, train_data_scaled, train_target, train_dataloader_scaled, EPOCHS, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy([x[0] for x in hist_cg], label=\"CG\")\n",
    "plt.semilogy([x[0] for x in hist_adam], label=\"Adam\")\n",
    "\n",
    "plt.semilogy([x[0] for x in hist_cg_scaled], linestyle=\"--\", label=\"CG\")\n",
    "plt.semilogy([x[0] for x in hist_adam_scaled], linestyle=\"--\", label=\"Adam\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# plt.savefig(f\"experiments/plots/non-convex-cg_vs_adam-gamma-0_1.jpeg\", format=\"jpeg\", dpi=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psps",
   "language": "python",
   "name": "psps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
